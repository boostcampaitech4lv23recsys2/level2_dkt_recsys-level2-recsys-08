import torch
import torch.nn as nn

try:
    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel
except:
    from transformers.models.bert.modeling_bert import (
        BertConfig,
        BertEncoder,
        BertModel,
    )


class LSTM(nn.Module):
    #new_feature ë¥¼ ì¶”ê°€í•˜ê³  ì‹¶ìœ¼ì‹œë©´, embedding ë¶€ë¶„ë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤.
    #lqtransformer.pyì˜ ì„ë² ë”© ë¶€ë¶„ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”! -> lqtransformer.pyì—ì„œ "new_feature" ì°¾ê¸° ê¸°ëŠ¥
    def __init__(self, args):
        super(LSTM, self).__init__()
        self.args = args

        self.hidden_dim = self.args.hidden_dim
        self.n_layers = self.args.n_layers

        self.lstm = nn.LSTM(
            self.hidden_dim, self.hidden_dim, self.n_layers, batch_first=True #nn.LSTM(input_size, hidden_size, num_layers, ...)
        )

        # Fully connected layer
        self.fc = nn.Linear(self.hidden_dim, 1)

        ######## ì‹ ë‚˜ëŠ” Embedding ########
        # interactionì€ í˜„ì¬ correctë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. correct(1, 2) + padding(0)
        #ğŸ˜˜1.FEí•  ë•Œ ì—¬ê¸°
        
        self.embedding_testId = nn.Embedding(self.args.n_testId + 1, self.hidden_dim // 6)
        self.embedding_assessmentItemID = nn.Embedding(self.args.n_assessmentItemID + 1, self.hidden_dim // 6)
        self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 6)
        self.embedding_big_category = nn.Embedding(self.args.n_big_category + 1, self.hidden_dim // 6)
        self.embedding_mid_category = nn.Embedding(self.args.n_mid_category + 1, self.hidden_dim // 6)
        self.embedding_problem_num = nn.Embedding(self.args.n_problem_num + 1, self.hidden_dim // 6)
        self.embedding_month = nn.Embedding(self.args.n_month + 1, self.hidden_dim // 6)
        self.embedding_dayname = nn.Embedding(self.args.n_dayname + 1, self.hidden_dim // 6)
        self.embedding_KnowledgeTag = nn.Embedding(self.args.n_KnowledgeTag + 1, self.hidden_dim // 6)
    
        self.solvesec_600 = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.big_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.big_std = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.tag_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.tag_std = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.test_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.test_std = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.month_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        
        #ğŸ˜˜2.FEí•  ë•Œ ì—¬ê¸°
        self.comb_proj = nn.Linear((self.hidden_dim // 6) * (9+8-2), self.hidden_dim) # ì›í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì¤„ì´ê¸°

    def forward(self, input):
        
        # test, question, tag, _, mask, interaction = input #(test, question, tag, correct, mask, interaction)
        #ğŸ˜˜3.FEí•  ë•Œ ì—¬ê¸°
        testId, assessmentItemID, big_category, answerCode, mid_category,\
        problem_num,  month, dayname, solvesec_600, \
        KnowledgeTag, big_mean, big_std, tag_mean, tag_std, \
        test_mean, test_std, month_mean, mask, interaction = input
        # test, question, tag, _, mask, interaction, new_feature = input
        batch_size = interaction.size(0) #(64, 20)

        ######## Embedding ########
        #ğŸ˜˜4.FEí•  ë•Œ ì—¬ê¸°
        
        embed_testId = self.embedding_testId(testId.type(torch.cuda.IntTensor))
        embed_assessmentItemID = self.embedding_assessmentItemID(assessmentItemID.type(torch.cuda.IntTensor))
        embed_big_category = self.embedding_big_category(big_category.type(torch.cuda.IntTensor))                #shape = (64,20,21)
        embed_mid_category = self.embedding_mid_category(mid_category.type(torch.cuda.IntTensor))
        embed_problem_num = self.embedding_problem_num(problem_num.type(torch.cuda.IntTensor)) 
        embed_interaction = self.embedding_interaction(interaction.type(torch.cuda.IntTensor))
        embed_month = self.embedding_month(month.type(torch.cuda.IntTensor))
        embed_dayname = self.embedding_dayname(dayname.type(torch.cuda.IntTensor))
        embed_KnowledgeTag = self.embedding_KnowledgeTag(KnowledgeTag.type(torch.cuda.IntTensor))
        
        # embed_new_feature = self.embedding_new_feature(new_feature)
        #ğŸ˜˜5.FEí•  ë•Œ ì—¬ê¸°
        embed = torch.cat(
            [
                embed_testId,
                embed_assessmentItemID,
                embed_big_category,
                embed_mid_category,
                embed_problem_num,
                embed_interaction,
                # embed_month,
                embed_dayname,
                embed_KnowledgeTag,

                self.big_mean(big_mean.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.solvesec_600(solvesec_600.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.big_std(big_std.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.tag_mean(solvesec_600.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.tag_std(tag_std.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.test_mean(test_mean.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.test_std(test_std.unsqueeze(2).type(torch.cuda.FloatTensor)),
                # self.month_mean(month_mean.unsqueeze(2).type(torch.cuda.FloatTensor)),
            ],
            2,
        )

        X = self.comb_proj(embed) #(64,20,64)

        out, _ = self.lstm(X) #hidden, cell state ë°˜í™˜                 #out.shape = (64,20,64)
        out = out.contiguous().view(batch_size, -1, self.hidden_dim)
        out = self.fc(out).view(batch_size, -1)                       #out.shape = (64,20)
        return out


class LSTMATTN(nn.Module):
    def __init__(self, args):
        super(LSTMATTN, self).__init__()
        self.args = args

        self.hidden_dim = self.args.hidden_dim
        self.n_layers = self.args.n_layers
        self.n_heads = self.args.n_heads
        self.drop_out = self.args.drop_out

        self.lstm = nn.LSTM(
            self.hidden_dim, self.hidden_dim, self.n_layers, batch_first=True
        )

        ## ì´ ë¶€ë¶„ë„ Attentionì— ì˜í•´ ì¶”ê°€ëœ ë¶€ë¶„
        self.config = BertConfig(
            3,  # not used
            hidden_size=self.hidden_dim,
            num_hidden_layers=1,
            num_attention_heads=self.n_heads,
            intermediate_size=self.hidden_dim,
            hidden_dropout_prob=self.drop_out,
            attention_probs_dropout_prob=self.drop_out,
        )
        self.attn = BertEncoder(self.config)

        # Fully connected layer
        self.fc = nn.Linear(self.hidden_dim, 1)

        self.activation = nn.Sigmoid()

        ######## ì‹ ë‚˜ëŠ” Embedding ########
        # interactionì€ í˜„ì¬ correctë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. correct(1, 2) + padding(0)
        #ğŸ˜˜1.FEí•  ë•Œ ì—¬ê¸°
        
        self.embedding_testId = nn.Embedding(self.args.n_testId + 1, self.hidden_dim // 6)
        self.embedding_assessmentItemID = nn.Embedding(self.args.n_assessmentItemID + 1, self.hidden_dim // 6)
        self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 6)
        self.embedding_big_category = nn.Embedding(self.args.n_big_category + 1, self.hidden_dim // 6)
        self.embedding_mid_category = nn.Embedding(self.args.n_mid_category + 1, self.hidden_dim // 6)
        self.embedding_problem_num = nn.Embedding(self.args.n_problem_num + 1, self.hidden_dim // 6)
        self.embedding_month = nn.Embedding(self.args.n_month + 1, self.hidden_dim // 6)
        self.embedding_dayname = nn.Embedding(self.args.n_dayname + 1, self.hidden_dim // 6)
        self.embedding_KnowledgeTag = nn.Embedding(self.args.n_KnowledgeTag + 1, self.hidden_dim // 6)
    
        self.solvesec_600 = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.big_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.big_std = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.tag_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.tag_std = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.test_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.test_std = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        self.month_mean = nn.Linear(in_features=1, out_features=self.hidden_dim//6)
        
        #ğŸ˜˜2.FEí•  ë•Œ ì—¬ê¸°
        self.comb_proj = nn.Linear((self.hidden_dim // 6) * (9+8-2), self.hidden_dim) # ì›í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì¤„ì´ê¸°

    def forward(self, input):
        
        # test, question, tag, _, mask, interaction = input #(test, question, tag, correct, mask, interaction)
        #ğŸ˜˜3.FEí•  ë•Œ ì—¬ê¸°
        testId, assessmentItemID, big_category, answerCode, mid_category,\
        problem_num,  month, dayname, solvesec_600, \
        KnowledgeTag, big_mean, big_std, tag_mean, tag_std, \
        test_mean, test_std, month_mean, mask, interaction = input
        # test, question, tag, _, mask, interaction, new_feature = input
        batch_size = interaction.size(0) #(64, 20)

        ######## Embedding ########
        #ğŸ˜˜4.FEí•  ë•Œ ì—¬ê¸°
        
        embed_testId = self.embedding_testId(testId.type(torch.cuda.IntTensor))
        embed_assessmentItemID = self.embedding_assessmentItemID(assessmentItemID.type(torch.cuda.IntTensor))
        embed_big_category = self.embedding_big_category(big_category.type(torch.cuda.IntTensor))                #shape = (64,20,21)
        embed_mid_category = self.embedding_mid_category(mid_category.type(torch.cuda.IntTensor))
        embed_problem_num = self.embedding_problem_num(problem_num.type(torch.cuda.IntTensor)) 
        embed_interaction = self.embedding_interaction(interaction.type(torch.cuda.IntTensor))
        embed_month = self.embedding_month(month.type(torch.cuda.IntTensor))
        embed_dayname = self.embedding_dayname(dayname.type(torch.cuda.IntTensor))
        embed_KnowledgeTag = self.embedding_KnowledgeTag(KnowledgeTag.type(torch.cuda.IntTensor))
        
        # embed_new_feature = self.embedding_new_feature(new_feature)
        #ğŸ˜˜5.FEí•  ë•Œ ì—¬ê¸°
        embed = torch.cat(
            [
                embed_testId,
                embed_assessmentItemID,
                embed_big_category,
                embed_mid_category,
                embed_problem_num,
                embed_interaction,
                # embed_month,
                embed_dayname,
                embed_KnowledgeTag,

                self.big_mean(big_mean.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.solvesec_600(solvesec_600.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.big_std(big_std.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.tag_mean(solvesec_600.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.tag_std(tag_std.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.test_mean(test_mean.unsqueeze(2).type(torch.cuda.FloatTensor)),
                self.test_std(test_std.unsqueeze(2).type(torch.cuda.FloatTensor)),
                # self.month_mean(month_mean.unsqueeze(2).type(torch.cuda.FloatTensor)),
            ],
            2,
        )

        X = self.comb_proj(embed)

        out, _ = self.lstm(X)
        out = out.contiguous().view(batch_size, -1, self.hidden_dim)

        ## ì´ ë¶€ë¶„ì´ íŠ¹ì§•ì ì¸  ë¶€ë¶„
        extended_attention_mask = mask.unsqueeze(1).unsqueeze(2) # ì°¨ì› ëŠ˜ë ¤ì£¼ê¸°
        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 # ë§ˆìŠ¤í‚¹ ëœ ë¶€ë¶„ ê°€ì¤‘ì¹˜ ë‚®ê²Œ ì¤˜ì„œ í•™ìŠµ ì•ˆë˜ë„ë¡
        head_mask = [None] * self.n_layers

        encoded_layers = self.attn(out, extended_attention_mask, head_mask=head_mask) # BERT Encoderë¥¼ ê°€ì ¸ë‹¤ ì”€
        sequence_output = encoded_layers[-1]

        out = self.fc(sequence_output).view(batch_size, -1)
        return out


class Bert(nn.Module):
    def __init__(self, args):
        super(Bert, self).__init__()
        self.args = args

        # Defining some parameters
        self.hidden_dim = self.args.hidden_dim
        self.n_layers = self.args.n_layers

        # Embedding
        # interactionì€ í˜„ì¬ correctìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. correct(1, 2) + padding(0)
        self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 3)

        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim // 3)
        self.embedding_question = nn.Embedding(
            self.args.n_questions + 1, self.hidden_dim // 3
        )

        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim // 3)

        # embedding combination projection
        self.comb_proj = nn.Linear((self.hidden_dim // 3) * 4, self.hidden_dim)

        # Bert config
        self.config = BertConfig(
            3,  # not used
            hidden_size=self.hidden_dim,
            num_hidden_layers=self.args.n_layers,
            num_attention_heads=self.args.n_heads,
            max_position_embeddings=self.args.max_seq_len,
        )

        # Defining the layers
        # Bert Layer
        self.encoder = BertModel(self.config) # ë²„íŠ¸ë¼ê³  ë˜ì–´ìˆì§€ë§Œ, íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë¶€ë¶„

        # Fully connected layer
        self.fc = nn.Linear(self.args.hidden_dim, 1)

        self.activation = nn.Sigmoid()

    def forward(self, input):
        test, question, tag, _, mask, interaction = input #(test, question, tag, correct, mask, interaction)
        # test, question, tag, _, mask, interaction, new_feature = input

        batch_size = interaction.size(0)

        # ì‹ ë‚˜ëŠ” embedding

        embed_interaction = self.embedding_interaction(interaction)

        embed_test = self.embedding_test(test)
        embed_question = self.embedding_question(question)

        embed_tag = self.embedding_tag(tag)

        embed = torch.cat( # ì—¬ê¸°ëŠ” Continuos ì—†ê³ , ë²”ì£¼í˜•ë§Œ ì¡´ì¬
            [
                embed_interaction,
                embed_test,
                embed_question,
                embed_tag,
            ],
            2,
        )

        X = self.comb_proj(embed)

        # Bert
        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)
        out = encoded_layers[0] # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜

        out = out.contiguous().view(batch_size, -1, self.hidden_dim) # ë§ˆì§€ë§‰ ê°’ë§Œ ë½‘ì•„ì„œ

        out = self.fc(out).view(batch_size, -1) # loss ê³„ì‚°
        return out
