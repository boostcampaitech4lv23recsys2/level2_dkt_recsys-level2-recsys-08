{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4443df37",
   "metadata": {},
   "source": [
    "## dataload & FeatureEngineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b40adf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T15:10:00.671710Z",
     "start_time": "2022-11-24T15:09:56.718452Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "plt.style.use('seaborn')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno\n",
    "import pandas as pd\n",
    "pd.set_option('display.min_rows', 500)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def concat_and_export(train_fe, test_fe):\n",
    "    \n",
    "    train_fe['kind']='train'\n",
    "    test_fe['kind']='test'\n",
    "    \n",
    "    df = pd.concat([train_fe,test_fe])\n",
    "    data_dir = '/opt/ml/input/data' # 경로\n",
    "    write_path=f'{data_dir}/after_fe_train_test_cumsum.pkl'\n",
    "    df.to_pickle(write_path,index=False)\n",
    "    print(f\"Write: {write_path}\")\n",
    "    \n",
    "def export(df, output='after_fe_train_test.pkl'):\n",
    "    data_dir = '/opt/ml/input/data' # 경로\n",
    "    write_path=f'{data_dir}/{output}'\n",
    "    df.to_pickle(write_path)\n",
    "    print(f\"Write: {write_path}\")\n",
    "    \n",
    "path='../../data/'\n",
    "train = pd.read_csv(f\"{path}/train_data.csv\")\n",
    "test = pd.read_csv(f\"{path}/test_data.csv\")\n",
    "\n",
    "day_dict = {'Tuesday': 0,\n",
    " 'Thursday': 1,\n",
    " 'Monday': 2,\n",
    " 'Saturday': 3,\n",
    " 'Friday': 4,\n",
    " 'Wednesday': 5,\n",
    " 'Sunday': 6}\n",
    "\n",
    "def feature_engineering(df):\n",
    "    uid2idx = {v:k for k,v in enumerate(sorted(df.userID.unique()))}\n",
    "    ass2idx = {v:k for k,v in enumerate(sorted(df.assessmentItemID.unique()))}\n",
    "    test2idx = {v:k for k,v in enumerate(sorted(df.testId.unique()))}\n",
    "\n",
    "    df2 = df.copy()\n",
    "    #유저별 시퀀스를 고려하기 위해 아래와 같이 정렬\n",
    "    df2.sort_values(by=['userID','Timestamp'], inplace=True)\n",
    "    \n",
    "    # userID, assessmentItemID, testId 라벨인코딩\n",
    "    df2['uidIdx'] = df2.userID.map(uid2idx)\n",
    "    df2['assIdx'] = df2.assessmentItemID.map(ass2idx)\n",
    "    df2['testIdx'] = df2.testId.map(test2idx)\n",
    "    \n",
    "    #유저들의 문제 풀이수, 정답 수, 정답률을 시간순으로 누적해서 계산\n",
    "    df2['user_correct_answer'] = df2.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
    "    df2['user_total_answer'] = df2.groupby('userID')['answerCode'].cumcount()\n",
    "    df2['user_acc'] = df2['user_correct_answer']/df2['user_total_answer']\n",
    "    df2['month'] = pd.to_datetime(df2.Timestamp).dt.month\n",
    "    df2['day'] = pd.to_datetime(df2.Timestamp).dt.day\n",
    "    df2['hour'] = pd.to_datetime(df2.Timestamp).dt.hour\n",
    "    df2['dayname'] = pd.to_datetime(df2.Timestamp).dt.day_name().map(day_dict)\n",
    "    df2['big_category'] = df2.testId.map(lambda x:x[2]).astype(int)\n",
    "    df2['problem_num'] = df2.assessmentItemID.map(lambda x: int(x[-3:]))\n",
    "    df2['mid_category'] = df2.testId.map(lambda x: int(x[-3:]))\n",
    "\n",
    "    # testId와 KnowledgeTag의 전체 정답률은 한번에 계산\n",
    "    # 아래 데이터는 제출용 데이터셋에 대해서도 재사용\n",
    "    correct_t = df2.groupby(['testId'])['answerCode'].agg(['mean', 'std', 'sum'])\n",
    "    correct_t.columns = [\"test_mean\", \"test_std\", 'test_sum']\n",
    "    correct_k = df2.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'std', 'sum'])\n",
    "    correct_k.columns = [\"tag_mean\", 'tag_std', 'tag_sum']\n",
    "\n",
    "    df2 = pd.merge(df2, correct_t, on=['testId'], how=\"left\")\n",
    "    df2 = pd.merge(df2, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
    "    \n",
    "    # 유저별 문제푼 시간, solvesec_3600, time_category\n",
    "    df2['Timestamp2'] = pd.to_datetime(df2.Timestamp)\n",
    "    df2['solvetime'] = df2.groupby('userID')['Timestamp2'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "    df2['solvesec'] = df2.solvetime.map(lambda x : x.total_seconds())\n",
    "    df2['solvesec_3600'] = df2.solvesec\n",
    "    df2.loc[df2.solvesec>=3600,'solvesec_3600']=3600\n",
    "\n",
    "    df2['time_category'] = ''\n",
    "    tc = [0,5,7,10,60,600,1200,2400,3600]\n",
    "    df2.loc[(df2.solvesec==0), 'time_category'] = \"0 - [0,0]\"\n",
    "    for i in range(len(tc)-1):\n",
    "        s,e = tc[i],tc[i+1]\n",
    "        df2.loc[(df2.solvesec>s) & (df2.solvesec<=e),'time_category']=f\"{i+1} - ({s}, {e}]\"\n",
    "    df2.loc[(df2.solvesec>=tc[-1]),'time_category'] = f\"{i+2} - ({e}, )\"\n",
    "    timecat2idx={k:v for v,k in enumerate(sorted(df2.time_category.unique()))}\n",
    "    df2['time_category'] = df2.time_category.map(timecat2idx)\n",
    "    \n",
    "    # Cumsum\n",
    "    df2['solvesec_cumsum'] = df2.groupby(['userID','testId'])['solvesec_3600'].cumsum()%3601\n",
    "    df2['solvecumsum_category'] = ''\n",
    "    tc = [0,5,7,10,60,600,1200,2400,3600,7200]\n",
    "    df2.loc[(df2.solvesec_cumsum==0), 'solvecumsum_category'] = \"0 - [0,0]\"\n",
    "    for i in range(len(tc)-1):\n",
    "        s,e = tc[i],tc[i+1]\n",
    "        df2.loc[(df2.solvesec_cumsum>s) & (df2.solvesec_cumsum<=e),'solvecumsum_category']=f\"{i+1} - ({s}, {e}]\"\n",
    "    df2.loc[(df2.solvesec_cumsum>=tc[-1]),'solvecumsum_category'] = f\"{i+2} - ({e}, )\"\n",
    "    solvecumsum_category2idx={k:v for v,k in enumerate(sorted(df2.solvecumsum_category.unique()))}\n",
    "    df2['solvecumsum_category'] = df2.solvecumsum_category.map(solvecumsum_category2idx)\n",
    "    return df2\n",
    "\n",
    "train['kind']='train'\n",
    "test['kind']='test'\n",
    "df = pd.concat([train,test])\n",
    "df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a857cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T15:12:44.281730Z",
     "start_time": "2022-11-24T15:12:44.275461Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "210ce42d",
   "metadata": {},
   "source": [
    "## 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1c2f416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T15:13:26.968582Z",
     "start_time": "2022-11-24T15:12:45.059365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write: /opt/ml/input/data/after_fe_train_test.csv\n"
     ]
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "export(df2, output='[내보낼 피클 파일 이름].pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
