{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a73ded2",
   "metadata": {},
   "source": [
    "## dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "464cf792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T15:14:36.363764Z",
     "start_time": "2022-12-01T15:14:32.657935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1845539, 40), (1974, 40), (260114, 40))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((1845539, 39), (1845539, 1), (1974, 39), (1974, 1))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from datetime import datetime\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from utils import custom_train_test_split, lgbm_predict, post_slack, title2filename\n",
    "\n",
    "SEED=13\n",
    "## 1. Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
    "data_dir = '/opt/ml/input/data' # Í≤ΩÎ°ú\n",
    "after_fe_path = os.path.join(data_dir, 'after_fe_train_test_bigcategory_fe.pkl')\n",
    "df = pd.read_pickle(after_fe_path)\n",
    "\n",
    "train_df = df[df.kind=='train']\n",
    "train, valid = custom_train_test_split(train_df, ratio=0.7, seed=SEED) # ÌõàÎ†®Îç∞Ïù¥ÌÑ∞ split\n",
    "test = df[df.kind=='test'] # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞\n",
    "train2 = test[test.answerCode!=-1] # ÌÖåÏä§Ìä∏Îç∞Ïù¥ÌÑ∞ ÎßàÏßÄÎßâ Ï†úÏ∂ú 2Î≤àÏ®∞Í∫ºÍπåÏßÄ ÌõàÎ†®Îç∞Ïù¥ÌÑ∞Î°ú ÏÇ¨Ïö©\n",
    "train = pd.concat([train,train2]) # ÌõàÎ†®Îç∞Ïù¥ÌÑ∞ Î≥ëÌï©\n",
    "train.shape, valid.shape, test.shape\n",
    "\n",
    "x_train = train.drop('answerCode',axis=1)\n",
    "y_train = train[['answerCode']]\n",
    "\n",
    "x_valid = valid.drop('answerCode',axis=1)\n",
    "y_valid = valid[['answerCode']]\n",
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc2dbad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T14:53:32.561592Z",
     "start_time": "2022-11-30T14:53:32.541831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>assessmentItemID</th>\n",
       "      <th>testId</th>\n",
       "      <th>answerCode</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>KnowledgeTag</th>\n",
       "      <th>kind</th>\n",
       "      <th>uidIdx</th>\n",
       "      <th>assIdx</th>\n",
       "      <th>testIdx</th>\n",
       "      <th>...</th>\n",
       "      <th>Timestamp2</th>\n",
       "      <th>solvetime</th>\n",
       "      <th>solvesec</th>\n",
       "      <th>solvesec_3600</th>\n",
       "      <th>time_category</th>\n",
       "      <th>solvesec_cumsum</th>\n",
       "      <th>solvecumsum_category</th>\n",
       "      <th>big_category_acc</th>\n",
       "      <th>big_category_std</th>\n",
       "      <th>big_category_cumconut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001001</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:11</td>\n",
       "      <td>7224</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>5354</td>\n",
       "      <td>975</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-03-24 00:17:11</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.711898</td>\n",
       "      <td>0.453371</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001002</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:14</td>\n",
       "      <td>7225</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>5355</td>\n",
       "      <td>975</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-03-24 00:17:14</td>\n",
       "      <td>0 days 00:00:03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.711898</td>\n",
       "      <td>0.453371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001003</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:22</td>\n",
       "      <td>7225</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>5356</td>\n",
       "      <td>975</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-03-24 00:17:22</td>\n",
       "      <td>0 days 00:00:08</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.711898</td>\n",
       "      <td>0.453371</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID assessmentItemID      testId  answerCode            Timestamp  \\\n",
       "0       0       A060001001  A060000001           1  2020-03-24 00:17:11   \n",
       "1       0       A060001002  A060000001           1  2020-03-24 00:17:14   \n",
       "2       0       A060001003  A060000001           1  2020-03-24 00:17:22   \n",
       "\n",
       "   KnowledgeTag   kind  uidIdx  assIdx  testIdx  ...          Timestamp2  \\\n",
       "0          7224  train       0    5354      975  ... 2020-03-24 00:17:11   \n",
       "1          7225  train       0    5355      975  ... 2020-03-24 00:17:14   \n",
       "2          7225  train       0    5356      975  ... 2020-03-24 00:17:22   \n",
       "\n",
       "        solvetime  solvesec  solvesec_3600  time_category  solvesec_cumsum  \\\n",
       "0 0 days 00:00:00       0.0            0.0              0              0.0   \n",
       "1 0 days 00:00:03       3.0            3.0              1              3.0   \n",
       "2 0 days 00:00:08       8.0            8.0              3             11.0   \n",
       "\n",
       "   solvecumsum_category  big_category_acc  big_category_std  \\\n",
       "0                     0          0.711898          0.453371   \n",
       "1                     1          0.711898          0.453371   \n",
       "2                     4          0.711898          0.453371   \n",
       "\n",
       "   big_category_cumconut  \n",
       "0                      0  \n",
       "1                      1  \n",
       "2                      2  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc103e3",
   "metadata": {},
   "source": [
    "## Hyper Parameter ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5bb8a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T15:26:22.632831Z",
     "start_time": "2022-12-01T15:26:22.629165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper parameter ÏÑ§Ï†ï\n",
    "params = {\n",
    "#     \"max_depth\": 8,  # 8,\n",
    "#     \"min_data_in_leaf\": 1000,\n",
    "    # \"feature_fraction\": 0.6,  # 0.8,\n",
    "#     \"bagging_fraction\": 0.75,\n",
    "    # \"max_cat_group\": 64,\n",
    "    \"objective\": \"binary\",\n",
    "#     \"boosting\": \"gbdt\",  # dart\n",
    "#     \"learning_rate\": 0.01,  # 0.01,\n",
    "    # \"bagging_freq\": 5,\n",
    "    \"seed\": 42,\n",
    "    # \"max_bin\": 50,\n",
    "#     \"num_leaves\": 80,  # 40,\n",
    "#     \"metric\": \"auc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db3fb0",
   "metadata": {},
   "source": [
    "##  big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a59531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T14:54:29.315238Z",
     "start_time": "2022-11-30T14:54:29.310652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userID', 'assessmentItemID', 'testId', 'Timestamp', 'KnowledgeTag',\n",
       "       'kind', 'uidIdx', 'assIdx', 'testIdx', 'user_correct_answer',\n",
       "       'user_total_answer', 'user_acc', 'month', 'day', 'hour', 'dayname',\n",
       "       'big_category', 'problem_num', 'mid_category', 'test_mean', 'test_std',\n",
       "       'test_sum', 'tag_mean', 'tag_std', 'tag_sum', 'Timestamp2', 'solvetime',\n",
       "       'solvesec', 'solvesec_3600', 'time_category', 'solvesec_cumsum',\n",
       "       'solvecumsum_category', 'big_category_acc', 'big_category_std',\n",
       "       'big_category_cumconut'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d3f826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T14:57:53.533761Z",
     "start_time": "2022-11-30T14:55:17.088468Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(11/30 Wed)[LGBM big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä] ÌîºÏ≤ò: 27Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(27)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category, big_category_acc, big_category_std, big_category_cumconut')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "[100]\tvalid_0's binary_logloss: 0.560243\n",
      "[200]\tvalid_0's binary_logloss: 0.551904\n",
      "[300]\tvalid_0's binary_logloss: 0.549396\n",
      "[400]\tvalid_0's binary_logloss: 0.547937\n",
      "[500]\tvalid_0's binary_logloss: 0.546311\n",
      "[600]\tvalid_0's binary_logloss: 0.544856\n",
      "[700]\tvalid_0's binary_logloss: 0.544792\n",
      "[800]\tvalid_0's binary_logloss: 0.544924\n",
      "[900]\tvalid_0's binary_logloss: 0.545558\n",
      "[1000]\tvalid_0's binary_logloss: 0.545992\n",
      "[1100]\tvalid_0's binary_logloss: 0.546207\n",
      "[1200]\tvalid_0's binary_logloss: 0.546522\n",
      "[1300]\tvalid_0's binary_logloss: 0.546992\n",
      "[1400]\tvalid_0's binary_logloss: 0.547506\n",
      "[1500]\tvalid_0's binary_logloss: 0.548088\n",
      "[1600]\tvalid_0's binary_logloss: 0.548276\n",
      "[1700]\tvalid_0's binary_logloss: 0.548859\n",
      "[1800]\tvalid_0's binary_logloss: 0.548357\n",
      "[1900]\tvalid_0's binary_logloss: 0.548478\n",
      "[2000]\tvalid_0's binary_logloss: 0.548896\n",
      "[2100]\tvalid_0's binary_logloss: 0.549\n",
      "[2200]\tvalid_0's binary_logloss: 0.549776\n",
      "[2300]\tvalid_0's binary_logloss: 0.550416\n",
      "[2400]\tvalid_0's binary_logloss: 0.550751\n",
      "[2500]\tvalid_0's binary_logloss: 0.551157\n",
      "[2600]\tvalid_0's binary_logloss: 0.551473\n",
      "[2700]\tvalid_0's binary_logloss: 0.551747\n",
      "[2800]\tvalid_0's binary_logloss: 0.552609\n",
      "[2900]\tvalid_0's binary_logloss: 0.552362\n",
      "[3000]\tvalid_0's binary_logloss: 0.552832\n",
      "[3100]\tvalid_0's binary_logloss: 0.553555\n",
      "[3200]\tvalid_0's binary_logloss: 0.553782\n",
      "VALID AUC : 0.7908488865847035 ACC : 0.7137791286727457\n",
      "\n",
      "model1_run_id='6a42312b2c7044a8b69dc362a392f4c8'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel1_run_id\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m file_name \u001b[38;5;241m=\u001b[39m title2filename(title)\n\u001b[0;32m---> 66\u001b[0m lgbm_predict(test, \u001b[43mmodel4\u001b[49m, FEATS, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model4' is not defined"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_acc',\n",
    "         'big_category_std',\n",
    "         'big_category_cumconut'\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model1 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx\n",
    "    #     early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model1.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model1_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model1_run_id=}\")\n",
    "    file_name = title2filename(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04d7402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T14:58:08.914523Z",
     "start_time": "2022-11-30T14:58:08.833305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing prediction : output/11_30_Wed_LGBM_big_category_Ï†ïÎãµÎ•†_std_cumcount_Ï∂îÍ∞Ä_ÌîºÏ≤ò_27Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "lgbm_predict(test, model1, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f90e0de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T15:17:06.035693Z",
     "start_time": "2022-11-30T15:17:06.004161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: 6a42312b2c7044a8b69dc362a392f4c8\n"
     ]
    }
   ],
   "source": [
    "### Ï†úÏ∂ú mlflow Îì±Î°ù\n",
    "# Ï†úÏ∂úÏãú\n",
    "LB_AUC=0.8111\n",
    "run_id = model1_run_id\n",
    "mlflow.start_run(run_id=run_id)\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "mlflow.log_metric(\"LB AUC\",LB_AUC)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a4b09",
   "metadata": {},
   "source": [
    "## big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä - earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca9cb6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T15:16:52.007944Z",
     "start_time": "2022-12-01T15:16:51.853102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>big_category_acc</th>\n",
       "      <td>1845539.0</td>\n",
       "      <td>0.654365</td>\n",
       "      <td>0.105236</td>\n",
       "      <td>0.454470</td>\n",
       "      <td>0.521167</td>\n",
       "      <td>0.679714</td>\n",
       "      <td>0.711898</td>\n",
       "      <td>0.801241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big_category_std</th>\n",
       "      <td>1845539.0</td>\n",
       "      <td>0.463369</td>\n",
       "      <td>0.031221</td>\n",
       "      <td>0.399685</td>\n",
       "      <td>0.453371</td>\n",
       "      <td>0.467279</td>\n",
       "      <td>0.498214</td>\n",
       "      <td>0.500802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big_category_cumconut</th>\n",
       "      <td>1845539.0</td>\n",
       "      <td>159.338010</td>\n",
       "      <td>159.836556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>1268.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count        mean         std       min        25%  \\\n",
       "big_category_acc       1845539.0    0.654365    0.105236  0.454470   0.521167   \n",
       "big_category_std       1845539.0    0.463369    0.031221  0.399685   0.453371   \n",
       "big_category_cumconut  1845539.0  159.338010  159.836556  0.000000  41.000000   \n",
       "\n",
       "                              50%         75%          max  \n",
       "big_category_acc         0.679714    0.711898     0.801241  \n",
       "big_category_std         0.467279    0.498214     0.500802  \n",
       "big_category_cumconut  110.000000  228.000000  1268.000000  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[['big_category_acc','big_category_std','big_category_cumconut']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fee814c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T15:16:26.260151Z",
     "start_time": "2022-12-01T15:15:24.008091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(12/02 Fri)[LGBM big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä earlystop] ÌîºÏ≤ò: 27Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(27)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category, big_category_acc, big_category_std, big_category_cumconut')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.560243\n",
      "[200]\tvalid_0's binary_logloss: 0.551904\n",
      "[300]\tvalid_0's binary_logloss: 0.549396\n",
      "[400]\tvalid_0's binary_logloss: 0.547937\n",
      "[500]\tvalid_0's binary_logloss: 0.546311\n",
      "[600]\tvalid_0's binary_logloss: 0.544856\n",
      "[700]\tvalid_0's binary_logloss: 0.544792\n",
      "[800]\tvalid_0's binary_logloss: 0.544924\n",
      "Early stopping, best iteration is:\n",
      "[672]\tvalid_0's binary_logloss: 0.544468\n",
      "VALID AUC : 0.7970238976941687 ACC : 0.7304964539007093\n",
      "\n",
      "model2re_run_id='46dda027b4024295b7c957f43f2d6984'\n",
      "writing prediction : output/12_02_Fri_LGBM_big_category_Ï†ïÎãµÎ•†_std_cumcount_Ï∂îÍ∞Ä_earlystop_ÌîºÏ≤ò_27Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_acc',\n",
    "         'big_category_std',\n",
    "         'big_category_cumconut'\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä earlystop] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model2re = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "        early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model2re.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model2re_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model2re_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model2re, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01fa935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T14:59:19.649540Z",
     "start_time": "2022-11-30T14:58:20.078479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(11/30 Wed)[LGBM big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä earlystop] ÌîºÏ≤ò: 27Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(27)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category, big_category_acc, big_category_std, big_category_cumconut')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.560243\n",
      "[200]\tvalid_0's binary_logloss: 0.551904\n",
      "[300]\tvalid_0's binary_logloss: 0.549396\n",
      "[400]\tvalid_0's binary_logloss: 0.547937\n",
      "[500]\tvalid_0's binary_logloss: 0.546311\n",
      "[600]\tvalid_0's binary_logloss: 0.544856\n",
      "[700]\tvalid_0's binary_logloss: 0.544792\n",
      "[800]\tvalid_0's binary_logloss: 0.544924\n",
      "Early stopping, best iteration is:\n",
      "[672]\tvalid_0's binary_logloss: 0.544468\n",
      "VALID AUC : 0.7970238976941687 ACC : 0.7304964539007093\n",
      "\n",
      "model2_run_id='88aac4c9c9cb42c089856d46ef8ba746'\n",
      "writing prediction : output/11_30_Wed_LGBM_big_category_Ï†ïÎãµÎ•†_std_cumcount_Ï∂îÍ∞Ä_earlystop_ÌîºÏ≤ò_27Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_acc',\n",
    "         'big_category_std',\n",
    "         'big_category_cumconut'\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä earlystop] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model2 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "        early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model2.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model2_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model2_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model2, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "775e4535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T15:16:37.737912Z",
     "start_time": "2022-11-30T15:16:37.719471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: 88aac4c9c9cb42c089856d46ef8ba746\n"
     ]
    }
   ],
   "source": [
    "### Ï†úÏ∂ú mlflow Îì±Î°ù\n",
    "# Ï†úÏ∂úÏãú\n",
    "LB_AUC=0.8155\n",
    "run_id = model2_run_id\n",
    "mlflow.start_run(run_id=run_id)\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "mlflow.log_metric(\"LB AUC\",LB_AUC)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4c539",
   "metadata": {},
   "source": [
    "## Í∏∞Ï°¥ month Ï∂îÍ∞ÄÌïúÍ≤É ÍπåÏßÄÎßå earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f01d7ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T15:13:15.426381Z",
     "start_time": "2022-12-01T15:13:15.421403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260114, 40)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b51154a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T15:10:47.700296Z",
     "start_time": "2022-11-30T15:09:56.516100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(12/01 Thu)[LGBM monthÏ∂îÍ∞ÄÌïúÍ≤É ÍπåÏßÄÎßå earlystop] ÌîºÏ≤ò: 24Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(24)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19041\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.561618\n",
      "[200]\tvalid_0's binary_logloss: 0.554541\n",
      "[300]\tvalid_0's binary_logloss: 0.552477\n",
      "[400]\tvalid_0's binary_logloss: 0.550644\n",
      "[500]\tvalid_0's binary_logloss: 0.549608\n",
      "[600]\tvalid_0's binary_logloss: 0.549414\n",
      "[700]\tvalid_0's binary_logloss: 0.549202\n",
      "Early stopping, best iteration is:\n",
      "[543]\tvalid_0's binary_logloss: 0.548983\n",
      "VALID AUC : 0.7934421238005892 ACC : 0.7239108409321175\n",
      "\n",
      "model5_run_id='d63b95d1bcd243e9ace084652f35c287'\n",
      "writing prediction : output/12_01_Thu_LGBM_monthÏ∂îÍ∞ÄÌïúÍ≤É_ÍπåÏßÄÎßå_earlystop_ÌîºÏ≤ò_24Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM monthÏ∂îÍ∞ÄÌïúÍ≤É ÍπåÏßÄÎßå earlystop] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model5 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "        early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model5.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model5_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model5_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model5, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d54e1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T15:10:47.700296Z",
     "start_time": "2022-11-30T15:09:56.516100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(12/01 Thu)[LGBM monthÏ∂îÍ∞ÄÌïúÍ≤É ÍπåÏßÄÎßå earlystop] ÌîºÏ≤ò: 24Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(24)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19041\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.561618\n",
      "[200]\tvalid_0's binary_logloss: 0.554541\n",
      "[300]\tvalid_0's binary_logloss: 0.552477\n",
      "[400]\tvalid_0's binary_logloss: 0.550644\n",
      "[500]\tvalid_0's binary_logloss: 0.549608\n",
      "[600]\tvalid_0's binary_logloss: 0.549414\n",
      "[700]\tvalid_0's binary_logloss: 0.549202\n",
      "Early stopping, best iteration is:\n",
      "[543]\tvalid_0's binary_logloss: 0.548983\n",
      "VALID AUC : 0.7934421238005892 ACC : 0.7239108409321175\n",
      "\n",
      "model5_run_id='d63b95d1bcd243e9ace084652f35c287'\n",
      "writing prediction : output/12_01_Thu_LGBM_monthÏ∂îÍ∞ÄÌïúÍ≤É_ÍπåÏßÄÎßå_earlystop_ÌîºÏ≤ò_24Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM monthÏ∂îÍ∞ÄÌïúÍ≤É ÍπåÏßÄÎßå earlystop] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model5 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "        early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model5.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model5_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model5_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model5, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec2b104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T15:16:24.481869Z",
     "start_time": "2022-11-30T15:16:24.462603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: d63b95d1bcd243e9ace084652f35c287\n"
     ]
    }
   ],
   "source": [
    "### Ï†úÏ∂ú mlflow Îì±Î°ù\n",
    "# Ï†úÏ∂úÏãú\n",
    "LB_AUC=0.8112\n",
    "run_id = model5_run_id\n",
    "mlflow.start_run(run_id=run_id)\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "mlflow.log_metric(\"LB AUC\",LB_AUC)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2f5f0ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:03:57.978315Z",
     "start_time": "2022-11-30T17:03:57.973836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userID', 'assessmentItemID', 'testId', 'answerCode', 'Timestamp',\n",
       "       'KnowledgeTag', 'kind', 'uidIdx', 'assIdx', 'testIdx',\n",
       "       'user_correct_answer', 'user_total_answer', 'user_acc', 'month', 'day',\n",
       "       'hour', 'dayname', 'big_category', 'problem_num', 'mid_category',\n",
       "       'test_mean', 'test_std', 'test_sum', 'tag_mean', 'tag_std', 'tag_sum',\n",
       "       'Timestamp2', 'solvetime', 'solvesec', 'solvesec_3600', 'time_category',\n",
       "       'solvesec_cumsum', 'solvecumsum_category', 'big_category_acc',\n",
       "       'big_category_std', 'big_category_cumconut', 'big_category_user_acc',\n",
       "       'big_category_user_std', 'big_category_answer',\n",
       "       'big_category_answer_log1p'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d8aa1",
   "metadata": {},
   "source": [
    "## big category Ïú†Ï†ÄÎ≥Ñ Ï†ïÎãµÎ•†, std, cumcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56d33989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:09:31.852443Z",
     "start_time": "2022-11-30T17:08:44.362508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(12/01 Thu)[LGBM big category Ïú†Ï†ÄÎ≥Ñ Ï†ïÎãµÎ•†, std, cumcount] ÌîºÏ≤ò: 27Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(27)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category, big_category_user_acc, big_category_user_std, big_category_cumconut')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19806\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.513417\n",
      "[200]\tvalid_0's binary_logloss: 0.502819\n",
      "[300]\tvalid_0's binary_logloss: 0.500721\n",
      "[400]\tvalid_0's binary_logloss: 0.500024\n",
      "[500]\tvalid_0's binary_logloss: 0.499942\n",
      "[600]\tvalid_0's binary_logloss: 0.500928\n",
      "Early stopping, best iteration is:\n",
      "[425]\tvalid_0's binary_logloss: 0.499722\n",
      "VALID AUC : 0.8335442074425188 ACC : 0.7563323201621074\n",
      "\n",
      "model6_run_id='3af96264ab7544119d567b2ad3ec4921'\n",
      "writing prediction : output/12_01_Thu_LGBM_big_category_Ïú†Ï†ÄÎ≥Ñ_Ï†ïÎãµÎ•†_std_cumcount_ÌîºÏ≤ò_27Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = [\n",
    "         'uidIdx','assIdx','testIdx','KnowledgeTag',\n",
    "         'user_correct_answer','user_total_answer','user_acc',\n",
    "         'month','day','dayname','hour',\n",
    "         'test_mean','test_sum','test_std',\n",
    "         'tag_std','tag_mean','tag_sum',\n",
    "         'solvesec_3600','time_category',\n",
    "         'solvesec_cumsum','solvecumsum_category',\n",
    "         'big_category',\n",
    "         'big_category_user_acc','big_category_user_std','big_category_cumconut'\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM big category Ïú†Ï†ÄÎ≥Ñ Ï†ïÎãµÎ•†, std, cumcount] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model6 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "        early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model6.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model6_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model6_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model6, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a89aea7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:15:01.410606Z",
     "start_time": "2022-11-30T17:15:01.392853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: 3af96264ab7544119d567b2ad3ec4921\n"
     ]
    }
   ],
   "source": [
    "### Ï†úÏ∂ú mlflow Îì±Î°ù\n",
    "# Ï†úÏ∂úÏãú\n",
    "LB_AUC=0.8024\n",
    "run_id = model6_run_id\n",
    "mlflow.start_run(run_id=run_id)\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "mlflow.log_metric(\"LB AUC\",LB_AUC)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171cd6c",
   "metadata": {},
   "source": [
    "## big_category_answer_log1p Ï∂îÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61cbde29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:10:25.488164Z",
     "start_time": "2022-11-30T17:09:31.858004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(12/01 Thu)[LGBM big_category_answer_log1p Ï∂îÍ∞Ä] ÌîºÏ≤ò: 28Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(28)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category, big_category_user_acc, big_category_user_std, big_category_cumconut, big_category_answer_log1p')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20061\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.512626\n",
      "[200]\tvalid_0's binary_logloss: 0.503799\n",
      "[300]\tvalid_0's binary_logloss: 0.500915\n",
      "[400]\tvalid_0's binary_logloss: 0.500644\n",
      "[500]\tvalid_0's binary_logloss: 0.500709\n",
      "[600]\tvalid_0's binary_logloss: 0.500692\n",
      "[700]\tvalid_0's binary_logloss: 0.501092\n",
      "Early stopping, best iteration is:\n",
      "[545]\tvalid_0's binary_logloss: 0.500358\n",
      "VALID AUC : 0.8335503793676656 ACC : 0.756838905775076\n",
      "\n",
      "model7_run_id='9673ba65009845779aed5d57850755e3'\n",
      "writing prediction : output/12_01_Thu_LGBM_big_category_answer_log1p_Ï∂îÍ∞Ä_ÌîºÏ≤ò_28Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_user_acc',\n",
    "         'big_category_user_std',\n",
    "         'big_category_cumconut',\n",
    "         'big_category_answer_log1p'\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM big_category_answer_log1p Ï∂îÍ∞Ä] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model7 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "        early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model7.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model7_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model7_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model7, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73d006de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:16:37.880233Z",
     "start_time": "2022-11-30T17:16:37.862092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: 9673ba65009845779aed5d57850755e3\n"
     ]
    }
   ],
   "source": [
    "### Ï†úÏ∂ú mlflow Îì±Î°ù\n",
    "# Ï†úÏ∂úÏãú\n",
    "LB_AUC=0.8062\n",
    "run_id = model7_run_id\n",
    "mlflow.start_run(run_id=run_id)\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "mlflow.log_metric(\"LB AUC\",LB_AUC)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5bfba",
   "metadata": {},
   "source": [
    "### big_category_answer_log1p Ï∂îÍ∞Ä 3200 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9acfb2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:20:48.876707Z",
     "start_time": "2022-11-30T17:17:59.871577Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(12/01 Thu)[LGBM big_category_answer_log1p Ï∂îÍ∞Ä 3200epoch] ÌîºÏ≤ò: 28Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(28)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category, big_category_user_acc, big_category_user_std, big_category_cumconut, big_category_answer_log1p')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20061\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "[100]\tvalid_0's binary_logloss: 0.512626\n",
      "[200]\tvalid_0's binary_logloss: 0.503799\n",
      "[300]\tvalid_0's binary_logloss: 0.500915\n",
      "[400]\tvalid_0's binary_logloss: 0.500644\n",
      "[500]\tvalid_0's binary_logloss: 0.500709\n",
      "[600]\tvalid_0's binary_logloss: 0.500692\n",
      "[700]\tvalid_0's binary_logloss: 0.501092\n",
      "[800]\tvalid_0's binary_logloss: 0.501289\n",
      "[900]\tvalid_0's binary_logloss: 0.501496\n",
      "[1000]\tvalid_0's binary_logloss: 0.502544\n",
      "[1100]\tvalid_0's binary_logloss: 0.503477\n",
      "[1200]\tvalid_0's binary_logloss: 0.504137\n",
      "[1300]\tvalid_0's binary_logloss: 0.503887\n",
      "[1400]\tvalid_0's binary_logloss: 0.504628\n",
      "[1500]\tvalid_0's binary_logloss: 0.50493\n",
      "[1600]\tvalid_0's binary_logloss: 0.50508\n",
      "[1700]\tvalid_0's binary_logloss: 0.505023\n",
      "[1800]\tvalid_0's binary_logloss: 0.505362\n",
      "[1900]\tvalid_0's binary_logloss: 0.505408\n",
      "[2000]\tvalid_0's binary_logloss: 0.505975\n",
      "[2100]\tvalid_0's binary_logloss: 0.505633\n",
      "[2200]\tvalid_0's binary_logloss: 0.506046\n",
      "[2300]\tvalid_0's binary_logloss: 0.506811\n",
      "[2400]\tvalid_0's binary_logloss: 0.507144\n",
      "[2500]\tvalid_0's binary_logloss: 0.507197\n",
      "[2600]\tvalid_0's binary_logloss: 0.507878\n",
      "[2700]\tvalid_0's binary_logloss: 0.508198\n",
      "[2800]\tvalid_0's binary_logloss: 0.508786\n",
      "[2900]\tvalid_0's binary_logloss: 0.509258\n",
      "[3000]\tvalid_0's binary_logloss: 0.509476\n",
      "[3100]\tvalid_0's binary_logloss: 0.510393\n",
      "[3200]\tvalid_0's binary_logloss: 0.510491\n",
      "VALID AUC : 0.8320763179117497 ACC : 0.7532928064842959\n",
      "\n",
      "model71_run_id='254fd0dba60b46cf84742c32cc677bb4'\n",
      "writing prediction : output/12_01_Thu_LGBM_big_category_answer_log1p_Ï∂îÍ∞Ä_3200epoch_ÌîºÏ≤ò_28Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_user_acc',\n",
    "         'big_category_user_std',\n",
    "         'big_category_cumconut',\n",
    "         'big_category_answer_log1p'\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM big_category_answer_log1p Ï∂îÍ∞Ä 3200epoch] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model71 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "#         early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model71.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model71_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model71_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model71, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8f5dd14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:22:09.911378Z",
     "start_time": "2022-11-30T17:22:09.879232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: 254fd0dba60b46cf84742c32cc677bb4\n"
     ]
    }
   ],
   "source": [
    "### Ï†úÏ∂ú mlflow Îì±Î°ù\n",
    "# Ï†úÏ∂úÏãú\n",
    "LB_AUC=0.7948\n",
    "run_id = model71_run_id\n",
    "mlflow.start_run(run_id=run_id)\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "mlflow.log_metric(\"LB AUC\",LB_AUC)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922b719",
   "metadata": {},
   "source": [
    "## big_category_answer log1p ÏïàÌïúÍ≤É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8206ef54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:11:19.215988Z",
     "start_time": "2022-11-30T17:10:25.492278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 6, 7, 8, 9, 11, 21, 23],\n",
       " 'üåà(12/01 Thu)[LGBM big_category_answer log1p ÏïàÌïúÍ≤É] ÌîºÏ≤ò: 28Í∞ú',\n",
       " 'ÏÇ¨Ïö©Îêú ÌîºÏ≤ò(28)\\nuidIdx, assIdx, testIdx, KnowledgeTag, user_correct_answer, user_total_answer, big_category, mid_category, problem_num, month, day, dayname, hour, user_acc, test_mean, test_sum, test_std, tag_std, tag_mean, tag_sum, solvesec_3600, time_category, solvesec_cumsum, solvecumsum_category, big_category_user_acc, big_category_user_std, big_category_cumconut, big_category_answer')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20061\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.512626\n",
      "[200]\tvalid_0's binary_logloss: 0.503799\n",
      "[300]\tvalid_0's binary_logloss: 0.500915\n",
      "[400]\tvalid_0's binary_logloss: 0.500644\n",
      "[500]\tvalid_0's binary_logloss: 0.500709\n",
      "[600]\tvalid_0's binary_logloss: 0.500692\n",
      "[700]\tvalid_0's binary_logloss: 0.501092\n",
      "Early stopping, best iteration is:\n",
      "[545]\tvalid_0's binary_logloss: 0.500358\n",
      "VALID AUC : 0.8335503793676656 ACC : 0.756838905775076\n",
      "\n",
      "model8_run_id='3c9e5adba003459694dbdd9ca5384119'\n",
      "writing prediction : output/12_01_Thu_LGBM_big_category_answer_log1p_ÏïàÌïúÍ≤É_ÌîºÏ≤ò_28Í∞ú.csv\n"
     ]
    }
   ],
   "source": [
    "### ÌîºÏ≤ò ÏÑ§Ï†ï\n",
    "# ÏÇ¨Ïö©Ìï† Feature ÏÑ§Ï†ï\n",
    "FEATS = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_user_acc',\n",
    "         'big_category_user_std',\n",
    "         'big_category_cumconut',\n",
    "         'big_category_answer'\n",
    "        ]\n",
    "\n",
    "cat_feats = ['uidIdx','assIdx','testIdx','KnowledgeTag','big_category','mid_category',\n",
    "             'problem_num','dayname','month','time_category','solvecumsum_category']\n",
    "cat_feats_idx = [i for i,e in enumerate(FEATS) if e in cat_feats]\n",
    "\n",
    "### ÌïôÏäµ Î∞è ÏòàÏ∏°\n",
    "\n",
    "date = datetime.now().strftime('%m/%d %a')\n",
    "title=f\"üåà({date})[LGBM big_category_answer log1p ÏïàÌïúÍ≤É] ÌîºÏ≤ò: {len(FEATS)}Í∞ú\"\n",
    "using_feats=\", \".join(FEATS)\n",
    "desc=f\"ÏÇ¨Ïö©Îêú ÌîºÏ≤ò({len(FEATS)})\\n{using_feats}\"\n",
    "cat_feats_idx, title, desc\n",
    "\n",
    "mlflow.lightgbm.autolog()\n",
    "lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "\n",
    "with mlflow.start_run(run_name=title, description=desc) as run:\n",
    "    model8 = lgb.train(\n",
    "        params, \n",
    "        lgb_x_train,\n",
    "        valid_sets=[lgb_x_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=3200,\n",
    "        categorical_feature=cat_feats_idx,\n",
    "        early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    preds = model8.predict(x_valid[FEATS])\n",
    "    acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "    mlflow.log_metric(\"VAL AUC\",auc)\n",
    "    mlflow.log_metric(\"VAL Set SEED\",SEED)\n",
    "    model8_run_id=run.info.run_id\n",
    "    post_slack(\"done\")\n",
    "    print(f\"{model8_run_id=}\")\n",
    "    file_name = title2filename(title)\n",
    "    lgbm_predict(test, model8, FEATS, f'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef082ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ï†úÏ∂ú mlflow Îì±Î°ù\n",
    "# Ï†úÏ∂úÏãú\n",
    "LB_AUC=0.8112\n",
    "run_id = model8_run_id\n",
    "mlflow.start_run(run_id=run_id)\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "mlflow.log_metric(\"LB AUC\",LB_AUC)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ea081",
   "metadata": {},
   "source": [
    "## Validation Set Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0e909a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T02:07:51.703832Z",
     "start_time": "2022-12-01T02:07:51.694224Z"
    }
   },
   "outputs": [],
   "source": [
    "# model1\n",
    "FEATS1 = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_acc',\n",
    "         'big_category_std',\n",
    "         'big_category_cumconut'\n",
    "        ]\n",
    "\n",
    "# model2\n",
    "FEATS2 = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_acc',\n",
    "         'big_category_std',\n",
    "         'big_category_cumconut'\n",
    "        ]\n",
    "\n",
    "# model5\n",
    "FEATS5 = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "        ]\n",
    "# model6\n",
    "FEATS6 = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_user_acc',\n",
    "         'big_category_user_std',\n",
    "         'big_category_cumconut'\n",
    "        ]\n",
    "\n",
    "# model7\n",
    "FEATS7 = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_user_acc',\n",
    "         'big_category_user_std',\n",
    "         'big_category_cumconut',\n",
    "         'big_category_answer_log1p'\n",
    "        ]\n",
    "\n",
    "# model71\n",
    "FEATS71 = ['uidIdx',\n",
    "         'assIdx',\n",
    "         'testIdx',\n",
    "         'KnowledgeTag',\n",
    "         'user_correct_answer',\n",
    "         'user_total_answer',\n",
    "         'big_category',\n",
    "         'mid_category',\n",
    "         'problem_num',\n",
    "         'month','day','dayname','hour',\n",
    "         'user_acc',\n",
    "         'test_mean',\n",
    "         'test_sum',\n",
    "         'test_std',\n",
    "         'tag_std',\n",
    "         'tag_mean',\n",
    "         'tag_sum',\n",
    "         'solvesec_3600',\n",
    "         'time_category',\n",
    "         'solvesec_cumsum',\n",
    "         'solvecumsum_category',\n",
    "         'big_category_user_acc',\n",
    "         'big_category_user_std',\n",
    "         'big_category_cumconut',\n",
    "         'big_category_answer_log1p'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17927beb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T04:43:15.694442Z",
     "start_time": "2022-12-01T04:43:15.690609Z"
    }
   },
   "outputs": [],
   "source": [
    "def model2auc(model, x_valid, y_valid, FEATS):\n",
    "    preds = model.predict(x_valid[FEATS])\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f87807e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T02:10:07.261701Z",
     "start_time": "2022-12-01T02:10:07.258925Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "model_dict = defaultdict(list)\n",
    "\n",
    "for n in [1,2,5,6,7,71]:\n",
    "    model_dict[f\"model{n}\"].append(eval(f'model{n}'))\n",
    "    model_dict[f\"model{n}\"].append(eval(f'FEATS{n}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "691fb872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T02:14:38.251097Z",
     "start_time": "2022-12-01T02:14:38.247085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model1', 'model2', 'model5', 'model6', 'model7', 'model71'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993e4d5",
   "metadata": {},
   "source": [
    "## seedÎ≥Ñ ÌïôÏäµ Î∞è auc ÎπÑÍµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 # 0.8155 - big_category Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä early stop\n",
    "model6 # 0.8024 - big_category Ïú†Ï†ÄÎ≥Ñ Ï†ïÎãµÎ•†, std, cumcount Ï∂îÍ∞Ä early stop\n",
    "model71 # 0.7948 - big_category_answer_log1p Ï∂îÍ∞Ä early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f694cc78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T05:34:15.480469Z",
     "start_time": "2022-12-01T05:34:15.475258Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_train2auc(run_title, x_valid, y_valid, FEATS, seed, experiment_id):\n",
    "    lgb_x_train = lgb.Dataset(x_train[FEATS], y_train)\n",
    "    lgb_x_valid = lgb.Dataset(x_valid[FEATS], y_valid)\n",
    "    with mlflow.start_run(run_name=run_title, experiment_id=experiment_id) as run:\n",
    "        model = lgb.train(\n",
    "            params, \n",
    "            lgb_x_train,\n",
    "            valid_sets=[lgb_x_valid],\n",
    "            verbose_eval=-1,\n",
    "            num_boost_round=3200,\n",
    "            categorical_feature=cat_feats_idx,\n",
    "            early_stopping_rounds=200,\n",
    "        )\n",
    "        preds = model.predict(x_valid[FEATS])\n",
    "        auc = roc_auc_score(y_valid, preds)\n",
    "        mlflow.log_metric(\"VAL AUC\",auc)\n",
    "        mlflow.log_metric(\"VAL Set SEED\",seed)\n",
    "    return model, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055159f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T05:15:15.258194Z",
     "start_time": "2022-12-01T05:15:15.248488Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "experiment_name = \"Validation Set Align2\"\n",
    "experiment_id = client.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1e8e442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T05:16:44.676974Z",
     "start_time": "2022-12-01T05:16:44.672710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'312762763748397651'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6145b290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T10:50:43.151150Z",
     "start_time": "2022-12-01T05:34:24.976697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- SEED 0 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845739, 39), (1845739, 1), (1968, 39), (1968, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204303, number of negative: 641436\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19310\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652477 -> initscore=0.629947\n",
      "[LightGBM] [Info] Start training from score 0.629947\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[760]\tvalid_0's binary_logloss: 0.525738\n",
      "model2 Val Auc: 0.812\n",
      "[LightGBM] [Info] Number of positive: 1204303, number of negative: 641436\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19800\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652477 -> initscore=0.629947\n",
      "[LightGBM] [Info] Start training from score 0.629947\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[502]\tvalid_0's binary_logloss: 0.485933\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1204303, number of negative: 641436\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20055\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652477 -> initscore=0.629947\n",
      "[LightGBM] [Info] Start training from score 0.629947\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[678]\tvalid_0's binary_logloss: 0.484449\n",
      "model71 Val Auc: 0.845\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 1 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845952, 39), (1845952, 1), (2015, 39), (2015, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208247, number of negative: 637705\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19249\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845952, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654539 -> initscore=0.639050\n",
      "[LightGBM] [Info] Start training from score 0.639050\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1182]\tvalid_0's binary_logloss: 0.519259\n",
      "model2 Val Auc: 0.817\n",
      "[LightGBM] [Info] Number of positive: 1208247, number of negative: 637705\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19739\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845952, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654539 -> initscore=0.639050\n",
      "[LightGBM] [Info] Start training from score 0.639050\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[381]\tvalid_0's binary_logloss: 0.476321\n",
      "model6 Val Auc: 0.850\n",
      "[LightGBM] [Info] Number of positive: 1208247, number of negative: 637705\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19994\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845952, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654539 -> initscore=0.639050\n",
      "[LightGBM] [Info] Start training from score 0.639050\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[630]\tvalid_0's binary_logloss: 0.475942\n",
      "model71 Val Auc: 0.850\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 2 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845977, 39), (1845977, 1), (2011, 39), (2011, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209855, number of negative: 636122\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19272\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845977, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655401 -> initscore=0.642865\n",
      "[LightGBM] [Info] Start training from score 0.642865\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[661]\tvalid_0's binary_logloss: 0.534324\n",
      "model2 Val Auc: 0.805\n",
      "[LightGBM] [Info] Number of positive: 1209855, number of negative: 636122\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19762\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845977, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655401 -> initscore=0.642865\n",
      "[LightGBM] [Info] Start training from score 0.642865\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\tvalid_0's binary_logloss: 0.499896\n",
      "model6 Val Auc: 0.833\n",
      "[LightGBM] [Info] Number of positive: 1209855, number of negative: 636122\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20017\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845977, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655401 -> initscore=0.642865\n",
      "[LightGBM] [Info] Start training from score 0.642865\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[481]\tvalid_0's binary_logloss: 0.500456\n",
      "model71 Val Auc: 0.833\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 3 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845528, 39), (1845528, 1), (2009, 39), (2009, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206497, number of negative: 639031\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19266\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845528, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653741 -> initscore=0.635523\n",
      "[LightGBM] [Info] Start training from score 0.635523\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[642]\tvalid_0's binary_logloss: 0.524985\n",
      "model2 Val Auc: 0.815\n",
      "[LightGBM] [Info] Number of positive: 1206497, number of negative: 639031\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19756\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845528, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653741 -> initscore=0.635523\n",
      "[LightGBM] [Info] Start training from score 0.635523\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[430]\tvalid_0's binary_logloss: 0.476926\n",
      "model6 Val Auc: 0.850\n",
      "[LightGBM] [Info] Number of positive: 1206497, number of negative: 639031\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20011\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845528, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653741 -> initscore=0.635523\n",
      "[LightGBM] [Info] Start training from score 0.635523\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[552]\tvalid_0's binary_logloss: 0.477389\n",
      "model71 Val Auc: 0.849\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 4 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845319, 39), (1845319, 1), (2016, 39), (2016, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1213592, number of negative: 631727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19221\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845319, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657660 -> initscore=0.652882\n",
      "[LightGBM] [Info] Start training from score 0.652882\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[581]\tvalid_0's binary_logloss: 0.532356\n",
      "model2 Val Auc: 0.808\n",
      "[LightGBM] [Info] Number of positive: 1213592, number of negative: 631727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19711\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845319, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657660 -> initscore=0.652882\n",
      "[LightGBM] [Info] Start training from score 0.652882\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[406]\tvalid_0's binary_logloss: 0.495462\n",
      "model6 Val Auc: 0.837\n",
      "[LightGBM] [Info] Number of positive: 1213592, number of negative: 631727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19966\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845319, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657660 -> initscore=0.652882\n",
      "[LightGBM] [Info] Start training from score 0.652882\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[714]\tvalid_0's binary_logloss: 0.496722\n",
      "model71 Val Auc: 0.835\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 5 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845636, 39), (1845636, 1), (1977, 39), (1977, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1203348, number of negative: 642288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19324\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845636, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.651996 -> initscore=0.627826\n",
      "[LightGBM] [Info] Start training from score 0.627826\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[679]\tvalid_0's binary_logloss: 0.540712\n",
      "model2 Val Auc: 0.801\n",
      "[LightGBM] [Info] Number of positive: 1203348, number of negative: 642288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19814\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845636, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.651996 -> initscore=0.627826\n",
      "[LightGBM] [Info] Start training from score 0.627826\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[543]\tvalid_0's binary_logloss: 0.490649\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1203348, number of negative: 642288\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20069\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845636, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.651996 -> initscore=0.627826\n",
      "[LightGBM] [Info] Start training from score 0.627826\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\tvalid_0's binary_logloss: 0.489833\n",
      "model71 Val Auc: 0.841\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 6 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845842, 39), (1845842, 1), (1992, 39), (1992, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209744, number of negative: 636098\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19283\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845842, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655389 -> initscore=0.642811\n",
      "[LightGBM] [Info] Start training from score 0.642811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[564]\tvalid_0's binary_logloss: 0.528752\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1209744, number of negative: 636098\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19773\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845842, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655389 -> initscore=0.642811\n",
      "[LightGBM] [Info] Start training from score 0.642811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[921]\tvalid_0's binary_logloss: 0.476145\n",
      "model6 Val Auc: 0.850\n",
      "[LightGBM] [Info] Number of positive: 1209744, number of negative: 636098\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20028\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845842, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655389 -> initscore=0.642811\n",
      "[LightGBM] [Info] Start training from score 0.642811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1093]\tvalid_0's binary_logloss: 0.473606\n",
      "model71 Val Auc: 0.852\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 7 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845872, 39), (1845872, 1), (1951, 39), (1951, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208582, number of negative: 637290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19318\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845872, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654749 -> initscore=0.639978\n",
      "[LightGBM] [Info] Start training from score 0.639978\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[816]\tvalid_0's binary_logloss: 0.532198\n",
      "model2 Val Auc: 0.806\n",
      "[LightGBM] [Info] Number of positive: 1208582, number of negative: 637290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19808\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845872, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654749 -> initscore=0.639978\n",
      "[LightGBM] [Info] Start training from score 0.639978\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[707]\tvalid_0's binary_logloss: 0.481973\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1208582, number of negative: 637290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20063\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845872, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654749 -> initscore=0.639978\n",
      "[LightGBM] [Info] Start training from score 0.639978\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[609]\tvalid_0's binary_logloss: 0.48209\n",
      "model71 Val Auc: 0.844\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 8 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845720, 39), (1845720, 1), (2000, 39), (2000, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204294, number of negative: 641426\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19259\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845720, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652479 -> initscore=0.629955\n",
      "[LightGBM] [Info] Start training from score 0.629955\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[568]\tvalid_0's binary_logloss: 0.53642\n",
      "model2 Val Auc: 0.804\n",
      "[LightGBM] [Info] Number of positive: 1204294, number of negative: 641426\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19749\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845720, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652479 -> initscore=0.629955\n",
      "[LightGBM] [Info] Start training from score 0.629955\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's binary_logloss: 0.49163\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1204294, number of negative: 641426\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20004\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845720, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652479 -> initscore=0.629955\n",
      "[LightGBM] [Info] Start training from score 0.629955\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[554]\tvalid_0's binary_logloss: 0.492313\n",
      "model71 Val Auc: 0.839\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 9 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845915, 39), (1845915, 1), (1940, 39), (1940, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210029, number of negative: 635886\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19343\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845915, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655517 -> initscore=0.643380\n",
      "[LightGBM] [Info] Start training from score 0.643380\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1004]\tvalid_0's binary_logloss: 0.524605\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1210029, number of negative: 635886\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19833\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845915, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655517 -> initscore=0.643380\n",
      "[LightGBM] [Info] Start training from score 0.643380\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[351]\tvalid_0's binary_logloss: 0.488714\n",
      "model6 Val Auc: 0.842\n",
      "[LightGBM] [Info] Number of positive: 1210029, number of negative: 635886\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20088\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845915, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655517 -> initscore=0.643380\n",
      "[LightGBM] [Info] Start training from score 0.643380\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[645]\tvalid_0's binary_logloss: 0.488554\n",
      "model71 Val Auc: 0.842\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 10 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845869, 39), (1845869, 1), (2036, 39), (2036, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208705, number of negative: 637164\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19280\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845869, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654816 -> initscore=0.640278\n",
      "[LightGBM] [Info] Start training from score 0.640278\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[336]\tvalid_0's binary_logloss: 0.534368\n",
      "model2 Val Auc: 0.807\n",
      "[LightGBM] [Info] Number of positive: 1208705, number of negative: 637164\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19770\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845869, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654816 -> initscore=0.640278\n",
      "[LightGBM] [Info] Start training from score 0.640278\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[564]\tvalid_0's binary_logloss: 0.48334\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1208705, number of negative: 637164\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20025\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845869, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654816 -> initscore=0.640278\n",
      "[LightGBM] [Info] Start training from score 0.640278\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[414]\tvalid_0's binary_logloss: 0.483335\n",
      "model71 Val Auc: 0.846\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 11 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845919, 39), (1845919, 1), (2006, 39), (2006, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206759, number of negative: 639160\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19257\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845919, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653744 -> initscore=0.635539\n",
      "[LightGBM] [Info] Start training from score 0.635539\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[852]\tvalid_0's binary_logloss: 0.520483\n",
      "model2 Val Auc: 0.816\n",
      "[LightGBM] [Info] Number of positive: 1206759, number of negative: 639160\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19747\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845919, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653744 -> initscore=0.635539\n",
      "[LightGBM] [Info] Start training from score 0.635539\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[720]\tvalid_0's binary_logloss: 0.482954\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1206759, number of negative: 639160\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20002\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845919, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653744 -> initscore=0.635539\n",
      "[LightGBM] [Info] Start training from score 0.635539\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[564]\tvalid_0's binary_logloss: 0.482558\n",
      "model71 Val Auc: 0.845\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 12 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845257, 39), (1845257, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208975, number of negative: 636282\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19241\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845257, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655180 -> initscore=0.641886\n",
      "[LightGBM] [Info] Start training from score 0.641886\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[860]\tvalid_0's binary_logloss: 0.533095\n",
      "model2 Val Auc: 0.805\n",
      "[LightGBM] [Info] Number of positive: 1208975, number of negative: 636282\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19731\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845257, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655180 -> initscore=0.641886\n",
      "[LightGBM] [Info] Start training from score 0.641886\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[635]\tvalid_0's binary_logloss: 0.490504\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1208975, number of negative: 636282\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19986\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845257, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655180 -> initscore=0.641886\n",
      "[LightGBM] [Info] Start training from score 0.641886\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[716]\tvalid_0's binary_logloss: 0.490877\n",
      "model71 Val Auc: 0.839\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 13 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845539, 39), (1845539, 1), (1974, 39), (1974, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\tvalid_0's binary_logloss: 0.544468\n",
      "model2 Val Auc: 0.797\n",
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19806\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[425]\tvalid_0's binary_logloss: 0.499722\n",
      "model6 Val Auc: 0.834\n",
      "[LightGBM] [Info] Number of positive: 1208276, number of negative: 637263\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20061\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845539, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654701 -> initscore=0.639767\n",
      "[LightGBM] [Info] Start training from score 0.639767\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[545]\tvalid_0's binary_logloss: 0.500358\n",
      "model71 Val Auc: 0.834\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 14 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845583, 39), (1845583, 1), (1991, 39), (1991, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206944, number of negative: 638639\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19270\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845583, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653964 -> initscore=0.636507\n",
      "[LightGBM] [Info] Start training from score 0.636507\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[636]\tvalid_0's binary_logloss: 0.528562\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1206944, number of negative: 638639\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19760\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845583, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653964 -> initscore=0.636507\n",
      "[LightGBM] [Info] Start training from score 0.636507\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[434]\tvalid_0's binary_logloss: 0.47962\n",
      "model6 Val Auc: 0.847\n",
      "[LightGBM] [Info] Number of positive: 1206944, number of negative: 638639\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20015\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845583, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653964 -> initscore=0.636507\n",
      "[LightGBM] [Info] Start training from score 0.636507\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[666]\tvalid_0's binary_logloss: 0.478432\n",
      "model71 Val Auc: 0.847\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 15 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845753, 39), (1845753, 1), (1975, 39), (1975, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205073, number of negative: 640680\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19287\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845753, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652890 -> initscore=0.631765\n",
      "[LightGBM] [Info] Start training from score 0.631765\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[915]\tvalid_0's binary_logloss: 0.536845\n",
      "model2 Val Auc: 0.806\n",
      "[LightGBM] [Info] Number of positive: 1205073, number of negative: 640680\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19777\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845753, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652890 -> initscore=0.631765\n",
      "[LightGBM] [Info] Start training from score 0.631765\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[609]\tvalid_0's binary_logloss: 0.490594\n",
      "model6 Val Auc: 0.842\n",
      "[LightGBM] [Info] Number of positive: 1205073, number of negative: 640680\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20032\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845753, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652890 -> initscore=0.631765\n",
      "[LightGBM] [Info] Start training from score 0.631765\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[522]\tvalid_0's binary_logloss: 0.491598\n",
      "model71 Val Auc: 0.841\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 16 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845890, 39), (1845890, 1), (1966, 39), (1966, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204859, number of negative: 641031\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19311\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845890, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652725 -> initscore=0.631040\n",
      "[LightGBM] [Info] Start training from score 0.631040\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[602]\tvalid_0's binary_logloss: 0.534157\n",
      "model2 Val Auc: 0.807\n",
      "[LightGBM] [Info] Number of positive: 1204859, number of negative: 641031\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19801\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845890, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652725 -> initscore=0.631040\n",
      "[LightGBM] [Info] Start training from score 0.631040\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[408]\tvalid_0's binary_logloss: 0.484874\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1204859, number of negative: 641031\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20056\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845890, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652725 -> initscore=0.631040\n",
      "[LightGBM] [Info] Start training from score 0.631040\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[363]\tvalid_0's binary_logloss: 0.484475\n",
      "model71 Val Auc: 0.846\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 17 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845409, 39), (1845409, 1), (1992, 39), (1992, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210497, number of negative: 634912\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19286\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845409, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655951 -> initscore=0.645300\n",
      "[LightGBM] [Info] Start training from score 0.645300\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[838]\tvalid_0's binary_logloss: 0.526817\n",
      "model2 Val Auc: 0.811\n",
      "[LightGBM] [Info] Number of positive: 1210497, number of negative: 634912\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19776\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845409, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655951 -> initscore=0.645300\n",
      "[LightGBM] [Info] Start training from score 0.645300\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[398]\tvalid_0's binary_logloss: 0.489957\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1210497, number of negative: 634912\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20031\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845409, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655951 -> initscore=0.645300\n",
      "[LightGBM] [Info] Start training from score 0.645300\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[391]\tvalid_0's binary_logloss: 0.489374\n",
      "model71 Val Auc: 0.840\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 18 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845711, 39), (1845711, 1), (2035, 39), (2035, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1213226, number of negative: 632485\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19230\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845711, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657322 -> initscore=0.651382\n",
      "[LightGBM] [Info] Start training from score 0.651382\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1032]\tvalid_0's binary_logloss: 0.53469\n",
      "model2 Val Auc: 0.804\n",
      "[LightGBM] [Info] Number of positive: 1213226, number of negative: 632485\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19720\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845711, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657322 -> initscore=0.651382\n",
      "[LightGBM] [Info] Start training from score 0.651382\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[374]\tvalid_0's binary_logloss: 0.487417\n",
      "model6 Val Auc: 0.841\n",
      "[LightGBM] [Info] Number of positive: 1213226, number of negative: 632485\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19975\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845711, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657322 -> initscore=0.651382\n",
      "[LightGBM] [Info] Start training from score 0.651382\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[510]\tvalid_0's binary_logloss: 0.48807\n",
      "model71 Val Auc: 0.841\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 19 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845937, 39), (1845937, 1), (2032, 39), (2032, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211726, number of negative: 634211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19249\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845937, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656429 -> initscore=0.647419\n",
      "[LightGBM] [Info] Start training from score 0.647419\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1094]\tvalid_0's binary_logloss: 0.525749\n",
      "model2 Val Auc: 0.811\n",
      "[LightGBM] [Info] Number of positive: 1211726, number of negative: 634211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19739\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845937, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656429 -> initscore=0.647419\n",
      "[LightGBM] [Info] Start training from score 0.647419\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[786]\tvalid_0's binary_logloss: 0.47774\n",
      "model6 Val Auc: 0.849\n",
      "[LightGBM] [Info] Number of positive: 1211726, number of negative: 634211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19994\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845937, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656429 -> initscore=0.647419\n",
      "[LightGBM] [Info] Start training from score 0.647419\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[799]\tvalid_0's binary_logloss: 0.475955\n",
      "model71 Val Auc: 0.851\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 20 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845954, 39), (1845954, 1), (1983, 39), (1983, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209141, number of negative: 636813\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19293\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845954, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655022 -> initscore=0.641189\n",
      "[LightGBM] [Info] Start training from score 0.641189\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1061]\tvalid_0's binary_logloss: 0.528091\n",
      "model2 Val Auc: 0.810\n",
      "[LightGBM] [Info] Number of positive: 1209141, number of negative: 636813\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19783\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845954, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655022 -> initscore=0.641189\n",
      "[LightGBM] [Info] Start training from score 0.641189\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[484]\tvalid_0's binary_logloss: 0.49278\n",
      "model6 Val Auc: 0.838\n",
      "[LightGBM] [Info] Number of positive: 1209141, number of negative: 636813\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20038\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845954, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655022 -> initscore=0.641189\n",
      "[LightGBM] [Info] Start training from score 0.641189\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[852]\tvalid_0's binary_logloss: 0.491798\n",
      "model71 Val Auc: 0.839\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 21 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845674, 39), (1845674, 1), (2027, 39), (2027, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206656, number of negative: 639018\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19271\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845674, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653775 -> initscore=0.635676\n",
      "[LightGBM] [Info] Start training from score 0.635676\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[875]\tvalid_0's binary_logloss: 0.532638\n",
      "model2 Val Auc: 0.806\n",
      "[LightGBM] [Info] Number of positive: 1206656, number of negative: 639018\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19761\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845674, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653775 -> initscore=0.635676\n",
      "[LightGBM] [Info] Start training from score 0.635676\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[350]\tvalid_0's binary_logloss: 0.484744\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1206656, number of negative: 639018\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20016\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845674, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653775 -> initscore=0.635676\n",
      "[LightGBM] [Info] Start training from score 0.635676\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[428]\tvalid_0's binary_logloss: 0.48257\n",
      "model71 Val Auc: 0.846\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 22 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845820, 39), (1845820, 1), (1981, 39), (1981, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208828, number of negative: 636992\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19281\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845820, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654900 -> initscore=0.640649\n",
      "[LightGBM] [Info] Start training from score 0.640649\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[652]\tvalid_0's binary_logloss: 0.539191\n",
      "model2 Val Auc: 0.801\n",
      "[LightGBM] [Info] Number of positive: 1208828, number of negative: 636992\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19771\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845820, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654900 -> initscore=0.640649\n",
      "[LightGBM] [Info] Start training from score 0.640649\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's binary_logloss: 0.496325\n",
      "model6 Val Auc: 0.835\n",
      "[LightGBM] [Info] Number of positive: 1208828, number of negative: 636992\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20026\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845820, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654900 -> initscore=0.640649\n",
      "[LightGBM] [Info] Start training from score 0.640649\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[616]\tvalid_0's binary_logloss: 0.495067\n",
      "model71 Val Auc: 0.836\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 23 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845978, 39), (1845978, 1), (1970, 39), (1970, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208587, number of negative: 637391\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19305\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845978, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654714 -> initscore=0.639824\n",
      "[LightGBM] [Info] Start training from score 0.639824\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[798]\tvalid_0's binary_logloss: 0.520826\n",
      "model2 Val Auc: 0.817\n",
      "[LightGBM] [Info] Number of positive: 1208587, number of negative: 637391\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19795\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845978, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654714 -> initscore=0.639824\n",
      "[LightGBM] [Info] Start training from score 0.639824\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[377]\tvalid_0's binary_logloss: 0.472681\n",
      "model6 Val Auc: 0.855\n",
      "[LightGBM] [Info] Number of positive: 1208587, number of negative: 637391\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20050\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845978, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654714 -> initscore=0.639824\n",
      "[LightGBM] [Info] Start training from score 0.639824\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[690]\tvalid_0's binary_logloss: 0.471854\n",
      "model71 Val Auc: 0.855\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 24 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845901, 39), (1845901, 1), (2044, 39), (2044, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208438, number of negative: 637463\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19213\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845901, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654660 -> initscore=0.639588\n",
      "[LightGBM] [Info] Start training from score 0.639588\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1351]\tvalid_0's binary_logloss: 0.521406\n",
      "model2 Val Auc: 0.814\n",
      "[LightGBM] [Info] Number of positive: 1208438, number of negative: 637463\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19703\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845901, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654660 -> initscore=0.639588\n",
      "[LightGBM] [Info] Start training from score 0.639588\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[681]\tvalid_0's binary_logloss: 0.48212\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1208438, number of negative: 637463\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19958\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845901, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654660 -> initscore=0.639588\n",
      "[LightGBM] [Info] Start training from score 0.639588\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[829]\tvalid_0's binary_logloss: 0.480436\n",
      "model71 Val Auc: 0.847\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 25 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845312, 39), (1845312, 1), (1966, 39), (1966, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208261, number of negative: 637051\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19307\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845312, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654773 -> initscore=0.640088\n",
      "[LightGBM] [Info] Start training from score 0.640088\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[600]\tvalid_0's binary_logloss: 0.529735\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1208261, number of negative: 637051\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19797\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845312, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654773 -> initscore=0.640088\n",
      "[LightGBM] [Info] Start training from score 0.640088\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1078]\tvalid_0's binary_logloss: 0.481947\n",
      "model6 Val Auc: 0.847\n",
      "[LightGBM] [Info] Number of positive: 1208261, number of negative: 637051\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20052\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845312, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654773 -> initscore=0.640088\n",
      "[LightGBM] [Info] Start training from score 0.640088\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1382]\tvalid_0's binary_logloss: 0.47992\n",
      "model71 Val Auc: 0.848\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 26 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1844865, 39), (1844865, 1), (2069, 39), (2069, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207442, number of negative: 637423\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19196\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844865, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654488 -> initscore=0.638826\n",
      "[LightGBM] [Info] Start training from score 0.638826\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[938]\tvalid_0's binary_logloss: 0.524123\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1207442, number of negative: 637423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19686\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844865, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654488 -> initscore=0.638826\n",
      "[LightGBM] [Info] Start training from score 0.638826\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[327]\tvalid_0's binary_logloss: 0.48273\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1207442, number of negative: 637423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19941\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844865, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654488 -> initscore=0.638826\n",
      "[LightGBM] [Info] Start training from score 0.638826\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[572]\tvalid_0's binary_logloss: 0.48176\n",
      "model71 Val Auc: 0.847\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 27 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845900, 39), (1845900, 1), (2019, 39), (2019, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1213485, number of negative: 632415\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19247\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845900, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657395 -> initscore=0.651706\n",
      "[LightGBM] [Info] Start training from score 0.651706\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[883]\tvalid_0's binary_logloss: 0.533255\n",
      "model2 Val Auc: 0.807\n",
      "[LightGBM] [Info] Number of positive: 1213485, number of negative: 632415\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19737\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845900, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657395 -> initscore=0.651706\n",
      "[LightGBM] [Info] Start training from score 0.651706\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\tvalid_0's binary_logloss: 0.491994\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1213485, number of negative: 632415\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19992\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845900, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657395 -> initscore=0.651706\n",
      "[LightGBM] [Info] Start training from score 0.651706\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[558]\tvalid_0's binary_logloss: 0.488774\n",
      "model71 Val Auc: 0.842\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 28 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845851, 39), (1845851, 1), (2003, 39), (2003, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1201478, number of negative: 644373\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19298\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845851, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650907 -> initscore=0.623030\n",
      "[LightGBM] [Info] Start training from score 0.623030\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[726]\tvalid_0's binary_logloss: 0.53551\n",
      "model2 Val Auc: 0.805\n",
      "[LightGBM] [Info] Number of positive: 1201478, number of negative: 644373\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19788\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845851, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650907 -> initscore=0.623030\n",
      "[LightGBM] [Info] Start training from score 0.623030\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[730]\tvalid_0's binary_logloss: 0.48987\n",
      "model6 Val Auc: 0.842\n",
      "[LightGBM] [Info] Number of positive: 1201478, number of negative: 644373\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20043\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845851, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650907 -> initscore=0.623030\n",
      "[LightGBM] [Info] Start training from score 0.623030\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[700]\tvalid_0's binary_logloss: 0.487943\n",
      "model71 Val Auc: 0.843\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 29 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845822, 39), (1845822, 1), (2007, 39), (2007, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207603, number of negative: 638219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19294\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845822, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654236 -> initscore=0.637711\n",
      "[LightGBM] [Info] Start training from score 0.637711\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.536371\n",
      "model2 Val Auc: 0.806\n",
      "[LightGBM] [Info] Number of positive: 1207603, number of negative: 638219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19784\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845822, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654236 -> initscore=0.637711\n",
      "[LightGBM] [Info] Start training from score 0.637711\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[613]\tvalid_0's binary_logloss: 0.494607\n",
      "model6 Val Auc: 0.839\n",
      "[LightGBM] [Info] Number of positive: 1207603, number of negative: 638219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20039\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845822, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654236 -> initscore=0.637711\n",
      "[LightGBM] [Info] Start training from score 0.637711\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[376]\tvalid_0's binary_logloss: 0.494463\n",
      "model71 Val Auc: 0.839\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 30 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845891, 39), (1845891, 1), (1996, 39), (1996, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205186, number of negative: 640705\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19270\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845891, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652902 -> initscore=0.631820\n",
      "[LightGBM] [Info] Start training from score 0.631820\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[589]\tvalid_0's binary_logloss: 0.545092\n",
      "model2 Val Auc: 0.795\n",
      "[LightGBM] [Info] Number of positive: 1205186, number of negative: 640705\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19760\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845891, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652902 -> initscore=0.631820\n",
      "[LightGBM] [Info] Start training from score 0.631820\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[529]\tvalid_0's binary_logloss: 0.497476\n",
      "model6 Val Auc: 0.835\n",
      "[LightGBM] [Info] Number of positive: 1205186, number of negative: 640705\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20015\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845891, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652902 -> initscore=0.631820\n",
      "[LightGBM] [Info] Start training from score 0.631820\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[363]\tvalid_0's binary_logloss: 0.498387\n",
      "model71 Val Auc: 0.834\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 31 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845542, 39), (1845542, 1), (1987, 39), (1987, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206721, number of negative: 638821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845542, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653857 -> initscore=0.636038\n",
      "[LightGBM] [Info] Start training from score 0.636038\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1251]\tvalid_0's binary_logloss: 0.519899\n",
      "model2 Val Auc: 0.817\n",
      "[LightGBM] [Info] Number of positive: 1206721, number of negative: 638821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19765\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845542, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653857 -> initscore=0.636038\n",
      "[LightGBM] [Info] Start training from score 0.636038\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[601]\tvalid_0's binary_logloss: 0.492241\n",
      "model6 Val Auc: 0.841\n",
      "[LightGBM] [Info] Number of positive: 1206721, number of negative: 638821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20020\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845542, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653857 -> initscore=0.636038\n",
      "[LightGBM] [Info] Start training from score 0.636038\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[748]\tvalid_0's binary_logloss: 0.492784\n",
      "model71 Val Auc: 0.840\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 32 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845146, 39), (1845146, 1), (1920, 39), (1920, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211445, number of negative: 633701\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19348\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845146, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656558 -> initscore=0.647992\n",
      "[LightGBM] [Info] Start training from score 0.647992\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[649]\tvalid_0's binary_logloss: 0.516279\n",
      "model2 Val Auc: 0.821\n",
      "[LightGBM] [Info] Number of positive: 1211445, number of negative: 633701\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19838\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845146, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656558 -> initscore=0.647992\n",
      "[LightGBM] [Info] Start training from score 0.647992\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[497]\tvalid_0's binary_logloss: 0.47101\n",
      "model6 Val Auc: 0.854\n",
      "[LightGBM] [Info] Number of positive: 1211445, number of negative: 633701\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20093\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845146, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656558 -> initscore=0.647992\n",
      "[LightGBM] [Info] Start training from score 0.647992\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[497]\tvalid_0's binary_logloss: 0.470396\n",
      "model71 Val Auc: 0.855\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 33 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845805, 39), (1845805, 1), (2007, 39), (2007, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211416, number of negative: 634389\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19253\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845805, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656308 -> initscore=0.646883\n",
      "[LightGBM] [Info] Start training from score 0.646883\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[695]\tvalid_0's binary_logloss: 0.52263\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1211416, number of negative: 634389\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19743\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845805, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656308 -> initscore=0.646883\n",
      "[LightGBM] [Info] Start training from score 0.646883\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[787]\tvalid_0's binary_logloss: 0.474483\n",
      "model6 Val Auc: 0.850\n",
      "[LightGBM] [Info] Number of positive: 1211416, number of negative: 634389\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19998\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845805, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656308 -> initscore=0.646883\n",
      "[LightGBM] [Info] Start training from score 0.646883\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\tvalid_0's binary_logloss: 0.475507\n",
      "model71 Val Auc: 0.850\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 34 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845771, 39), (1845771, 1), (2020, 39), (2020, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207993, number of negative: 637778\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19226\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845771, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654465 -> initscore=0.638725\n",
      "[LightGBM] [Info] Start training from score 0.638725\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[540]\tvalid_0's binary_logloss: 0.532763\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1207993, number of negative: 637778\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19716\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845771, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654465 -> initscore=0.638725\n",
      "[LightGBM] [Info] Start training from score 0.638725\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's binary_logloss: 0.483603\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1207993, number of negative: 637778\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19971\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845771, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654465 -> initscore=0.638725\n",
      "[LightGBM] [Info] Start training from score 0.638725\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[508]\tvalid_0's binary_logloss: 0.483458\n",
      "model71 Val Auc: 0.847\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 35 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845977, 39), (1845977, 1), (1947, 39), (1947, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209559, number of negative: 636418\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19344\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845977, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655241 -> initscore=0.642156\n",
      "[LightGBM] [Info] Start training from score 0.642156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[633]\tvalid_0's binary_logloss: 0.542738\n",
      "model2 Val Auc: 0.798\n",
      "[LightGBM] [Info] Number of positive: 1209559, number of negative: 636418\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845977, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655241 -> initscore=0.642156\n",
      "[LightGBM] [Info] Start training from score 0.642156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[351]\tvalid_0's binary_logloss: 0.497162\n",
      "model6 Val Auc: 0.835\n",
      "[LightGBM] [Info] Number of positive: 1209559, number of negative: 636418\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20089\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845977, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655241 -> initscore=0.642156\n",
      "[LightGBM] [Info] Start training from score 0.642156\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[591]\tvalid_0's binary_logloss: 0.498262\n",
      "model71 Val Auc: 0.834\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 36 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845739, 39), (1845739, 1), (2013, 39), (2013, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204410, number of negative: 641329\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19272\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652535 -> initscore=0.630203\n",
      "[LightGBM] [Info] Start training from score 0.630203\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\tvalid_0's binary_logloss: 0.535851\n",
      "model2 Val Auc: 0.805\n",
      "[LightGBM] [Info] Number of positive: 1204410, number of negative: 641329\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19762\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652535 -> initscore=0.630203\n",
      "[LightGBM] [Info] Start training from score 0.630203\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1027]\tvalid_0's binary_logloss: 0.490832\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1204410, number of negative: 641329\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20017\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652535 -> initscore=0.630203\n",
      "[LightGBM] [Info] Start training from score 0.630203\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1011]\tvalid_0's binary_logloss: 0.489768\n",
      "model71 Val Auc: 0.841\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 37 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845868, 39), (1845868, 1), (1990, 39), (1990, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209502, number of negative: 636366\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19288\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845868, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655248 -> initscore=0.642190\n",
      "[LightGBM] [Info] Start training from score 0.642190\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[847]\tvalid_0's binary_logloss: 0.53164\n",
      "model2 Val Auc: 0.806\n",
      "[LightGBM] [Info] Number of positive: 1209502, number of negative: 636366\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19778\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845868, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655248 -> initscore=0.642190\n",
      "[LightGBM] [Info] Start training from score 0.642190\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[776]\tvalid_0's binary_logloss: 0.483792\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1209502, number of negative: 636366\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20033\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845868, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655248 -> initscore=0.642190\n",
      "[LightGBM] [Info] Start training from score 0.642190\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[581]\tvalid_0's binary_logloss: 0.483065\n",
      "model71 Val Auc: 0.845\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 38 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845887, 39), (1845887, 1), (2043, 39), (2043, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1212391, number of negative: 633496\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19236\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845887, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656807 -> initscore=0.649096\n",
      "[LightGBM] [Info] Start training from score 0.649096\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[829]\tvalid_0's binary_logloss: 0.526048\n",
      "model2 Val Auc: 0.812\n",
      "[LightGBM] [Info] Number of positive: 1212391, number of negative: 633496\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19726\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845887, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656807 -> initscore=0.649096\n",
      "[LightGBM] [Info] Start training from score 0.649096\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[627]\tvalid_0's binary_logloss: 0.47966\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1212391, number of negative: 633496\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19981\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845887, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656807 -> initscore=0.649096\n",
      "[LightGBM] [Info] Start training from score 0.649096\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1267]\tvalid_0's binary_logloss: 0.476376\n",
      "model71 Val Auc: 0.848\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 39 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845699, 39), (1845699, 1), (1977, 39), (1977, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205979, number of negative: 639720\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19296\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845699, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653400 -> initscore=0.634016\n",
      "[LightGBM] [Info] Start training from score 0.634016\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.533223\n",
      "model2 Val Auc: 0.806\n",
      "[LightGBM] [Info] Number of positive: 1205979, number of negative: 639720\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19786\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845699, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653400 -> initscore=0.634016\n",
      "[LightGBM] [Info] Start training from score 0.634016\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[740]\tvalid_0's binary_logloss: 0.48582\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1205979, number of negative: 639720\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20041\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845699, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653400 -> initscore=0.634016\n",
      "[LightGBM] [Info] Start training from score 0.634016\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[636]\tvalid_0's binary_logloss: 0.485147\n",
      "model71 Val Auc: 0.845\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 40 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845922, 39), (1845922, 1), (2042, 39), (2042, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210772, number of negative: 635150\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19243\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845922, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655917 -> initscore=0.645152\n",
      "[LightGBM] [Info] Start training from score 0.645152\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[443]\tvalid_0's binary_logloss: 0.526731\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1210772, number of negative: 635150\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19733\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845922, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655917 -> initscore=0.645152\n",
      "[LightGBM] [Info] Start training from score 0.645152\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[732]\tvalid_0's binary_logloss: 0.464591\n",
      "model6 Val Auc: 0.859\n",
      "[LightGBM] [Info] Number of positive: 1210772, number of negative: 635150\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19988\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845922, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655917 -> initscore=0.645152\n",
      "[LightGBM] [Info] Start training from score 0.645152\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[788]\tvalid_0's binary_logloss: 0.463296\n",
      "model71 Val Auc: 0.860\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 41 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845652, 39), (1845652, 1), (2037, 39), (2037, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1212937, number of negative: 632715\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19229\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845652, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657186 -> initscore=0.650780\n",
      "[LightGBM] [Info] Start training from score 0.650780\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1022]\tvalid_0's binary_logloss: 0.539056\n",
      "model2 Val Auc: 0.800\n",
      "[LightGBM] [Info] Number of positive: 1212937, number of negative: 632715\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19719\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845652, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657186 -> initscore=0.650780\n",
      "[LightGBM] [Info] Start training from score 0.650780\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[421]\tvalid_0's binary_logloss: 0.499196\n",
      "model6 Val Auc: 0.834\n",
      "[LightGBM] [Info] Number of positive: 1212937, number of negative: 632715\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19974\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845652, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657186 -> initscore=0.650780\n",
      "[LightGBM] [Info] Start training from score 0.650780\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[369]\tvalid_0's binary_logloss: 0.499994\n",
      "model71 Val Auc: 0.833\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 42 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845527, 39), (1845527, 1), (2007, 39), (2007, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209948, number of negative: 635579\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19229\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845527, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655611 -> initscore=0.643796\n",
      "[LightGBM] [Info] Start training from score 0.643796\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[558]\tvalid_0's binary_logloss: 0.52991\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1209948, number of negative: 635579\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19719\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845527, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655611 -> initscore=0.643796\n",
      "[LightGBM] [Info] Start training from score 0.643796\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[591]\tvalid_0's binary_logloss: 0.48267\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1209948, number of negative: 635579\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19974\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845527, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655611 -> initscore=0.643796\n",
      "[LightGBM] [Info] Start training from score 0.643796\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[729]\tvalid_0's binary_logloss: 0.481034\n",
      "model71 Val Auc: 0.847\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 43 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845922, 39), (1845922, 1), (1993, 39), (1993, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205958, number of negative: 639964\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19277\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845922, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653309 -> initscore=0.633618\n",
      "[LightGBM] [Info] Start training from score 0.633618\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[604]\tvalid_0's binary_logloss: 0.533884\n",
      "model2 Val Auc: 0.804\n",
      "[LightGBM] [Info] Number of positive: 1205958, number of negative: 639964\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19767\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845922, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653309 -> initscore=0.633618\n",
      "[LightGBM] [Info] Start training from score 0.633618\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\tvalid_0's binary_logloss: 0.491217\n",
      "model6 Val Auc: 0.838\n",
      "[LightGBM] [Info] Number of positive: 1205958, number of negative: 639964\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20022\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845922, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653309 -> initscore=0.633618\n",
      "[LightGBM] [Info] Start training from score 0.633618\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[761]\tvalid_0's binary_logloss: 0.488351\n",
      "model71 Val Auc: 0.840\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 44 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845794, 39), (1845794, 1), (1961, 39), (1961, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208300, number of negative: 637494\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19306\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845794, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654623 -> initscore=0.639425\n",
      "[LightGBM] [Info] Start training from score 0.639425\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[753]\tvalid_0's binary_logloss: 0.516833\n",
      "model2 Val Auc: 0.823\n",
      "[LightGBM] [Info] Number of positive: 1208300, number of negative: 637494\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19796\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845794, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654623 -> initscore=0.639425\n",
      "[LightGBM] [Info] Start training from score 0.639425\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[542]\tvalid_0's binary_logloss: 0.476104\n",
      "model6 Val Auc: 0.853\n",
      "[LightGBM] [Info] Number of positive: 1208300, number of negative: 637494\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20051\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845794, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654623 -> initscore=0.639425\n",
      "[LightGBM] [Info] Start training from score 0.639425\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[574]\tvalid_0's binary_logloss: 0.474645\n",
      "model71 Val Auc: 0.854\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 45 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845500, 39), (1845500, 1), (2016, 39), (2016, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210144, number of negative: 635356\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19258\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845500, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655727 -> initscore=0.644309\n",
      "[LightGBM] [Info] Start training from score 0.644309\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1268]\tvalid_0's binary_logloss: 0.52766\n",
      "model2 Val Auc: 0.812\n",
      "[LightGBM] [Info] Number of positive: 1210144, number of negative: 635356\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19748\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845500, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655727 -> initscore=0.644309\n",
      "[LightGBM] [Info] Start training from score 0.644309\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[759]\tvalid_0's binary_logloss: 0.484501\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1210144, number of negative: 635356\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20003\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845500, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655727 -> initscore=0.644309\n",
      "[LightGBM] [Info] Start training from score 0.644309\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[851]\tvalid_0's binary_logloss: 0.482593\n",
      "model71 Val Auc: 0.846\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 46 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845974, 39), (1845974, 1), (1944, 39), (1944, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209901, number of negative: 636073\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19309\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845974, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655427 -> initscore=0.642980\n",
      "[LightGBM] [Info] Start training from score 0.642980\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[904]\tvalid_0's binary_logloss: 0.546054\n",
      "model2 Val Auc: 0.795\n",
      "[LightGBM] [Info] Number of positive: 1209901, number of negative: 636073\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19799\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845974, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655427 -> initscore=0.642980\n",
      "[LightGBM] [Info] Start training from score 0.642980\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[411]\tvalid_0's binary_logloss: 0.508883\n",
      "model6 Val Auc: 0.825\n",
      "[LightGBM] [Info] Number of positive: 1209901, number of negative: 636073\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20054\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845974, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655427 -> initscore=0.642980\n",
      "[LightGBM] [Info] Start training from score 0.642980\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[730]\tvalid_0's binary_logloss: 0.508585\n",
      "model71 Val Auc: 0.826\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 47 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845521, 39), (1845521, 1), (2060, 39), (2060, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210416, number of negative: 635105\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19224\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845521, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655867 -> initscore=0.644929\n",
      "[LightGBM] [Info] Start training from score 0.644929\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[773]\tvalid_0's binary_logloss: 0.535615\n",
      "model2 Val Auc: 0.804\n",
      "[LightGBM] [Info] Number of positive: 1210416, number of negative: 635105\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19714\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845521, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655867 -> initscore=0.644929\n",
      "[LightGBM] [Info] Start training from score 0.644929\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[512]\tvalid_0's binary_logloss: 0.49113\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1210416, number of negative: 635105\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19969\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845521, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655867 -> initscore=0.644929\n",
      "[LightGBM] [Info] Start training from score 0.644929\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[514]\tvalid_0's binary_logloss: 0.491255\n",
      "model71 Val Auc: 0.840\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 48 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845500, 39), (1845500, 1), (1995, 39), (1995, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208061, number of negative: 637439\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19272\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845500, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654598 -> initscore=0.639313\n",
      "[LightGBM] [Info] Start training from score 0.639313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[714]\tvalid_0's binary_logloss: 0.530679\n",
      "model2 Val Auc: 0.808\n",
      "[LightGBM] [Info] Number of positive: 1208061, number of negative: 637439\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19762\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845500, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654598 -> initscore=0.639313\n",
      "[LightGBM] [Info] Start training from score 0.639313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's binary_logloss: 0.485606\n",
      "model6 Val Auc: 0.843\n",
      "[LightGBM] [Info] Number of positive: 1208061, number of negative: 637439\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20017\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845500, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654598 -> initscore=0.639313\n",
      "[LightGBM] [Info] Start training from score 0.639313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\tvalid_0's binary_logloss: 0.485283\n",
      "model71 Val Auc: 0.843\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 49 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845797, 39), (1845797, 1), (2000, 39), (2000, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204139, number of negative: 641658\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19289\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845797, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652368 -> initscore=0.629465\n",
      "[LightGBM] [Info] Start training from score 0.629465\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[960]\tvalid_0's binary_logloss: 0.525903\n",
      "model2 Val Auc: 0.812\n",
      "[LightGBM] [Info] Number of positive: 1204139, number of negative: 641658\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19779\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845797, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652368 -> initscore=0.629465\n",
      "[LightGBM] [Info] Start training from score 0.629465\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[844]\tvalid_0's binary_logloss: 0.481572\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1204139, number of negative: 641658\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20034\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845797, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652368 -> initscore=0.629465\n",
      "[LightGBM] [Info] Start training from score 0.629465\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[726]\tvalid_0's binary_logloss: 0.481001\n",
      "model71 Val Auc: 0.846\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 50 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845470, 39), (1845470, 1), (2073, 39), (2073, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206350, number of negative: 639120\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19245\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845470, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653682 -> initscore=0.635262\n",
      "[LightGBM] [Info] Start training from score 0.635262\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[927]\tvalid_0's binary_logloss: 0.526986\n",
      "model2 Val Auc: 0.811\n",
      "[LightGBM] [Info] Number of positive: 1206350, number of negative: 639120\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19735\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845470, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653682 -> initscore=0.635262\n",
      "[LightGBM] [Info] Start training from score 0.635262\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[495]\tvalid_0's binary_logloss: 0.488659\n",
      "model6 Val Auc: 0.841\n",
      "[LightGBM] [Info] Number of positive: 1206350, number of negative: 639120\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19990\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845470, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653682 -> initscore=0.635262\n",
      "[LightGBM] [Info] Start training from score 0.635262\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[472]\tvalid_0's binary_logloss: 0.488974\n",
      "model71 Val Auc: 0.841\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 51 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845514, 39), (1845514, 1), (1955, 39), (1955, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208789, number of negative: 636725\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845514, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654988 -> initscore=0.641036\n",
      "[LightGBM] [Info] Start training from score 0.641036\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[815]\tvalid_0's binary_logloss: 0.5138\n",
      "model2 Val Auc: 0.823\n",
      "[LightGBM] [Info] Number of positive: 1208789, number of negative: 636725\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19806\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845514, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654988 -> initscore=0.641036\n",
      "[LightGBM] [Info] Start training from score 0.641036\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[445]\tvalid_0's binary_logloss: 0.481729\n",
      "model6 Val Auc: 0.847\n",
      "[LightGBM] [Info] Number of positive: 1208789, number of negative: 636725\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20061\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845514, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654988 -> initscore=0.641036\n",
      "[LightGBM] [Info] Start training from score 0.641036\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[833]\tvalid_0's binary_logloss: 0.481617\n",
      "model71 Val Auc: 0.848\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 52 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845303, 39), (1845303, 1), (1996, 39), (1996, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207262, number of negative: 638041\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19284\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845303, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654235 -> initscore=0.637708\n",
      "[LightGBM] [Info] Start training from score 0.637708\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.512973\n",
      "model2 Val Auc: 0.822\n",
      "[LightGBM] [Info] Number of positive: 1207262, number of negative: 638041\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19774\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845303, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654235 -> initscore=0.637708\n",
      "[LightGBM] [Info] Start training from score 0.637708\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[529]\tvalid_0's binary_logloss: 0.467874\n",
      "model6 Val Auc: 0.855\n",
      "[LightGBM] [Info] Number of positive: 1207262, number of negative: 638041\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20029\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845303, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654235 -> initscore=0.637708\n",
      "[LightGBM] [Info] Start training from score 0.637708\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[659]\tvalid_0's binary_logloss: 0.467602\n",
      "model71 Val Auc: 0.855\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 53 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845978, 39), (1845978, 1), (2058, 39), (2058, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208157, number of negative: 637821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845978, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654481 -> initscore=0.638794\n",
      "[LightGBM] [Info] Start training from score 0.638794\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[711]\tvalid_0's binary_logloss: 0.5319\n",
      "model2 Val Auc: 0.808\n",
      "[LightGBM] [Info] Number of positive: 1208157, number of negative: 637821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19718\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845978, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654481 -> initscore=0.638794\n",
      "[LightGBM] [Info] Start training from score 0.638794\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[509]\tvalid_0's binary_logloss: 0.483665\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1208157, number of negative: 637821\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19973\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845978, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654481 -> initscore=0.638794\n",
      "[LightGBM] [Info] Start training from score 0.638794\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\tvalid_0's binary_logloss: 0.484334\n",
      "model71 Val Auc: 0.844\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 54 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845753, 39), (1845753, 1), (2076, 39), (2076, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209669, number of negative: 636084\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19204\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845753, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655380 -> initscore=0.642771\n",
      "[LightGBM] [Info] Start training from score 0.642771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's binary_logloss: 0.539966\n",
      "model2 Val Auc: 0.800\n",
      "[LightGBM] [Info] Number of positive: 1209669, number of negative: 636084\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19694\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845753, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655380 -> initscore=0.642771\n",
      "[LightGBM] [Info] Start training from score 0.642771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[658]\tvalid_0's binary_logloss: 0.496142\n",
      "model6 Val Auc: 0.836\n",
      "[LightGBM] [Info] Number of positive: 1209669, number of negative: 636084\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19949\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845753, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655380 -> initscore=0.642771\n",
      "[LightGBM] [Info] Start training from score 0.642771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[828]\tvalid_0's binary_logloss: 0.496992\n",
      "model71 Val Auc: 0.836\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 55 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845855, 39), (1845855, 1), (2033, 39), (2033, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1213621, number of negative: 632234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19243\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845855, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657484 -> initscore=0.652104\n",
      "[LightGBM] [Info] Start training from score 0.652104\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\tvalid_0's binary_logloss: 0.538484\n",
      "model2 Val Auc: 0.801\n",
      "[LightGBM] [Info] Number of positive: 1213621, number of negative: 632234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19733\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845855, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657484 -> initscore=0.652104\n",
      "[LightGBM] [Info] Start training from score 0.652104\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[463]\tvalid_0's binary_logloss: 0.489193\n",
      "model6 Val Auc: 0.840\n",
      "[LightGBM] [Info] Number of positive: 1213621, number of negative: 632234\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19988\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845855, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657484 -> initscore=0.652104\n",
      "[LightGBM] [Info] Start training from score 0.652104\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[618]\tvalid_0's binary_logloss: 0.489822\n",
      "model71 Val Auc: 0.840\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 56 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845418, 39), (1845418, 1), (2015, 39), (2015, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206995, number of negative: 638423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19249\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845418, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654050 -> initscore=0.636888\n",
      "[LightGBM] [Info] Start training from score 0.636888\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's binary_logloss: 0.528328\n",
      "model2 Val Auc: 0.810\n",
      "[LightGBM] [Info] Number of positive: 1206995, number of negative: 638423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19739\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845418, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654050 -> initscore=0.636888\n",
      "[LightGBM] [Info] Start training from score 0.636888\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[300]\tvalid_0's binary_logloss: 0.481338\n",
      "model6 Val Auc: 0.847\n",
      "[LightGBM] [Info] Number of positive: 1206995, number of negative: 638423\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19994\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845418, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654050 -> initscore=0.636888\n",
      "[LightGBM] [Info] Start training from score 0.636888\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[512]\tvalid_0's binary_logloss: 0.479024\n",
      "model71 Val Auc: 0.849\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 57 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845562, 39), (1845562, 1), (1972, 39), (1972, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208965, number of negative: 636597\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19294\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845562, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655066 -> initscore=0.641383\n",
      "[LightGBM] [Info] Start training from score 0.641383\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[494]\tvalid_0's binary_logloss: 0.532757\n",
      "model2 Val Auc: 0.807\n",
      "[LightGBM] [Info] Number of positive: 1208965, number of negative: 636597\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19784\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845562, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655066 -> initscore=0.641383\n",
      "[LightGBM] [Info] Start training from score 0.641383\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[502]\tvalid_0's binary_logloss: 0.483301\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1208965, number of negative: 636597\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20039\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845562, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655066 -> initscore=0.641383\n",
      "[LightGBM] [Info] Start training from score 0.641383\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1039]\tvalid_0's binary_logloss: 0.481379\n",
      "model71 Val Auc: 0.847\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 58 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845929, 39), (1845929, 1), (2079, 39), (2079, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205385, number of negative: 640544\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19179\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845929, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652996 -> initscore=0.632236\n",
      "[LightGBM] [Info] Start training from score 0.632236\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1332]\tvalid_0's binary_logloss: 0.518993\n",
      "model2 Val Auc: 0.818\n",
      "[LightGBM] [Info] Number of positive: 1205385, number of negative: 640544\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19669\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845929, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652996 -> initscore=0.632236\n",
      "[LightGBM] [Info] Start training from score 0.632236\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[524]\tvalid_0's binary_logloss: 0.489085\n",
      "model6 Val Auc: 0.841\n",
      "[LightGBM] [Info] Number of positive: 1205385, number of negative: 640544\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19924\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845929, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652996 -> initscore=0.632236\n",
      "[LightGBM] [Info] Start training from score 0.632236\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[546]\tvalid_0's binary_logloss: 0.489253\n",
      "model71 Val Auc: 0.841\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 59 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845923, 39), (1845923, 1), (1994, 39), (1994, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208712, number of negative: 637211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19248\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845923, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654801 -> initscore=0.640210\n",
      "[LightGBM] [Info] Start training from score 0.640210\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1123]\tvalid_0's binary_logloss: 0.525709\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1208712, number of negative: 637211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19738\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845923, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654801 -> initscore=0.640210\n",
      "[LightGBM] [Info] Start training from score 0.640210\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[570]\tvalid_0's binary_logloss: 0.487832\n",
      "model6 Val Auc: 0.843\n",
      "[LightGBM] [Info] Number of positive: 1208712, number of negative: 637211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19993\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845923, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654801 -> initscore=0.640210\n",
      "[LightGBM] [Info] Start training from score 0.640210\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[467]\tvalid_0's binary_logloss: 0.488387\n",
      "model71 Val Auc: 0.842\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 60 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1844831, 39), (1844831, 1), (2036, 39), (2036, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209632, number of negative: 635199\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19247\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844831, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655687 -> initscore=0.644133\n",
      "[LightGBM] [Info] Start training from score 0.644133\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[636]\tvalid_0's binary_logloss: 0.536773\n",
      "model2 Val Auc: 0.805\n",
      "[LightGBM] [Info] Number of positive: 1209632, number of negative: 635199\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19737\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844831, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655687 -> initscore=0.644133\n",
      "[LightGBM] [Info] Start training from score 0.644133\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[333]\tvalid_0's binary_logloss: 0.494757\n",
      "model6 Val Auc: 0.837\n",
      "[LightGBM] [Info] Number of positive: 1209632, number of negative: 635199\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19992\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844831, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655687 -> initscore=0.644133\n",
      "[LightGBM] [Info] Start training from score 0.644133\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[458]\tvalid_0's binary_logloss: 0.495095\n",
      "model71 Val Auc: 0.837\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 61 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845846, 39), (1845846, 1), (1945, 39), (1945, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207410, number of negative: 638436\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19329\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845846, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654123 -> initscore=0.637211\n",
      "[LightGBM] [Info] Start training from score 0.637211\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[611]\tvalid_0's binary_logloss: 0.523162\n",
      "model2 Val Auc: 0.817\n",
      "[LightGBM] [Info] Number of positive: 1207410, number of negative: 638436\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19819\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845846, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654123 -> initscore=0.637211\n",
      "[LightGBM] [Info] Start training from score 0.637211\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[721]\tvalid_0's binary_logloss: 0.48034\n",
      "model6 Val Auc: 0.847\n",
      "[LightGBM] [Info] Number of positive: 1207410, number of negative: 638436\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20074\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845846, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654123 -> initscore=0.637211\n",
      "[LightGBM] [Info] Start training from score 0.637211\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[683]\tvalid_0's binary_logloss: 0.482042\n",
      "model71 Val Auc: 0.846\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 62 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845429, 39), (1845429, 1), (1987, 39), (1987, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205781, number of negative: 639648\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19281\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845429, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653388 -> initscore=0.633965\n",
      "[LightGBM] [Info] Start training from score 0.633965\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[697]\tvalid_0's binary_logloss: 0.539608\n",
      "model2 Val Auc: 0.801\n",
      "[LightGBM] [Info] Number of positive: 1205781, number of negative: 639648\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19771\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845429, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653388 -> initscore=0.633965\n",
      "[LightGBM] [Info] Start training from score 0.633965\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[387]\tvalid_0's binary_logloss: 0.495316\n",
      "model6 Val Auc: 0.837\n",
      "[LightGBM] [Info] Number of positive: 1205781, number of negative: 639648\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20026\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845429, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653388 -> initscore=0.633965\n",
      "[LightGBM] [Info] Start training from score 0.633965\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1057]\tvalid_0's binary_logloss: 0.494346\n",
      "model71 Val Auc: 0.839\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 63 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845700, 39), (1845700, 1), (1990, 39), (1990, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209634, number of negative: 636066\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19279\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845700, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655380 -> initscore=0.642771\n",
      "[LightGBM] [Info] Start training from score 0.642771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1114]\tvalid_0's binary_logloss: 0.531611\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1209634, number of negative: 636066\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19769\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845700, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655380 -> initscore=0.642771\n",
      "[LightGBM] [Info] Start training from score 0.642771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[663]\tvalid_0's binary_logloss: 0.502411\n",
      "model6 Val Auc: 0.832\n",
      "[LightGBM] [Info] Number of positive: 1209634, number of negative: 636066\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20024\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845700, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655380 -> initscore=0.642771\n",
      "[LightGBM] [Info] Start training from score 0.642771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[597]\tvalid_0's binary_logloss: 0.50091\n",
      "model71 Val Auc: 0.833\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 64 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845876, 39), (1845876, 1), (2050, 39), (2050, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1203629, number of negative: 642247\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19235\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845876, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652064 -> initscore=0.628123\n",
      "[LightGBM] [Info] Start training from score 0.628123\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1002]\tvalid_0's binary_logloss: 0.529765\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1203629, number of negative: 642247\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19725\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845876, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652064 -> initscore=0.628123\n",
      "[LightGBM] [Info] Start training from score 0.628123\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[621]\tvalid_0's binary_logloss: 0.479371\n",
      "model6 Val Auc: 0.849\n",
      "[LightGBM] [Info] Number of positive: 1203629, number of negative: 642247\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19980\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845876, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652064 -> initscore=0.628123\n",
      "[LightGBM] [Info] Start training from score 0.628123\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[663]\tvalid_0's binary_logloss: 0.479322\n",
      "model71 Val Auc: 0.849\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 65 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845728, 39), (1845728, 1), (2011, 39), (2011, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207190, number of negative: 638538\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19286\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845728, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654045 -> initscore=0.636869\n",
      "[LightGBM] [Info] Start training from score 0.636869\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[820]\tvalid_0's binary_logloss: 0.539985\n",
      "model2 Val Auc: 0.799\n",
      "[LightGBM] [Info] Number of positive: 1207190, number of negative: 638538\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19776\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845728, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654045 -> initscore=0.636869\n",
      "[LightGBM] [Info] Start training from score 0.636869\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[520]\tvalid_0's binary_logloss: 0.494112\n",
      "model6 Val Auc: 0.835\n",
      "[LightGBM] [Info] Number of positive: 1207190, number of negative: 638538\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20031\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845728, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654045 -> initscore=0.636869\n",
      "[LightGBM] [Info] Start training from score 0.636869\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[623]\tvalid_0's binary_logloss: 0.494754\n",
      "model71 Val Auc: 0.835\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 66 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845940, 39), (1845940, 1), (2092, 39), (2092, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1213758, number of negative: 632182\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19195\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845940, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657528 -> initscore=0.652299\n",
      "[LightGBM] [Info] Start training from score 0.652299\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[743]\tvalid_0's binary_logloss: 0.529695\n",
      "model2 Val Auc: 0.808\n",
      "[LightGBM] [Info] Number of positive: 1213758, number of negative: 632182\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19685\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845940, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657528 -> initscore=0.652299\n",
      "[LightGBM] [Info] Start training from score 0.652299\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalid_0's binary_logloss: 0.485585\n",
      "model6 Val Auc: 0.843\n",
      "[LightGBM] [Info] Number of positive: 1213758, number of negative: 632182\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19940\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845940, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657528 -> initscore=0.652299\n",
      "[LightGBM] [Info] Start training from score 0.652299\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[441]\tvalid_0's binary_logloss: 0.485917\n",
      "model71 Val Auc: 0.843\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 67 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845310, 39), (1845310, 1), (1993, 39), (1993, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205517, number of negative: 639793\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19286\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845310, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653287 -> initscore=0.633519\n",
      "[LightGBM] [Info] Start training from score 0.633519\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[645]\tvalid_0's binary_logloss: 0.552769\n",
      "model2 Val Auc: 0.790\n",
      "[LightGBM] [Info] Number of positive: 1205517, number of negative: 639793\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19776\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845310, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653287 -> initscore=0.633519\n",
      "[LightGBM] [Info] Start training from score 0.633519\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[435]\tvalid_0's binary_logloss: 0.508952\n",
      "model6 Val Auc: 0.828\n",
      "[LightGBM] [Info] Number of positive: 1205517, number of negative: 639793\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20031\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845310, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653287 -> initscore=0.633519\n",
      "[LightGBM] [Info] Start training from score 0.633519\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[602]\tvalid_0's binary_logloss: 0.507389\n",
      "model71 Val Auc: 0.829\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 68 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845740, 39), (1845740, 1), (1990, 39), (1990, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211638, number of negative: 634102\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19280\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845740, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656451 -> initscore=0.647519\n",
      "[LightGBM] [Info] Start training from score 0.647519\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[978]\tvalid_0's binary_logloss: 0.534114\n",
      "model2 Val Auc: 0.807\n",
      "[LightGBM] [Info] Number of positive: 1211638, number of negative: 634102\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19770\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845740, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656451 -> initscore=0.647519\n",
      "[LightGBM] [Info] Start training from score 0.647519\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[463]\tvalid_0's binary_logloss: 0.497285\n",
      "model6 Val Auc: 0.836\n",
      "[LightGBM] [Info] Number of positive: 1211638, number of negative: 634102\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20025\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845740, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656451 -> initscore=0.647519\n",
      "[LightGBM] [Info] Start training from score 0.647519\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[523]\tvalid_0's binary_logloss: 0.496371\n",
      "model71 Val Auc: 0.837\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 69 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845969, 39), (1845969, 1), (2048, 39), (2048, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211463, number of negative: 634506\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19227\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845969, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656275 -> initscore=0.646737\n",
      "[LightGBM] [Info] Start training from score 0.646737\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[932]\tvalid_0's binary_logloss: 0.522412\n",
      "model2 Val Auc: 0.815\n",
      "[LightGBM] [Info] Number of positive: 1211463, number of negative: 634506\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19717\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845969, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656275 -> initscore=0.646737\n",
      "[LightGBM] [Info] Start training from score 0.646737\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[514]\tvalid_0's binary_logloss: 0.484214\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1211463, number of negative: 634506\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19972\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845969, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656275 -> initscore=0.646737\n",
      "[LightGBM] [Info] Start training from score 0.646737\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[500]\tvalid_0's binary_logloss: 0.483779\n",
      "model71 Val Auc: 0.845\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 70 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845893, 39), (1845893, 1), (1995, 39), (1995, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207411, number of negative: 638482\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19306\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845893, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654107 -> initscore=0.637140\n",
      "[LightGBM] [Info] Start training from score 0.637140\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1378]\tvalid_0's binary_logloss: 0.52415\n",
      "model2 Val Auc: 0.814\n",
      "[LightGBM] [Info] Number of positive: 1207411, number of negative: 638482\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19796\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845893, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654107 -> initscore=0.637140\n",
      "[LightGBM] [Info] Start training from score 0.637140\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[754]\tvalid_0's binary_logloss: 0.483847\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1207411, number of negative: 638482\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20051\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845893, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654107 -> initscore=0.637140\n",
      "[LightGBM] [Info] Start training from score 0.637140\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[797]\tvalid_0's binary_logloss: 0.48332\n",
      "model71 Val Auc: 0.846\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 71 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845789, 39), (1845789, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207112, number of negative: 638677\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19218\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845789, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653982 -> initscore=0.636587\n",
      "[LightGBM] [Info] Start training from score 0.636587\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[938]\tvalid_0's binary_logloss: 0.512201\n",
      "model2 Val Auc: 0.825\n",
      "[LightGBM] [Info] Number of positive: 1207112, number of negative: 638677\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19708\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845789, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653982 -> initscore=0.636587\n",
      "[LightGBM] [Info] Start training from score 0.636587\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1620]\tvalid_0's binary_logloss: 0.474277\n",
      "model6 Val Auc: 0.853\n",
      "[LightGBM] [Info] Number of positive: 1207112, number of negative: 638677\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19963\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845789, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653982 -> initscore=0.636587\n",
      "[LightGBM] [Info] Start training from score 0.636587\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1766]\tvalid_0's binary_logloss: 0.473679\n",
      "model71 Val Auc: 0.854\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 72 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845679, 39), (1845679, 1), (2020, 39), (2020, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204724, number of negative: 640955\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19278\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845679, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652727 -> initscore=0.631047\n",
      "[LightGBM] [Info] Start training from score 0.631047\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[642]\tvalid_0's binary_logloss: 0.530253\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1204724, number of negative: 640955\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19768\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845679, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652727 -> initscore=0.631047\n",
      "[LightGBM] [Info] Start training from score 0.631047\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[487]\tvalid_0's binary_logloss: 0.484553\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1204724, number of negative: 640955\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20023\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845679, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652727 -> initscore=0.631047\n",
      "[LightGBM] [Info] Start training from score 0.631047\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[609]\tvalid_0's binary_logloss: 0.485031\n",
      "model71 Val Auc: 0.844\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 73 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845678, 39), (1845678, 1), (2035, 39), (2035, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211928, number of negative: 633750\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19248\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845678, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656630 -> initscore=0.648313\n",
      "[LightGBM] [Info] Start training from score 0.648313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1003]\tvalid_0's binary_logloss: 0.53484\n",
      "model2 Val Auc: 0.804\n",
      "[LightGBM] [Info] Number of positive: 1211928, number of negative: 633750\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19738\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845678, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656630 -> initscore=0.648313\n",
      "[LightGBM] [Info] Start training from score 0.648313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[953]\tvalid_0's binary_logloss: 0.495952\n",
      "model6 Val Auc: 0.837\n",
      "[LightGBM] [Info] Number of positive: 1211928, number of negative: 633750\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19993\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845678, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656630 -> initscore=0.648313\n",
      "[LightGBM] [Info] Start training from score 0.648313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1331]\tvalid_0's binary_logloss: 0.49431\n",
      "model71 Val Auc: 0.839\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 74 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845704, 39), (1845704, 1), (2072, 39), (2072, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208635, number of negative: 637069\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19245\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845704, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654837 -> initscore=0.640369\n",
      "[LightGBM] [Info] Start training from score 0.640369\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[728]\tvalid_0's binary_logloss: 0.530834\n",
      "model2 Val Auc: 0.808\n",
      "[LightGBM] [Info] Number of positive: 1208635, number of negative: 637069\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19735\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845704, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654837 -> initscore=0.640369\n",
      "[LightGBM] [Info] Start training from score 0.640369\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[595]\tvalid_0's binary_logloss: 0.492013\n",
      "model6 Val Auc: 0.839\n",
      "[LightGBM] [Info] Number of positive: 1208635, number of negative: 637069\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19990\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845704, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654837 -> initscore=0.640369\n",
      "[LightGBM] [Info] Start training from score 0.640369\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[962]\tvalid_0's binary_logloss: 0.491757\n",
      "model71 Val Auc: 0.839\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 75 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845956, 39), (1845956, 1), (1989, 39), (1989, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1209212, number of negative: 636744\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19303\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845956, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655060 -> initscore=0.641356\n",
      "[LightGBM] [Info] Start training from score 0.641356\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\tvalid_0's binary_logloss: 0.543325\n",
      "model2 Val Auc: 0.798\n",
      "[LightGBM] [Info] Number of positive: 1209212, number of negative: 636744\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19793\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845956, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655060 -> initscore=0.641356\n",
      "[LightGBM] [Info] Start training from score 0.641356\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's binary_logloss: 0.511068\n",
      "model6 Val Auc: 0.827\n",
      "[LightGBM] [Info] Number of positive: 1209212, number of negative: 636744\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20048\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845956, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655060 -> initscore=0.641356\n",
      "[LightGBM] [Info] Start training from score 0.641356\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[276]\tvalid_0's binary_logloss: 0.510934\n",
      "model71 Val Auc: 0.827\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 76 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845524, 39), (1845524, 1), (2022, 39), (2022, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207433, number of negative: 638091\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19279\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845524, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654249 -> initscore=0.637771\n",
      "[LightGBM] [Info] Start training from score 0.637771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[928]\tvalid_0's binary_logloss: 0.529117\n",
      "model2 Val Auc: 0.809\n",
      "[LightGBM] [Info] Number of positive: 1207433, number of negative: 638091\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19769\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845524, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654249 -> initscore=0.637771\n",
      "[LightGBM] [Info] Start training from score 0.637771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[673]\tvalid_0's binary_logloss: 0.483982\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1207433, number of negative: 638091\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20024\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845524, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654249 -> initscore=0.637771\n",
      "[LightGBM] [Info] Start training from score 0.637771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's binary_logloss: 0.483823\n",
      "model71 Val Auc: 0.844\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 77 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845930, 39), (1845930, 1), (1988, 39), (1988, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211477, number of negative: 634453\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19295\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845930, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656296 -> initscore=0.646832\n",
      "[LightGBM] [Info] Start training from score 0.646832\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[871]\tvalid_0's binary_logloss: 0.531952\n",
      "model2 Val Auc: 0.806\n",
      "[LightGBM] [Info] Number of positive: 1211477, number of negative: 634453\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19785\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845930, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656296 -> initscore=0.646832\n",
      "[LightGBM] [Info] Start training from score 0.646832\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[433]\tvalid_0's binary_logloss: 0.482674\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1211477, number of negative: 634453\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20040\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845930, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656296 -> initscore=0.646832\n",
      "[LightGBM] [Info] Start training from score 0.646832\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[374]\tvalid_0's binary_logloss: 0.483059\n",
      "model71 Val Auc: 0.845\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 78 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845521, 39), (1845521, 1), (2014, 39), (2014, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204443, number of negative: 641078\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19274\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845521, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652630 -> initscore=0.630621\n",
      "[LightGBM] [Info] Start training from score 0.630621\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[770]\tvalid_0's binary_logloss: 0.51333\n",
      "model2 Val Auc: 0.823\n",
      "[LightGBM] [Info] Number of positive: 1204443, number of negative: 641078\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19764\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845521, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652630 -> initscore=0.630621\n",
      "[LightGBM] [Info] Start training from score 0.630621\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[550]\tvalid_0's binary_logloss: 0.466627\n",
      "model6 Val Auc: 0.858\n",
      "[LightGBM] [Info] Number of positive: 1204443, number of negative: 641078\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20019\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845521, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652630 -> initscore=0.630621\n",
      "[LightGBM] [Info] Start training from score 0.630621\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[600]\tvalid_0's binary_logloss: 0.464674\n",
      "model71 Val Auc: 0.859\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 79 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845719, 39), (1845719, 1), (2020, 39), (2020, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1215266, number of negative: 630453\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19248\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845719, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658424 -> initscore=0.656280\n",
      "[LightGBM] [Info] Start training from score 0.656280\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[709]\tvalid_0's binary_logloss: 0.557438\n",
      "model2 Val Auc: 0.788\n",
      "[LightGBM] [Info] Number of positive: 1215266, number of negative: 630453\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19738\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845719, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658424 -> initscore=0.656280\n",
      "[LightGBM] [Info] Start training from score 0.656280\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalid_0's binary_logloss: 0.510162\n",
      "model6 Val Auc: 0.827\n",
      "[LightGBM] [Info] Number of positive: 1215266, number of negative: 630453\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19993\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845719, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658424 -> initscore=0.656280\n",
      "[LightGBM] [Info] Start training from score 0.656280\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's binary_logloss: 0.510095\n",
      "model71 Val Auc: 0.827\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 80 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845813, 39), (1845813, 1), (2013, 39), (2013, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205061, number of negative: 640752\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19298\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845813, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652862 -> initscore=0.631643\n",
      "[LightGBM] [Info] Start training from score 0.631643\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[388]\tvalid_0's binary_logloss: 0.526302\n",
      "model2 Val Auc: 0.812\n",
      "[LightGBM] [Info] Number of positive: 1205061, number of negative: 640752\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19788\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845813, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652862 -> initscore=0.631643\n",
      "[LightGBM] [Info] Start training from score 0.631643\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[688]\tvalid_0's binary_logloss: 0.480466\n",
      "model6 Val Auc: 0.848\n",
      "[LightGBM] [Info] Number of positive: 1205061, number of negative: 640752\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20043\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845813, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652862 -> initscore=0.631643\n",
      "[LightGBM] [Info] Start training from score 0.631643\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[740]\tvalid_0's binary_logloss: 0.47973\n",
      "model71 Val Auc: 0.848\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 81 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845888, 39), (1845888, 1), (1998, 39), (1998, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205999, number of negative: 639889\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19252\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845888, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653344 -> initscore=0.633769\n",
      "[LightGBM] [Info] Start training from score 0.633769\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[640]\tvalid_0's binary_logloss: 0.539683\n",
      "model2 Val Auc: 0.799\n",
      "[LightGBM] [Info] Number of positive: 1205999, number of negative: 639889\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19742\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845888, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653344 -> initscore=0.633769\n",
      "[LightGBM] [Info] Start training from score 0.633769\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[555]\tvalid_0's binary_logloss: 0.492395\n",
      "model6 Val Auc: 0.839\n",
      "[LightGBM] [Info] Number of positive: 1205999, number of negative: 639889\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19997\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845888, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653344 -> initscore=0.633769\n",
      "[LightGBM] [Info] Start training from score 0.633769\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[590]\tvalid_0's binary_logloss: 0.492435\n",
      "model71 Val Auc: 0.839\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 82 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845517, 39), (1845517, 1), (2028, 39), (2028, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1212483, number of negative: 633034\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19264\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845517, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656988 -> initscore=0.649901\n",
      "[LightGBM] [Info] Start training from score 0.649901\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[723]\tvalid_0's binary_logloss: 0.519001\n",
      "model2 Val Auc: 0.818\n",
      "[LightGBM] [Info] Number of positive: 1212483, number of negative: 633034\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19754\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845517, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656988 -> initscore=0.649901\n",
      "[LightGBM] [Info] Start training from score 0.649901\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[389]\tvalid_0's binary_logloss: 0.477249\n",
      "model6 Val Auc: 0.850\n",
      "[LightGBM] [Info] Number of positive: 1212483, number of negative: 633034\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20009\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845517, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656988 -> initscore=0.649901\n",
      "[LightGBM] [Info] Start training from score 0.649901\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[450]\tvalid_0's binary_logloss: 0.477277\n",
      "model71 Val Auc: 0.850\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 83 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845847, 39), (1845847, 1), (2078, 39), (2078, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1205673, number of negative: 640174\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19188\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845847, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653181 -> initscore=0.633053\n",
      "[LightGBM] [Info] Start training from score 0.633053\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[989]\tvalid_0's binary_logloss: 0.5347\n",
      "model2 Val Auc: 0.805\n",
      "[LightGBM] [Info] Number of positive: 1205673, number of negative: 640174\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19678\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845847, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653181 -> initscore=0.633053\n",
      "[LightGBM] [Info] Start training from score 0.633053\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[707]\tvalid_0's binary_logloss: 0.471427\n",
      "model6 Val Auc: 0.854\n",
      "[LightGBM] [Info] Number of positive: 1205673, number of negative: 640174\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19933\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845847, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653181 -> initscore=0.633053\n",
      "[LightGBM] [Info] Start training from score 0.633053\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1022]\tvalid_0's binary_logloss: 0.470365\n",
      "model71 Val Auc: 0.856\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 84 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1844623, 39), (1844623, 1), (1985, 39), (1985, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1204441, number of negative: 640182\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19279\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844623, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652947 -> initscore=0.632018\n",
      "[LightGBM] [Info] Start training from score 0.632018\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[929]\tvalid_0's binary_logloss: 0.534791\n",
      "model2 Val Auc: 0.805\n",
      "[LightGBM] [Info] Number of positive: 1204441, number of negative: 640182\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19769\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844623, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652947 -> initscore=0.632018\n",
      "[LightGBM] [Info] Start training from score 0.632018\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[493]\tvalid_0's binary_logloss: 0.482941\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1204441, number of negative: 640182\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20024\n",
      "[LightGBM] [Info] Number of data points in the train set: 1844623, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.652947 -> initscore=0.632018\n",
      "[LightGBM] [Info] Start training from score 0.632018\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.482102\n",
      "model71 Val Auc: 0.845\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 85 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845839, 39), (1845839, 1), (2040, 39), (2040, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207423, number of negative: 638416\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19248\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845839, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654132 -> initscore=0.637254\n",
      "[LightGBM] [Info] Start training from score 0.637254\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[497]\tvalid_0's binary_logloss: 0.528029\n",
      "model2 Val Auc: 0.811\n",
      "[LightGBM] [Info] Number of positive: 1207423, number of negative: 638416\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19738\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845839, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654132 -> initscore=0.637254\n",
      "[LightGBM] [Info] Start training from score 0.637254\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.489924\n",
      "model6 Val Auc: 0.841\n",
      "[LightGBM] [Info] Number of positive: 1207423, number of negative: 638416\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19993\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845839, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654132 -> initscore=0.637254\n",
      "[LightGBM] [Info] Start training from score 0.637254\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[728]\tvalid_0's binary_logloss: 0.489788\n",
      "model71 Val Auc: 0.841\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 86 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845739, 39), (1845739, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206345, number of negative: 639394\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19257\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653584 -> initscore=0.634830\n",
      "[LightGBM] [Info] Start training from score 0.634830\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.526636\n",
      "model2 Val Auc: 0.812\n",
      "[LightGBM] [Info] Number of positive: 1206345, number of negative: 639394\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19747\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653584 -> initscore=0.634830\n",
      "[LightGBM] [Info] Start training from score 0.634830\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[534]\tvalid_0's binary_logloss: 0.4812\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1206345, number of negative: 639394\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20002\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845739, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653584 -> initscore=0.634830\n",
      "[LightGBM] [Info] Start training from score 0.634830\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[465]\tvalid_0's binary_logloss: 0.479587\n",
      "model71 Val Auc: 0.847\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 87 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845638, 39), (1845638, 1), (1980, 39), (1980, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208598, number of negative: 637040\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19280\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845638, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654840 -> initscore=0.640384\n",
      "[LightGBM] [Info] Start training from score 0.640384\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.515243\n",
      "model2 Val Auc: 0.821\n",
      "[LightGBM] [Info] Number of positive: 1208598, number of negative: 637040\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19770\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845638, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654840 -> initscore=0.640384\n",
      "[LightGBM] [Info] Start training from score 0.640384\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's binary_logloss: 0.466423\n",
      "model6 Val Auc: 0.857\n",
      "[LightGBM] [Info] Number of positive: 1208598, number of negative: 637040\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20025\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845638, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654840 -> initscore=0.640384\n",
      "[LightGBM] [Info] Start training from score 0.640384\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[728]\tvalid_0's binary_logloss: 0.465096\n",
      "model71 Val Auc: 0.858\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 88 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845590, 39), (1845590, 1), (2008, 39), (2008, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207960, number of negative: 637630\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19254\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845590, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654512 -> initscore=0.638930\n",
      "[LightGBM] [Info] Start training from score 0.638930\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[289]\tvalid_0's binary_logloss: 0.552042\n",
      "model2 Val Auc: 0.789\n",
      "[LightGBM] [Info] Number of positive: 1207960, number of negative: 637630\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19744\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845590, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654512 -> initscore=0.638930\n",
      "[LightGBM] [Info] Start training from score 0.638930\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[376]\tvalid_0's binary_logloss: 0.497193\n",
      "model6 Val Auc: 0.836\n",
      "[LightGBM] [Info] Number of positive: 1207960, number of negative: 637630\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19999\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845590, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654512 -> initscore=0.638930\n",
      "[LightGBM] [Info] Start training from score 0.638930\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\tvalid_0's binary_logloss: 0.497022\n",
      "model71 Val Auc: 0.836\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 89 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845848, 39), (1845848, 1), (1966, 39), (1966, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1212445, number of negative: 633403\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19312\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845848, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656850 -> initscore=0.649287\n",
      "[LightGBM] [Info] Start training from score 0.649287\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[843]\tvalid_0's binary_logloss: 0.523505\n",
      "model2 Val Auc: 0.815\n",
      "[LightGBM] [Info] Number of positive: 1212445, number of negative: 633403\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19802\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845848, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656850 -> initscore=0.649287\n",
      "[LightGBM] [Info] Start training from score 0.649287\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[734]\tvalid_0's binary_logloss: 0.483615\n",
      "model71 Val Auc: 0.845\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 90 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845900, 39), (1845900, 1), (1987, 39), (1987, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1206692, number of negative: 639208\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19275\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845900, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653715 -> initscore=0.635408\n",
      "[LightGBM] [Info] Start training from score 0.635408\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[603]\tvalid_0's binary_logloss: 0.532848\n",
      "model2 Val Auc: 0.804\n",
      "[LightGBM] [Info] Number of positive: 1206692, number of negative: 639208\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19765\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845900, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653715 -> initscore=0.635408\n",
      "[LightGBM] [Info] Start training from score 0.635408\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's binary_logloss: 0.488181\n",
      "model6 Val Auc: 0.841\n",
      "[LightGBM] [Info] Number of positive: 1206692, number of negative: 639208\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20020\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845900, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653715 -> initscore=0.635408\n",
      "[LightGBM] [Info] Start training from score 0.635408\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[485]\tvalid_0's binary_logloss: 0.487886\n",
      "model71 Val Auc: 0.841\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 91 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845587, 39), (1845587, 1), (2062, 39), (2062, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208311, number of negative: 637276\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19219\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845587, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654703 -> initscore=0.639776\n",
      "[LightGBM] [Info] Start training from score 0.639776\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[768]\tvalid_0's binary_logloss: 0.53791\n",
      "model2 Val Auc: 0.804\n",
      "[LightGBM] [Info] Number of positive: 1208311, number of negative: 637276\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19709\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845587, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654703 -> initscore=0.639776\n",
      "[LightGBM] [Info] Start training from score 0.639776\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[616]\tvalid_0's binary_logloss: 0.485513\n",
      "model6 Val Auc: 0.844\n",
      "[LightGBM] [Info] Number of positive: 1208311, number of negative: 637276\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845587, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654703 -> initscore=0.639776\n",
      "[LightGBM] [Info] Start training from score 0.639776\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[695]\tvalid_0's binary_logloss: 0.484085\n",
      "model71 Val Auc: 0.845\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 92 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845809, 39), (1845809, 1), (2043, 39), (2043, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208465, number of negative: 637344\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19238\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845809, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654708 -> initscore=0.639797\n",
      "[LightGBM] [Info] Start training from score 0.639797\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[823]\tvalid_0's binary_logloss: 0.525798\n",
      "model2 Val Auc: 0.812\n",
      "[LightGBM] [Info] Number of positive: 1208465, number of negative: 637344\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19728\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845809, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654708 -> initscore=0.639797\n",
      "[LightGBM] [Info] Start training from score 0.639797\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[851]\tvalid_0's binary_logloss: 0.484674\n",
      "model6 Val Auc: 0.846\n",
      "[LightGBM] [Info] Number of positive: 1208465, number of negative: 637344\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19983\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845809, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654708 -> initscore=0.639797\n",
      "[LightGBM] [Info] Start training from score 0.639797\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.483519\n",
      "model71 Val Auc: 0.846\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 93 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845524, 39), (1845524, 1), (1943, 39), (1943, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207246, number of negative: 638278\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19314\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845524, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654148 -> initscore=0.637323\n",
      "[LightGBM] [Info] Start training from score 0.637323\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\tvalid_0's binary_logloss: 0.546346\n",
      "model2 Val Auc: 0.793\n",
      "[LightGBM] [Info] Number of positive: 1207246, number of negative: 638278\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19804\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845524, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654148 -> initscore=0.637323\n",
      "[LightGBM] [Info] Start training from score 0.637323\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[479]\tvalid_0's binary_logloss: 0.510385\n",
      "model6 Val Auc: 0.824\n",
      "[LightGBM] [Info] Number of positive: 1207246, number of negative: 638278\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20059\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845524, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654148 -> initscore=0.637323\n",
      "[LightGBM] [Info] Start training from score 0.637323\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[500]\tvalid_0's binary_logloss: 0.510907\n",
      "model71 Val Auc: 0.824\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 94 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845408, 39), (1845408, 1), (2047, 39), (2047, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1211366, number of negative: 634042\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19204\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845408, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656422 -> initscore=0.647389\n",
      "[LightGBM] [Info] Start training from score 0.647389\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[925]\tvalid_0's binary_logloss: 0.535539\n",
      "model2 Val Auc: 0.803\n",
      "[LightGBM] [Info] Number of positive: 1211366, number of negative: 634042\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19694\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845408, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656422 -> initscore=0.647389\n",
      "[LightGBM] [Info] Start training from score 0.647389\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[793]\tvalid_0's binary_logloss: 0.492346\n",
      "model6 Val Auc: 0.838\n",
      "[LightGBM] [Info] Number of positive: 1211366, number of negative: 634042\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19949\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845408, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656422 -> initscore=0.647389\n",
      "[LightGBM] [Info] Start training from score 0.647389\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[716]\tvalid_0's binary_logloss: 0.490795\n",
      "model71 Val Auc: 0.839\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 95 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845774, 39), (1845774, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210969, number of negative: 634805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19232\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845774, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656077 -> initscore=0.645858\n",
      "[LightGBM] [Info] Start training from score 0.645858\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[988]\tvalid_0's binary_logloss: 0.544921\n",
      "model2 Val Auc: 0.796\n",
      "[LightGBM] [Info] Number of positive: 1210969, number of negative: 634805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19722\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845774, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656077 -> initscore=0.645858\n",
      "[LightGBM] [Info] Start training from score 0.645858\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's binary_logloss: 0.495962\n",
      "model6 Val Auc: 0.837\n",
      "[LightGBM] [Info] Number of positive: 1210969, number of negative: 634805\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19977\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845774, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656077 -> initscore=0.645858\n",
      "[LightGBM] [Info] Start training from score 0.645858\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[853]\tvalid_0's binary_logloss: 0.494875\n",
      "model71 Val Auc: 0.838\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 96 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845491, 39), (1845491, 1), (2040, 39), (2040, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210990, number of negative: 634501\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19254\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845491, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656189 -> initscore=0.646355\n",
      "[LightGBM] [Info] Start training from score 0.646355\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[808]\tvalid_0's binary_logloss: 0.524009\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1210990, number of negative: 634501\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19744\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845491, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656189 -> initscore=0.646355\n",
      "[LightGBM] [Info] Start training from score 0.646355\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[491]\tvalid_0's binary_logloss: 0.475539\n",
      "model6 Val Auc: 0.851\n",
      "[LightGBM] [Info] Number of positive: 1210990, number of negative: 634501\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19999\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845491, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.656189 -> initscore=0.646355\n",
      "[LightGBM] [Info] Start training from score 0.646355\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[467]\tvalid_0's binary_logloss: 0.475385\n",
      "model71 Val Auc: 0.851\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 97 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845808, 39), (1845808, 1), (2110, 39), (2110, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1208434, number of negative: 637374\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19196\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845808, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654691 -> initscore=0.639724\n",
      "[LightGBM] [Info] Start training from score 0.639724\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[827]\tvalid_0's binary_logloss: 0.525786\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1208434, number of negative: 637374\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19686\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845808, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654691 -> initscore=0.639724\n",
      "[LightGBM] [Info] Start training from score 0.639724\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[792]\tvalid_0's binary_logloss: 0.484202\n",
      "model6 Val Auc: 0.847\n",
      "[LightGBM] [Info] Number of positive: 1208434, number of negative: 637374\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19941\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845808, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654691 -> initscore=0.639724\n",
      "[LightGBM] [Info] Start training from score 0.639724\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1160]\tvalid_0's binary_logloss: 0.481676\n",
      "model71 Val Auc: 0.849\n",
      "[0, 1, 2]\n",
      "----------------------------- SEED 98 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845858, 39), (1845858, 1), (2014, 39), (2014, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1207033, number of negative: 638825\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19287\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845858, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653914 -> initscore=0.636290\n",
      "[LightGBM] [Info] Start training from score 0.636290\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[965]\tvalid_0's binary_logloss: 0.522364\n",
      "model2 Val Auc: 0.813\n",
      "[LightGBM] [Info] Number of positive: 1207033, number of negative: 638825\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19777\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845858, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653914 -> initscore=0.636290\n",
      "[LightGBM] [Info] Start training from score 0.636290\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\tvalid_0's binary_logloss: 0.480754\n",
      "model6 Val Auc: 0.845\n",
      "[LightGBM] [Info] Number of positive: 1207033, number of negative: 638825\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20032\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845858, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.653914 -> initscore=0.636290\n",
      "[LightGBM] [Info] Start training from score 0.636290\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[812]\tvalid_0's binary_logloss: 0.482169\n",
      "model71 Val Auc: 0.844\n",
      "[0, 2, 1]\n",
      "----------------------------- SEED 99 -----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845722, 39), (1845722, 1), (1980, 39), (1980, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1210271, number of negative: 635451\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19309\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845722, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655717 -> initscore=0.644265\n",
      "[LightGBM] [Info] Start training from score 0.644265\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[812]\tvalid_0's binary_logloss: 0.543128\n",
      "model2 Val Auc: 0.798\n",
      "[LightGBM] [Info] Number of positive: 1210271, number of negative: 635451\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19799\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845722, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655717 -> initscore=0.644265\n",
      "[LightGBM] [Info] Start training from score 0.644265\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[368]\tvalid_0's binary_logloss: 0.500137\n",
      "model6 Val Auc: 0.831\n",
      "[LightGBM] [Info] Number of positive: 1210271, number of negative: 635451\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20054\n",
      "[LightGBM] [Info] Number of data points in the train set: 1845722, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655717 -> initscore=0.644265\n",
      "[LightGBM] [Info] Start training from score 0.644265\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[394]\tvalid_0's binary_logloss: 0.501827\n",
      "model71 Val Auc: 0.830\n",
      "[0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "mlflow.lightgbm.autolog()\n",
    "for s in range(100):\n",
    "    print(f\"----------------------------- SEED {s} -----------------------------\")\n",
    "    train, valid = custom_train_test_split(train_df, ratio=0.7, seed=s) # ÌõàÎ†®Îç∞Ïù¥ÌÑ∞ split\n",
    "    test = df[df.kind=='test'] # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞\n",
    "    train2 = test[test.answerCode!=-1] # ÌÖåÏä§Ìä∏Îç∞Ïù¥ÌÑ∞ ÎßàÏßÄÎßâ Ï†úÏ∂ú 2Î≤àÏ®∞Í∫ºÍπåÏßÄ ÌõàÎ†®Îç∞Ïù¥ÌÑ∞Î°ú ÏÇ¨Ïö©\n",
    "    train = pd.concat([train,train2]) # ÌõàÎ†®Îç∞Ïù¥ÌÑ∞ Î≥ëÌï©\n",
    "    x_train = train.drop('answerCode',axis=1)\n",
    "    y_train = train[['answerCode']]\n",
    "    x_valid = valid.drop('answerCode',axis=1)\n",
    "    y_valid = valid[['answerCode']]\n",
    "    x_train.shape, y_train.shape, x_valid.shape, y_valid.shape\n",
    "    aucs=[]\n",
    "    for n,lb in zip([2,6,71],[0.8155,0.8024,0.7948]):\n",
    "        model_name = f\"model{n}\"\n",
    "        run_title = f\"model{n} - LB AUC:{lb}\"\n",
    "        model,feats = model_dict[model_name]\n",
    "        model, auc = model_train2auc(run_title, x_valid, y_valid, feats, s, experiment_id)\n",
    "        print(f\"{model_name} Val Auc: {auc:.3f}\")\n",
    "        model_dict[model_name][0] = model\n",
    "        aucs.append(auc)\n",
    "    saucs = sorted(aucs)\n",
    "    sort_index = [saucs.index(i) for i in aucs]\n",
    "    print(sort_index)\n",
    "    if sort_index==[3,4,1,2,0]:\n",
    "        print(s)\n",
    "        post_slack(\"done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ca11b",
   "metadata": {},
   "source": [
    "### seedÎ≥Ñ auc ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e1197d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T02:30:31.153858Z",
     "start_time": "2022-12-01T02:26:37.243761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1845739, 39), (1845739, 1), (1968, 39), (1968, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.888 model2 0.856 model6 0.867 model7 0.870 model71 0.905 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845952, 39), (1845952, 1), (2015, 39), (2015, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.896 model2 0.864 model6 0.871 model7 0.876 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845977, 39), (1845977, 1), (2011, 39), (2011, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.882 model2 0.849 model6 0.859 model7 0.864 model71 0.903 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845528, 39), (1845528, 1), (2009, 39), (2009, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.899 model2 0.865 model6 0.872 model7 0.877 model71 0.913 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845319, 39), (1845319, 1), (2016, 39), (2016, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.856 model6 0.861 model7 0.868 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845636, 39), (1845636, 1), (1977, 39), (1977, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.881 model2 0.845 model6 0.864 model7 0.868 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845842, 39), (1845842, 1), (1992, 39), (1992, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.890 model2 0.857 model6 0.870 model7 0.874 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845872, 39), (1845872, 1), (1951, 39), (1951, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.889 model2 0.856 model6 0.868 model7 0.873 model71 0.909 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845720, 39), (1845720, 1), (2000, 39), (2000, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.878 model2 0.849 model6 0.862 model7 0.866 model71 0.904 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845915, 39), (1845915, 1), (1940, 39), (1940, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.894 model2 0.857 model6 0.866 model7 0.870 model71 0.911 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845869, 39), (1845869, 1), (2036, 39), (2036, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.855 model6 0.865 model7 0.870 model71 0.906 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845919, 39), (1845919, 1), (2006, 39), (2006, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.894 model2 0.860 model6 0.865 model7 0.869 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845257, 39), (1845257, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.883 model2 0.849 model6 0.864 model7 0.868 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845539, 39), (1845539, 1), (1974, 39), (1974, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.791 model2 0.797 model6 0.834 model7 0.834 model71 0.832 [0, 1, 3, 4, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845583, 39), (1845583, 1), (1991, 39), (1991, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.888 model2 0.855 model6 0.867 model7 0.872 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845753, 39), (1845753, 1), (1975, 39), (1975, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.888 model2 0.851 model6 0.864 model7 0.870 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845890, 39), (1845890, 1), (1966, 39), (1966, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.889 model2 0.852 model6 0.863 model7 0.868 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845409, 39), (1845409, 1), (1992, 39), (1992, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.886 model2 0.857 model6 0.863 model7 0.867 model71 0.901 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845711, 39), (1845711, 1), (2035, 39), (2035, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.852 model6 0.864 model7 0.869 model71 0.906 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845937, 39), (1845937, 1), (2032, 39), (2032, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.897 model2 0.862 model6 0.872 model7 0.877 model71 0.913 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845954, 39), (1845954, 1), (1983, 39), (1983, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.882 model2 0.844 model6 0.858 model7 0.863 model71 0.903 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845674, 39), (1845674, 1), (2027, 39), (2027, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.889 model2 0.855 model6 0.869 model7 0.873 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845820, 39), (1845820, 1), (1981, 39), (1981, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.883 model2 0.846 model6 0.859 model7 0.864 model71 0.903 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845978, 39), (1845978, 1), (1970, 39), (1970, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.892 model2 0.863 model6 0.877 model7 0.881 model71 0.914 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845901, 39), (1845901, 1), (2044, 39), (2044, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.894 model2 0.860 model6 0.869 model7 0.874 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845312, 39), (1845312, 1), (1966, 39), (1966, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.889 model2 0.856 model6 0.868 model7 0.875 model71 0.913 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1844865, 39), (1844865, 1), (2069, 39), (2069, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.894 model2 0.858 model6 0.870 model7 0.875 model71 0.914 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845900, 39), (1845900, 1), (2019, 39), (2019, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.855 model6 0.865 model7 0.869 model71 0.904 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845851, 39), (1845851, 1), (2003, 39), (2003, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.851 model6 0.865 model7 0.870 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845822, 39), (1845822, 1), (2007, 39), (2007, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.848 model6 0.858 model7 0.863 model71 0.903 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845891, 39), (1845891, 1), (1996, 39), (1996, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.883 model2 0.847 model6 0.858 model7 0.863 model71 0.903 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845542, 39), (1845542, 1), (1987, 39), (1987, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.904 model2 0.867 model6 0.862 model7 0.869 model71 0.909 [3, 1, 0, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845146, 39), (1845146, 1), (1920, 39), (1920, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.898 model2 0.866 model6 0.875 model7 0.879 model71 0.914 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845805, 39), (1845805, 1), (2007, 39), (2007, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.896 model2 0.864 model6 0.871 model7 0.875 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845771, 39), (1845771, 1), (2020, 39), (2020, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.890 model2 0.857 model6 0.868 model7 0.873 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845977, 39), (1845977, 1), (1947, 39), (1947, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.877 model2 0.846 model6 0.858 model7 0.863 model71 0.899 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845739, 39), (1845739, 1), (2013, 39), (2013, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.849 model6 0.862 model7 0.867 model71 0.906 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845868, 39), (1845868, 1), (1990, 39), (1990, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.886 model2 0.850 model6 0.863 model7 0.870 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845887, 39), (1845887, 1), (2043, 39), (2043, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.893 model2 0.855 model6 0.869 model7 0.874 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845699, 39), (1845699, 1), (1977, 39), (1977, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.892 model2 0.856 model6 0.867 model7 0.873 model71 0.915 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845922, 39), (1845922, 1), (2042, 39), (2042, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.896 model2 0.863 model6 0.877 model7 0.881 model71 0.913 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845652, 39), (1845652, 1), (2037, 39), (2037, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.879 model2 0.844 model6 0.856 model7 0.862 model71 0.902 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845527, 39), (1845527, 1), (2007, 39), (2007, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.854 model6 0.864 model7 0.868 model71 0.903 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845922, 39), (1845922, 1), (1993, 39), (1993, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.850 model6 0.860 model7 0.866 model71 0.909 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845794, 39), (1845794, 1), (1961, 39), (1961, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.895 model2 0.864 model6 0.876 model7 0.880 model71 0.911 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845500, 39), (1845500, 1), (2016, 39), (2016, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.894 model2 0.857 model6 0.866 model7 0.872 model71 0.913 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845974, 39), (1845974, 1), (1944, 39), (1944, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.879 model2 0.840 model6 0.852 model7 0.858 model71 0.900 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845521, 39), (1845521, 1), (2060, 39), (2060, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.849 model6 0.863 model7 0.868 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845500, 39), (1845500, 1), (1995, 39), (1995, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.888 model2 0.855 model6 0.864 model7 0.869 model71 0.909 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845797, 39), (1845797, 1), (2000, 39), (2000, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.891 model2 0.857 model6 0.866 model7 0.871 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845470, 39), (1845470, 1), (2073, 39), (2073, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.892 model2 0.856 model6 0.866 model7 0.870 model71 0.911 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845514, 39), (1845514, 1), (1955, 39), (1955, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.898 model2 0.863 model6 0.870 model7 0.875 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845303, 39), (1845303, 1), (1996, 39), (1996, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.895 model2 0.864 model6 0.876 model7 0.881 model71 0.920 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845978, 39), (1845978, 1), (2058, 39), (2058, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.886 model2 0.852 model6 0.866 model7 0.871 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845753, 39), (1845753, 1), (2076, 39), (2076, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.879 model2 0.843 model6 0.854 model7 0.859 model71 0.900 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845855, 39), (1845855, 1), (2033, 39), (2033, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.885 model2 0.849 model6 0.861 model7 0.867 model71 0.905 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845418, 39), (1845418, 1), (2015, 39), (2015, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.886 model2 0.856 model6 0.870 model7 0.874 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845562, 39), (1845562, 1), (1972, 39), (1972, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.883 model2 0.849 model6 0.867 model7 0.872 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845929, 39), (1845929, 1), (2079, 39), (2079, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.898 model2 0.862 model6 0.865 model7 0.870 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845923, 39), (1845923, 1), (1994, 39), (1994, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.895 model2 0.861 model6 0.863 model7 0.869 model71 0.909 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1844831, 39), (1844831, 1), (2036, 39), (2036, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.892 model2 0.855 model6 0.859 model7 0.864 model71 0.905 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845846, 39), (1845846, 1), (1945, 39), (1945, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.894 model2 0.860 model6 0.863 model7 0.867 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845429, 39), (1845429, 1), (1987, 39), (1987, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.885 model2 0.846 model6 0.859 model7 0.864 model71 0.901 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845700, 39), (1845700, 1), (1990, 39), (1990, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.851 model6 0.855 model7 0.862 model71 0.901 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845876, 39), (1845876, 1), (2050, 39), (2050, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.855 model6 0.871 model7 0.875 model71 0.913 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845728, 39), (1845728, 1), (2011, 39), (2011, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.850 model6 0.860 model7 0.865 model71 0.904 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845940, 39), (1845940, 1), (2092, 39), (2092, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.888 model2 0.855 model6 0.865 model7 0.870 model71 0.906 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845310, 39), (1845310, 1), (1993, 39), (1993, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.877 model2 0.839 model6 0.852 model7 0.858 model71 0.896 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845740, 39), (1845740, 1), (1990, 39), (1990, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.886 model2 0.848 model6 0.857 model7 0.863 model71 0.905 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845969, 39), (1845969, 1), (2048, 39), (2048, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.891 model2 0.859 model6 0.867 model7 0.872 model71 0.909 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845893, 39), (1845893, 1), (1995, 39), (1995, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.898 model2 0.863 model6 0.868 model7 0.875 model71 0.917 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845789, 39), (1845789, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.899 model2 0.863 model6 0.871 model7 0.876 model71 0.915 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845679, 39), (1845679, 1), (2020, 39), (2020, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.852 model6 0.866 model7 0.871 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845678, 39), (1845678, 1), (2035, 39), (2035, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.846 model6 0.856 model7 0.861 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845704, 39), (1845704, 1), (2072, 39), (2072, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.853 model6 0.862 model7 0.867 model71 0.906 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845956, 39), (1845956, 1), (1989, 39), (1989, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.880 model2 0.844 model6 0.850 model7 0.854 model71 0.895 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845524, 39), (1845524, 1), (2022, 39), (2022, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.892 model2 0.860 model6 0.867 model7 0.872 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845930, 39), (1845930, 1), (1988, 39), (1988, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.889 model2 0.856 model6 0.866 model7 0.870 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845521, 39), (1845521, 1), (2014, 39), (2014, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.903 model2 0.870 model6 0.879 model7 0.883 model71 0.919 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845719, 39), (1845719, 1), (2020, 39), (2020, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.877 model2 0.839 model6 0.853 model7 0.858 model71 0.900 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845813, 39), (1845813, 1), (2013, 39), (2013, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.890 model2 0.856 model6 0.870 model7 0.875 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845888, 39), (1845888, 1), (1998, 39), (1998, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.878 model2 0.845 model6 0.859 model7 0.864 model71 0.902 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845517, 39), (1845517, 1), (2028, 39), (2028, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.890 model2 0.861 model6 0.871 model7 0.877 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845847, 39), (1845847, 1), (2078, 39), (2078, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.891 model2 0.856 model6 0.874 model7 0.879 model71 0.916 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1844623, 39), (1844623, 1), (1985, 39), (1985, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.881 model2 0.848 model6 0.864 model7 0.868 model71 0.904 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845839, 39), (1845839, 1), (2040, 39), (2040, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.891 model2 0.858 model6 0.861 model7 0.865 model71 0.905 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845739, 39), (1845739, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.898 model2 0.860 model6 0.871 model7 0.877 model71 0.918 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845638, 39), (1845638, 1), (1980, 39), (1980, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.892 model2 0.862 model6 0.877 model7 0.881 model71 0.915 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845590, 39), (1845590, 1), (2008, 39), (2008, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.881 model2 0.844 model6 0.863 model7 0.869 model71 0.909 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845848, 39), (1845848, 1), (1966, 39), (1966, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.892 model2 0.861 model6 0.867 model7 0.873 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845900, 39), (1845900, 1), (1987, 39), (1987, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.881 model2 0.848 model6 0.860 model7 0.864 model71 0.908 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845587, 39), (1845587, 1), (2062, 39), (2062, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.890 model2 0.852 model6 0.866 model7 0.871 model71 0.912 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845809, 39), (1845809, 1), (2043, 39), (2043, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.891 model2 0.856 model6 0.864 model7 0.869 model71 0.905 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845524, 39), (1845524, 1), (1943, 39), (1943, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.879 model2 0.843 model6 0.849 model7 0.854 model71 0.895 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845408, 39), (1845408, 1), (2047, 39), (2047, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.884 model2 0.848 model6 0.860 model7 0.865 model71 0.907 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845774, 39), (1845774, 1), (2031, 39), (2031, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.879 model2 0.844 model6 0.860 model7 0.865 model71 0.904 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845491, 39), (1845491, 1), (2040, 39), (2040, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.894 model2 0.861 model6 0.870 model7 0.875 model71 0.909 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845808, 39), (1845808, 1), (2110, 39), (2110, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.897 model2 0.860 model6 0.868 model7 0.874 model71 0.918 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845858, 39), (1845858, 1), (2014, 39), (2014, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.893 model2 0.863 model6 0.870 model7 0.876 model71 0.910 [3, 0, 1, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1845722, 39), (1845722, 1), (1980, 39), (1980, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 0.887 model2 0.848 model6 0.856 model7 0.860 model71 0.901 [3, 0, 1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "for s in range(100):\n",
    "    train, valid = custom_train_test_split(train_df, ratio=0.7, seed=s) # ÌõàÎ†®Îç∞Ïù¥ÌÑ∞ split\n",
    "    test = df[df.kind=='test'] # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞\n",
    "    train2 = test[test.answerCode!=-1] # ÌÖåÏä§Ìä∏Îç∞Ïù¥ÌÑ∞ ÎßàÏßÄÎßâ Ï†úÏ∂ú 2Î≤àÏ®∞Í∫ºÍπåÏßÄ ÌõàÎ†®Îç∞Ïù¥ÌÑ∞Î°ú ÏÇ¨Ïö©\n",
    "    train = pd.concat([train,train2]) # ÌõàÎ†®Îç∞Ïù¥ÌÑ∞ Î≥ëÌï©\n",
    "    x_train = train.drop('answerCode',axis=1)\n",
    "    y_train = train[['answerCode']]\n",
    "    x_valid = valid.drop('answerCode',axis=1)\n",
    "    y_valid = valid[['answerCode']]\n",
    "    x_train.shape, y_train.shape, x_valid.shape, y_valid.shape\n",
    "    aucs=[]\n",
    "    for n in [1,2,6,7,71]:\n",
    "        model_name = f\"model{n}\"\n",
    "        model,feats = model_dict[model_name]\n",
    "        auc = model2auc(model, x_valid, y_valid, feats)\n",
    "        print(f\"{model_name} {auc:.3f}\",end=' ')\n",
    "        aucs.append(auc)\n",
    "    saucs = sorted(aucs)\n",
    "    sort_index = [saucs.index(i) for i in aucs]\n",
    "    print(sort_index)\n",
    "    if sort_index==[3,4,1,2,0]:\n",
    "        print(s)\n",
    "        post_slack(\"done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5b87b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T02:23:56.567658Z",
     "start_time": "2022-12-01T02:23:56.564688Z"
    }
   },
   "outputs": [],
   "source": [
    "saucs = sorted(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48d16ce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T02:24:59.392357Z",
     "start_time": "2022-12-01T02:24:59.387586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[saucs.index(i) for i in aucs]==[3,4,1,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07070ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 # 0.8155\n",
    "model6 # 0.8024 \n",
    "model71 # 0.7948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f391a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 # 0.8111 3\n",
    "model2 # 0.8155 4\n",
    "model5 # 0.8112\n",
    "model6 # 0.8024 1\n",
    "model7 # 0.8062 2\n",
    "model71 # 0.7948 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATS1 # 0.8111\n",
    "FEATS2 # 0.8155\n",
    "FEATS5 # 0.8112\n",
    "FEATS6 # 0.8024\n",
    "FEATS7 # 0.8062\n",
    "FEATS71 # 0.7948"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
