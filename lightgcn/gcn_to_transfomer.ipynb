{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lgcnmodel/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from config import CFG, logging_conf\n",
    "from lightgcn.datasets import prepare_dataset\n",
    "from lightgcn.models import build, train\n",
    "from lightgcn.utils import class2dict, get_logger\n",
    "\n",
    "from lightgcn.datasets import prepare_dataset\n",
    "from lightgcn.models import build, inference\n",
    "from lightgcn.utils import get_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(logging_conf)\n",
    "use_cuda = torch.cuda.is_available() and CFG.use_cuda_if_available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(basepath):\n",
    "    path1 = os.path.join(basepath, \"train_data.csv\")\n",
    "    path2 = os.path.join(basepath, \"test_data.csv\")\n",
    "    data1 = pd.read_csv(path1)\n",
    "    data2 = pd.read_csv(path2)\n",
    "\n",
    "    data = pd.concat([data1, data2])\n",
    "    data.drop_duplicates(\n",
    "        subset=[\"userID\", \"assessmentItemID\"], keep=\"last\", inplace=True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def separate_data(data):\n",
    "    train_data = data[data.answerCode >= 0]\n",
    "    test_data = data[data.answerCode < 0]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def indexing_data(data):\n",
    "    userid, itemid = (\n",
    "        sorted(list(set(data.userID))),\n",
    "        sorted(list(set(data.assessmentItemID))),\n",
    "    )\n",
    "    n_user, n_item = len(userid), len(itemid)\n",
    "\n",
    "    userid_2_index = {v: i for i, v in enumerate(userid)}\n",
    "    itemid_2_index = {v: i + n_user for i, v in enumerate(itemid)}\n",
    "    id_2_index = dict(userid_2_index, **itemid_2_index)\n",
    "\n",
    "    return id_2_index\n",
    "\n",
    "\n",
    "def process_data(data, id_2_index, device):\n",
    "    edge, label = [], []\n",
    "    for user, item, acode in zip(data.userID, data.assessmentItemID, data.answerCode):\n",
    "        uid, iid = id_2_index[user], id_2_index[item]\n",
    "        edge.append([uid, iid])\n",
    "        label.append(acode)\n",
    "\n",
    "    edge = torch.LongTensor(edge).T\n",
    "    label = torch.LongTensor(label)\n",
    "\n",
    "    return dict(edge=edge.to(device), label=label.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(device, basepath, verbose=True, logger=None):\n",
    "    data = load_data(basepath)\n",
    "    train_data, test_data = separate_data(data)\n",
    "    id2index = indexing_data(data)\n",
    "    train_data_proc = process_data(train_data, id2index, device)\n",
    "    test_data_proc = process_data(test_data, id2index, device)\n",
    "\n",
    "    # if verbose:\n",
    "    #     print_data_stat(train_data, \"Train\", logger=logger)\n",
    "    #     print_data_stat(test_data, \"Test\", logger=logger)\n",
    "\n",
    "    return train_data_proc, test_data_proc, len(id2index), id2index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prepare\n",
    "train_data, test_data, n_node, edge_index = prepare_dataset(\n",
    "    device, CFG.basepath, verbose=CFG.loader_verbose, logger=logger.getChild(\"data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-28 07:26:38,826 - build - INFO - No load model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGCN(16896, 64, num_layers=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model build\n",
    "model = build(\n",
    "    n_node,\n",
    "    embedding_dim=CFG.embedding_dim,\n",
    "    num_layers=CFG.num_layers,\n",
    "    alpha=CFG.alpha,\n",
    "    logger=logger.getChild(\"build\"),\n",
    "    **CFG.build_kwargs\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-28 07:26:39,109 - train - INFO - Training Started : n_epoch=20\n",
      "2022-11-28 07:26:39,148 - train - INFO -  * In epoch 0001, loss=0.693, acc=0.528, AUC=0.526\n",
      "2022-11-28 07:26:39,149 - train - INFO -  * In epoch 0001, loss=0.693, acc=0.528, AUC=0.526, Best AUC\n",
      "2022-11-28 07:26:39,207 - train - INFO -  * In epoch 0002, loss=0.693, acc=0.538, AUC=0.546\n",
      "2022-11-28 07:26:39,209 - train - INFO -  * In epoch 0002, loss=0.693, acc=0.538, AUC=0.546, Best AUC\n",
      "2022-11-28 07:26:39,266 - train - INFO -  * In epoch 0003, loss=0.693, acc=0.549, AUC=0.567\n",
      "2022-11-28 07:26:39,268 - train - INFO -  * In epoch 0003, loss=0.693, acc=0.549, AUC=0.567, Best AUC\n",
      "2022-11-28 07:26:39,325 - train - INFO -  * In epoch 0004, loss=0.693, acc=0.560, AUC=0.588\n",
      "2022-11-28 07:26:39,327 - train - INFO -  * In epoch 0004, loss=0.693, acc=0.560, AUC=0.588, Best AUC\n",
      "2022-11-28 07:26:39,382 - train - INFO -  * In epoch 0005, loss=0.693, acc=0.586, AUC=0.609\n",
      "2022-11-28 07:26:39,384 - train - INFO -  * In epoch 0005, loss=0.693, acc=0.586, AUC=0.609, Best AUC\n",
      "2022-11-28 07:26:39,438 - train - INFO -  * In epoch 0006, loss=0.693, acc=0.603, AUC=0.630\n",
      "2022-11-28 07:26:39,440 - train - INFO -  * In epoch 0006, loss=0.693, acc=0.603, AUC=0.630, Best AUC\n",
      "2022-11-28 07:26:39,494 - train - INFO -  * In epoch 0007, loss=0.693, acc=0.619, AUC=0.653\n",
      "2022-11-28 07:26:39,496 - train - INFO -  * In epoch 0007, loss=0.693, acc=0.619, AUC=0.653, Best AUC\n",
      "2022-11-28 07:26:39,550 - train - INFO -  * In epoch 0008, loss=0.693, acc=0.633, AUC=0.675\n",
      "2022-11-28 07:26:39,552 - train - INFO -  * In epoch 0008, loss=0.693, acc=0.633, AUC=0.675, Best AUC\n",
      "2022-11-28 07:26:39,606 - train - INFO -  * In epoch 0009, loss=0.693, acc=0.654, AUC=0.697\n",
      "2022-11-28 07:26:39,608 - train - INFO -  * In epoch 0009, loss=0.693, acc=0.654, AUC=0.697, Best AUC\n",
      "2022-11-28 07:26:39,657 - train - INFO -  * In epoch 0010, loss=0.693, acc=0.680, AUC=0.719\n",
      "2022-11-28 07:26:39,659 - train - INFO -  * In epoch 0010, loss=0.693, acc=0.680, AUC=0.719, Best AUC\n",
      "2022-11-28 07:26:39,709 - train - INFO -  * In epoch 0011, loss=0.693, acc=0.701, AUC=0.739\n",
      "2022-11-28 07:26:39,711 - train - INFO -  * In epoch 0011, loss=0.693, acc=0.701, AUC=0.739, Best AUC\n",
      "2022-11-28 07:26:39,762 - train - INFO -  * In epoch 0012, loss=0.693, acc=0.715, AUC=0.758\n",
      "2022-11-28 07:26:39,763 - train - INFO -  * In epoch 0012, loss=0.693, acc=0.715, AUC=0.758, Best AUC\n",
      "2022-11-28 07:26:39,812 - train - INFO -  * In epoch 0013, loss=0.693, acc=0.732, AUC=0.775\n",
      "2022-11-28 07:26:39,814 - train - INFO -  * In epoch 0013, loss=0.693, acc=0.732, AUC=0.775, Best AUC\n",
      "2022-11-28 07:26:39,863 - train - INFO -  * In epoch 0014, loss=0.693, acc=0.739, AUC=0.790\n",
      "2022-11-28 07:26:39,865 - train - INFO -  * In epoch 0014, loss=0.693, acc=0.739, AUC=0.790, Best AUC\n",
      "2022-11-28 07:26:39,922 - train - INFO -  * In epoch 0015, loss=0.693, acc=0.756, AUC=0.802\n",
      "2022-11-28 07:26:39,924 - train - INFO -  * In epoch 0015, loss=0.693, acc=0.756, AUC=0.802, Best AUC\n",
      "2022-11-28 07:26:39,977 - train - INFO -  * In epoch 0016, loss=0.693, acc=0.764, AUC=0.811\n",
      "2022-11-28 07:26:39,979 - train - INFO -  * In epoch 0016, loss=0.693, acc=0.764, AUC=0.811, Best AUC\n",
      "2022-11-28 07:26:40,031 - train - INFO -  * In epoch 0017, loss=0.693, acc=0.771, AUC=0.818\n",
      "2022-11-28 07:26:40,033 - train - INFO -  * In epoch 0017, loss=0.693, acc=0.771, AUC=0.818, Best AUC\n",
      "2022-11-28 07:26:40,084 - train - INFO -  * In epoch 0018, loss=0.693, acc=0.776, AUC=0.824\n",
      "2022-11-28 07:26:40,086 - train - INFO -  * In epoch 0018, loss=0.693, acc=0.776, AUC=0.824, Best AUC\n",
      "2022-11-28 07:26:40,139 - train - INFO -  * In epoch 0019, loss=0.693, acc=0.774, AUC=0.827\n",
      "2022-11-28 07:26:40,140 - train - INFO -  * In epoch 0019, loss=0.693, acc=0.774, AUC=0.827, Best AUC\n",
      "2022-11-28 07:26:40,192 - train - INFO -  * In epoch 0020, loss=0.693, acc=0.775, AUC=0.829\n",
      "2022-11-28 07:26:40,194 - train - INFO -  * In epoch 0020, loss=0.693, acc=0.775, AUC=0.829, Best AUC\n",
      "2022-11-28 07:26:40,237 - train - INFO - Best Weight Confirmed : 20'th epoch\n",
      "2022-11-28 07:26:40,239 - root - INFO - Task Complete\n"
     ]
    }
   ],
   "source": [
    "# model train\n",
    "train(\n",
    "    model,\n",
    "    train_data,\n",
    "    n_epoch=CFG.n_epoch,\n",
    "    learning_rate=CFG.learning_rate,\n",
    "    use_wandb=False,\n",
    "    weight=CFG.weight_basepath,\n",
    "    logger=logger.getChild(\"train\"),\n",
    ")\n",
    "logger.info(\"Task Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(logging_conf)\n",
    "use_cuda = torch.cuda.is_available() and CFG.use_cuda_if_available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if not os.path.exists(CFG.output_dir):\n",
    "    os.makedirs(CFG.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inference(model, test_data, logger=logger.getChild(\"infer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.detach().cpu().numpy()\n",
    "pd.DataFrame({\"prediction\": pred}).to_csv(\n",
    "        os.path.join(CFG.output_dir, \"plus_test_submission.csv\"), index_label=\"id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGCN 임베딩 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16896, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0115, -0.0279,  0.0204,  ...,  0.0023, -0.0060, -0.0223],\n",
       "        [-0.0256, -0.0012,  0.0083,  ..., -0.0217,  0.0143,  0.0082],\n",
       "        [ 0.0099,  0.0029,  0.0068,  ...,  0.0236,  0.0211, -0.0271],\n",
       "        ...,\n",
       "        [-0.0225, -0.0119,  0.0070,  ...,  0.0156,  0.0111, -0.0194],\n",
       "        [ 0.0111,  0.0186,  0.0226,  ..., -0.0269, -0.0027,  0.0142],\n",
       "        [-0.0164, -0.0097, -0.0248,  ..., -0.0152, -0.0031, -0.0119]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.get_embedding(train_data['edge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0057, -0.0139,  0.0102, -0.0062,  0.0080,  0.0005, -0.0093, -0.0041,\n",
      "         0.0051,  0.0141, -0.0069,  0.0071, -0.0016,  0.0124,  0.0006, -0.0137,\n",
      "        -0.0029, -0.0056, -0.0054, -0.0099, -0.0043,  0.0116,  0.0059, -0.0152,\n",
      "         0.0043, -0.0113,  0.0148, -0.0084, -0.0094,  0.0088,  0.0040,  0.0085,\n",
      "        -0.0016, -0.0037,  0.0005, -0.0165, -0.0147, -0.0028,  0.0050,  0.0029,\n",
      "        -0.0117, -0.0131, -0.0002,  0.0041,  0.0038, -0.0166, -0.0069, -0.0012,\n",
      "         0.0115, -0.0040, -0.0002,  0.0011,  0.0022,  0.0125,  0.0034,  0.0015,\n",
      "        -0.0017, -0.0129, -0.0115, -0.0134,  0.0015,  0.0012, -0.0030, -0.0112],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "p1 = embed[edge_index[0]]\n",
    "p2 = embed[edge_index['A060001001']]\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riiid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 : 5강 실습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# RiiiD 데이터셋 path 설정\n",
    "RIIID_PATH = \"/opt/ml/input/data/\"\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "train_df = pd.read_csv(os.path.join(RIIID_PATH, 'train_data.csv'))\n",
    "test_df = pd.read_csv(os.path.join(RIIID_PATH, 'test_data.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(RIIID_PATH, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정에서 학습 샘플을 생성하기 위해서 필요한 유저별 row_ids를 저장\n",
    "question_row_ids_by_user_id = train_df.groupby('userID').apply(lambda x: x.index.tolist())\n",
    "question_row_ids_by_user_id.reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정에서 학습 샘플을 생성하기 위해서 필요한 유저별 시작 row_id를 저장\n",
    "start_row_id_by_user_id = train_df.groupby('userID').apply(lambda x: x.index[0])\n",
    "start_row_id_by_user_id.reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 추가\n",
    "train_df['big_category'] = train_df.testId.map(lambda x:x[2]).astype(int)\n",
    "train_df['mid_category'] = train_df.testId.map(lambda x: int(x[-3:]))\n",
    "train_df['problem_num'] = train_df.assessmentItemID.map(lambda x: int(x[-3:]))\n",
    "\n",
    "# 데이터 타입 변경\n",
    "train_df['KnowledgeTag'] = train_df['KnowledgeTag'].astype(str)\n",
    "train_df['big_category'] = train_df['big_category'].astype(str)\n",
    "train_df['mid_category'] = train_df['mid_category'].astype(str)\n",
    "train_df['problem_num'] = train_df['problem_num'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate2id_dict = {}\n",
    "\n",
    "offset = 0\n",
    "\n",
    "# assessmentItemID2id\n",
    "Item2id = dict([(v, i+offset) for i, v in enumerate(train_df['assessmentItemID'].unique())])\n",
    "cate2id_dict['Item2id'] = Item2id\n",
    "offset += len(Item2id)\n",
    "\n",
    "# testId2id\n",
    "testId2id = dict([(v, i+offset) for i, v in enumerate(train_df['testId'].unique())])\n",
    "cate2id_dict['testId2id'] = testId2id\n",
    "offset += len(testId2id)\n",
    "\n",
    "# KnowledgeTag2id\n",
    "KnowledgeTag2id = dict([(v, i+offset) for i, v in enumerate(train_df['KnowledgeTag'].unique())])\n",
    "cate2id_dict['KnowledgeTag2id'] = KnowledgeTag2id\n",
    "offset += len(KnowledgeTag2id)\n",
    "\n",
    "# big_category2id\n",
    "big_category2id = dict([(v, i+offset) for i, v in enumerate(train_df['big_category'].unique())])\n",
    "cate2id_dict['big_category2id'] = big_category2id\n",
    "offset += len(big_category2id)\n",
    "        \n",
    "# mid_category2id\n",
    "mid_category2id = dict([(v, i+offset) for i, v in enumerate(train_df['mid_category'].unique())])\n",
    "cate2id_dict['mid_category2id'] = mid_category2id\n",
    "offset += len(mid_category2id)\n",
    "\n",
    "# problem_num2id\n",
    "problem_num2id = dict([(v, i+offset) for i, v in enumerate(train_df['problem_num'].unique())])\n",
    "cate2id_dict['problem_num2id'] = problem_num2id\n",
    "offset += len(problem_num2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# mapping\n",
    "train_df['assessmentItemID'] = train_df['assessmentItemID'].map(Item2id)\n",
    "train_df['testId'] = train_df['testId'].map(testId2id)\n",
    "train_df['KnowledgeTag'] = train_df['KnowledgeTag'].map(KnowledgeTag2id)\n",
    "train_df['big_category'] = train_df['big_category'].map(big_category2id)\n",
    "train_df['mid_category'] = train_df['mid_category'].map(mid_category2id)\n",
    "train_df['problem_num'] = train_df['problem_num'].map(problem_num2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp 변경하기\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def convert_time(s):\n",
    "    timestamp = time.mktime(\n",
    "        datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple()\n",
    "    )\n",
    "    return int(timestamp)\n",
    "\n",
    "train_df[\"Timestamp\"] = train_df[\"Timestamp\"].apply(convert_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = [ #'Timestamp',\n",
    "             'answerCode']\n",
    "\n",
    "cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag', 'big_category', 'mid_category', 'problem_num'] \n",
    "\n",
    "train_df[cate_cols] = train_df[cate_cols].astype(np.int16)\n",
    "# train_df[cont_cols] = train_df[cont_cols].astype(np.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"훈련 데이터셋 shape : {train_df.shape}\")\n",
    "print(f\"category 값들의 총 갯수 : {offset}\")\n",
    "print(f\"category feature들의 column 이름 : {cate_cols}\")\n",
    "print(f\"continuous feature들의 column 이름 : {cont_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"category feature들의 index : {cate2id_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train셋 sequence 데이터들의 indices : {question_row_ids_by_user_id}\\n\")\n",
    "print(f\"train셋 각 sequence 데이터들의 첫 row의 index : {start_row_id_by_user_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG_T:\n",
    "    seed=7\n",
    "    device='cpu'\n",
    "\n",
    "    batch_size=16\n",
    "\n",
    "    dropout=0.2\n",
    "    emb_size=100\n",
    "    hidden_size=128\n",
    "    nlayers=2\n",
    "    nheads=8\n",
    "  \n",
    "    seq_len=32\n",
    "    target_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_T.total_cate_size = offset\n",
    "CFG_T.cate_cols = cate_cols\n",
    "CFG_T.cont_cols = cont_cols\n",
    "CFG_T.start_row_id_by_user_id = start_row_id_by_user_id\n",
    "\n",
    "CFG_T.cate_vocab_size = offset\n",
    "\n",
    "CFG_T.cate_col_size = len(cate_cols)\n",
    "CFG_T.cont_col_size = len(cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 sequence들의 index들을 그대로 사용하지 않는다\n",
    "question_row_ids_by_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_id_row_id_list = [(user_id, row_id)\n",
    "                             for user_id, row_ids in question_row_ids_by_user_id.items()\n",
    "                             for row_id in row_ids]\n",
    "train_user_id_row_id_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row가 꽤나 늘어났음을 알 수 있다.\n",
    "len(train_user_id_row_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration에 등록!\n",
    "CFG_T.train_user_id_row_id_list = train_user_id_row_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[cate_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[cont_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class RiiidDataset(Dataset):\n",
    "    def __init__(self, df, cfg, max_seq_len=100, max_content_len=1000):        \n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_content_len = max_content_len\n",
    "        \n",
    "        self.user_id_row_id_list = cfg.train_user_id_row_id_list\n",
    "        self.start_row_id_by_user_id = cfg.start_row_id_by_user_id\n",
    "\n",
    "        self.cate_cols = cfg.cate_cols\n",
    "        self.cont_cols = cfg.cont_cols\n",
    "        \n",
    "        self.cate_features = df[self.cate_cols].values\n",
    "        self.cont_features = df[self.cont_cols].values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        user_id, end_row_id = self.user_id_row_id_list[idx]\n",
    "        end_row_id += 1\n",
    "        \n",
    "        start_row_id = self.start_row_id_by_user_id[user_id]\n",
    "        start_row_id = max(end_row_id - self.max_seq_len, start_row_id) # lower bound\n",
    "        seq_len = end_row_id - start_row_id\n",
    "\n",
    "        # 0으로 채워진 output tensor 제작                  \n",
    "        cate_feature = torch.zeros(self.max_seq_len, len(self.cate_cols), dtype=torch.long)\n",
    "        cont_feature = torch.zeros(self.max_seq_len, len(self.cont_cols), dtype=torch.float)\n",
    "        mask = torch.zeros(self.max_seq_len, dtype=torch.int16)\n",
    "       \n",
    "        # tensor에 값 채워넣기\n",
    "        cate_feature[-seq_len:] = torch.ShortTensor(self.cate_features[start_row_id:end_row_id])\n",
    "        cont_feature[-seq_len:] = torch.HalfTensor(self.cont_features[start_row_id:end_row_id])\n",
    "        mask[-seq_len:] = 1        \n",
    "            \n",
    "        # answered_correctly가 cont_feature[-1]에 위치한다\n",
    "        target = torch.FloatTensor([cont_feature[-1, -1]])\n",
    "\n",
    "        # answered_correctly 및 relative_answered_correctly는\n",
    "        # data leakage가 발생할 수 있으므로 0으로 모두 채운다\n",
    "        # cont_feature[-1, -1] = 0\n",
    "        # cont_feature[-1, -2] = 0\n",
    "        \n",
    "        return cate_feature, cont_feature, mask, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.user_id_row_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = RiiidDataset(train_df, CFG_T, max_seq_len=CFG_T.seq_len)\n",
    "train_loader = DataLoader(train_db, batch_size=CFG_T.batch_size, shuffle=True,\n",
    "                          drop_last=False, pin_memory=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence 데이터 하나의 shape을 살펴보자\n",
    "for cate_x, cont_x, mask, target in train_db:\n",
    "    print(f\"category size : {cate_x.size()}\")\n",
    "    print(f\"continous size : {cont_x.size()}\")\n",
    "    print(f\"mask size : {mask.size()}\")\n",
    "    print(f\"target size : {target.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 단위로 주어지는 데이터를 살펴보자\n",
    "for cate_x, cont_x, mask, target in train_loader:\n",
    "    print(f\"category size : {cate_x.size()}\")\n",
    "    print(f\"continous size : {cont_x.size()}\")\n",
    "    print(f\"mask size : {mask.size()}\")\n",
    "    print(f\"target size : {target.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📗 Transformer Input / Output 구현\n",
    "> transformer에 입력시킬 input을 구현하고 transformer를 거친 output을 우리가 원하는 최종 출력값으로 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 입력값\n",
    "for cate_x, cont_x, mask, target in train_loader:\n",
    "    print(f\"category size : {cate_x.size()}\")\n",
    "    print(f\"continous size : {cont_x.size()}\")\n",
    "    print(f\"mask size : {mask.size()}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🟡 Category Embedding\n",
    "> 범주형 feature를 임베딩하는 과정을 살펴보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 크기\n",
    "CFG_T.emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = cate_x.size(0)\n",
    "\n",
    "# 범주형 하나당 100개로 임베딩된다!\n",
    "# [16, 32, 6] -> [16, 32, 6, 100]\n",
    "cate_emb = nn.Embedding(CFG_T.total_cate_size, CFG_T.emb_size, padding_idx=0)\n",
    "cate_embed_x = cate_emb(cate_x)\n",
    "\n",
    "cate_embed_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence 길이를 몇 배 줄일 것인지\n",
    "# 메모리 절약의 의도가 있다\n",
    "CFG_T.n_rows_per_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_embed_normal_x = cate_embed_x.view(batch_size, CFG_T.seq_len, -1)\n",
    "cate_embed_normal_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_seq_len = cate_x.size(1) // CFG_T.n_rows_per_step\n",
    "\n",
    "# transformer input은 3차원이고 마지막 차원은 hidden 값이다.\n",
    "# sequence의 각 위치에 카테고리별로 임베딩되어있는 것을 하나로 합치자!\n",
    "# [16, 32, 6, 100] -> [16, 16, 1200]\n",
    "cate_embed_x = cate_embed_x.view(batch_size, half_seq_len, -1)\n",
    "cate_embed_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이후에 우리가 원하는 hidden_size의 절반으로 projection한다!\n",
    "# 이렇게 하는 이유는 반은 category로 반은 continous으로 hidden 값을 채우기 위해서이다\n",
    "# [16, 16, 1200] -> [16, 16, 128]\n",
    "cate_proj = nn.Sequential(nn.Linear(CFG_T.emb_size * CFG_T.cate_col_size * CFG_T.n_rows_per_step, CFG_T.hidden_size),\n",
    "                          nn.LayerNorm(CFG_T.hidden_size))     \n",
    "cate_embed_x = cate_proj(cate_embed_x)\n",
    "cate_embed_x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🟡 Continuous Embedding\n",
    "> 수치형 feature를 임베딩하는 과정을 살펴보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_bn = nn.BatchNorm1d(CFG_T.cont_col_size)\n",
    "\n",
    "# batchnorm 1d 적용\n",
    "cont_bn_x = cont_bn(cont_x.view(-1, cont_x.size(-1)))\n",
    "cont_bn_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchnorm 적용 이후 원래 사이즈 복구\n",
    "cont_bn_x = cont_bn_x.view(batch_size, -1, cont_x.size(-1))\n",
    "cont_bn_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cate에서 사용한 half_seq_len 그대로 사용\n",
    "cont_bn_x = cont_bn_x.view(batch_size, half_seq_len, -1)\n",
    "cont_bn_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형과는 다르게 embedding없이 바로 projection을 통해 원하는 사이즈로 줄인다\n",
    "# 여기서는 embedding이라고 부른다\n",
    "cont_emb = nn.Sequential(nn.Linear(CFG_T.cont_col_size * CFG_T.n_rows_per_step, CFG_T.hidden_size),\n",
    "                         nn.LayerNorm(CFG_T.hidden_size))\n",
    "cont_embed_x = cont_emb(cont_bn_x)\n",
    "cont_embed_x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🟡 범주형 / 수치형 embedding tensor concat\n",
    "> Transformer에 입력값으로 주려면 범주형 / 수치형으로 embedding된 2개의 tensor를 하나로 합쳐야 한다. 이를 통해 우리는 많은 feature들이 포함된 데이터를 성공적으로 하나의 입력값으로 만들 수 있다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_embed_x.size(), cont_embed_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_emb = torch.cat([cate_embed_x, cont_embed_x], 2)\n",
    "seq_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_proj = nn.Sequential(nn.ReLU(),\n",
    "                          nn.Linear(CFG_T.hidden_size*2, CFG_T.hidden_size),\n",
    "                          nn.LayerNorm(CFG_T.hidden_size))\n",
    "\n",
    "# concat한 sequence를 projection을 통해 원하는 사이즈로 변환한다\n",
    "# 여기서는 embedding이라고 부른다\n",
    "# [16, 16, 256] -> [16, 16, 128]\n",
    "seq_emb = comb_proj(seq_emb)\n",
    "seq_emb.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🟡 Encoder\n",
    "> 이제 완성된 입력값을 모델에 넣어보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "except:\n",
    "    from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel   \n",
    "\n",
    "config = BertConfig(3, # not used\n",
    "                    hidden_size=CFG_T.hidden_size,\n",
    "                    num_hidden_layers=CFG_T.nlayers,\n",
    "                    num_attention_heads=CFG_T.nheads,\n",
    "                    intermediate_size=CFG_T.hidden_size,\n",
    "                    hidden_dropout_prob=CFG_T.dropout,\n",
    "                    attention_probs_dropout_prob=CFG_T.dropout)\n",
    "\n",
    "encoder = BertEncoder(config)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Encoder를 거친 tensor의 크기는 동일하게 나온다\n",
    "# [16, 16, 128] -> [16, 16, 128]\n",
    "encoded_layers = encoder(seq_emb)\n",
    "sequence_output = encoded_layers[-1]\n",
    "sequence_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 필요한건 Bert의 마지막 query다\n",
    "# [16, 16, 128] -> [16, 128]\n",
    "sequence_output = sequence_output[:, -1]\n",
    "sequence_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🟡 분류 단계\n",
    "> 이제 우리는 최종 분류를 해야한다! 이걸 위해서 우리는 출력의 크기를 클래스 숫자인 1로 변환한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg():\n",
    "    return nn.Sequential(nn.Linear(CFG_T.hidden_size, CFG_T.hidden_size),\n",
    "                         nn.LayerNorm(CFG_T.hidden_size),\n",
    "                         nn.Dropout(CFG_T.dropout),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(CFG_T.hidden_size, CFG_T.target_size))\n",
    "\n",
    "reg_layer = get_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 😍 우리는 원하는 결과값을 얻었다 😍\n",
    "# [16, 128] -> [16, 1]\n",
    "pred_y = reg_layer(sequence_output)\n",
    "pred_y.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('lgcnmodel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dcb66f4364347f1194a95291094c517d424054fad9034ad9eacecef50e85ee0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
