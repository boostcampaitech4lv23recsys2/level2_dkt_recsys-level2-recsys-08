
import os
import torch
import torch.nn as nn
import numpy as np
from .get_embed import get_embed

try:
    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel
except:
    from transformers.models.bert.modeling_bert import (
        BertConfig,
        BertEncoder,
        BertModel,
    )

import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))))

from lightgcn.config import CFG, logging_conf
from lightgcn.lightgcn.utils import class2dict, get_logger

logger = get_logger(logging_conf)
use_cuda = torch.cuda.is_available() and CFG.use_cuda_if_available
device = torch.device("cuda" if use_cuda else "cpu")

class LSTM(nn.Module):
    #new_feature ë¥¼ ì¶”ê°€í•˜ê³  ì‹¶ìœ¼ì‹œë©´, embedding ë¶€ë¶„ë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤.
    #lqtransformer.pyì˜ ì„ë² ë”© ë¶€ë¶„ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”! -> lqtransformer.pyì—ì„œ "new_feature" ì°¾ê¸° ê¸°ëŠ¥
    def __init__(self, args):
        super(LSTM, self).__init__()
        self.args = args

        self.hidden_dim = self.args.hidden_dim
        self.n_layers = self.args.n_layers

        # Embedding
        # interactionì€ í˜„ì¬ correctë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. correct(1, 2) + padding(0)
        self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 3)
        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim // 3)
        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim // 3)
        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim // 3)
        # big, mid, problem, month, dayname
        self.embedding_big = nn.Embedding(self.args.n_big + 1, self.hidden_dim // 3)
        self.embedding_mid = nn.Embedding(self.args.n_mid + 1, self.hidden_dim // 3)
        self.embedding_problem = nn.Embedding(self.args.n_problem + 1, self.hidden_dim // 3) 
        self.embedding_assIdx = nn.Embedding(self.args.n_assIdx + 1, self.hidden_dim // 3) 
        self.embedding_month = nn.Embedding(self.args.n_month + 1, self.hidden_dim // 3)
        self.embedding_day = nn.Embedding(self.args.n_day+ 1, self.hidden_dim // 3) 
        self.embedding_hour= nn.Embedding(self.args.n_hour + 1, self.hidden_dim // 3) 
        self.embedding_dayname = nn.Embedding(self.args.n_dayname + 1, self.hidden_dim // 3)
        self.embedding_time_category = nn.Embedding(self.args.n_time_category + 1, self.hidden_dim // 3) 
        self.embedding_solvecumsum_category = nn.Embedding(self.args.n_solvecumsum_category + 1, self.hidden_dim // 3) 
        # self.embedding_user_tag_cluster = nn.Embedding(self.args.n_user_tag_cluster + 1, self.hidden_dim // 3)        
        
        # big, mid, problem, month, dayname
        self.cat_proj = nn.Linear((self.hidden_dim // 3) * 13, self.hidden_dim//2) 
        
        # solvesec_600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum
        self.num_proj = nn.Sequential(nn.Linear(17, self.hidden_dim//2),
                                nn.LayerNorm(self.hidden_dim//2))
        
        # embedding combination projection
        self.comb_proj = nn.Linear((self.hidden_dim // 3) * 4, self.hidden_dim) # ì›í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì¤„ì´ê¸°

        self.lstm = nn.LSTM(
            self.hidden_dim, self.hidden_dim, self.n_layers, batch_first=True #nn.LSTM(input_size, hidden_size, num_layers, ...)
        )

        # Fully connected layer
        self.fc = nn.Linear(self.hidden_dim, 1)
        
        # lightGCN embed matrixì™€ id2index
        self.test_embed_matrix, self.test_n_user = get_embed('testId')
        self.question_embed_matrix, self.question_n_user = get_embed('assessmentItemID')
        
        # ë‹¤ë¥¸ ì„ë² ë”© ë²¡í„°ë“¤ê³¼ ì°¨ì› ë§ì¶°ì£¼ê¸°
        ## assessmentItemID
        self.lgcn_linear = nn.Linear(128, self.hidden_dim // 3)
        ## testId
        self.lgcn_linear_test = nn.Linear(256, self.hidden_dim // 3)
        
    # ë¬¸ì œ lgcn embedding êµ¬í•˜ê¸°
    def lgcn_embedding(self, itemnode, item):
        item = item.detach().cpu().numpy()
        
        if itemnode == 'testId':
            embed_matrix, n_user = self.test_embed_matrix, self.test_n_user
        else:
            embed_matrix, n_user = self.question_embed_matrix, self.question_n_user
        embed_matrix = embed_matrix.detach().cpu().numpy()
        
        len_user = n_user - 1

        item_embed = []
        for user in item:
            user_li = []
            for i in user:
                user_li.append(embed_matrix[len_user + i])
            item_embed.append(user_li)

        item_embed = torch.Tensor(np.array(item_embed))
        
        return item_embed
    
    def forward(self, input):

        # test, question, tag, _, mask, interaction, big, mid, problem, month, dayname, month_mean, solvesec_3600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum, user_correct_answer, user_total_answer, user_acc = input #(test, question, tag, correct, mask, interaction)
        test, question, tag, _, mask, interaction, big_category, mid_category, problem_num, assIdx, month, day, hour, dayname, time_category, solvecumsum_category,  \
        solvesec_3600, test_mean, test_std, tag_mean, tag_std, big_mean, big_std, user_correct_answer, user_total_answer, user_acc, \
        solvesec_cumsum,  big_category_cumconut, big_category_user_acc, big_category_user_std, big_category_answer, big_category_answer_log1p, elo_assessmentItemID = input

        batch_size = interaction.size(0)

        # Embedding
        embed_interaction = self.embedding_interaction(interaction) #interactionì˜ ê°’ì€ 0/1/2 ì¤‘ í•˜ë‚˜ì´ë‹¤.
        
        embed_test = self.embedding_test(test)                #shape = (64,20,21)
        embed_test_lgcn = self.lgcn_embedding('testId', test).to(device)
        embed_test_lgcn = self.lgcn_linear_test(embed_test_lgcn)
        
        embed_question = self.embedding_question(question)
        embed_question_lgcn = self.lgcn_embedding('assessmentItemID', question).to(device)
        embed_question_lgcn = self.lgcn_linear(embed_question_lgcn)
        
        embed_tag = self.embedding_tag(tag)

        # big, mid, problem, month, dayname
        embed_big = self.embedding_big(big_category)
        embed_mid = self.embedding_mid(mid_category)
        embed_problem = self.embedding_problem(problem_num) 
        embed_assIdx= self.embedding_assIdx(assIdx)
        embed_month = self.embedding_month(month)
        embed_day = self.embedding_day(day)
        embed_hour = self.embedding_hour(hour)
        embed_dayname = self.embedding_dayname(dayname)
        embed_time_category = self.embedding_time_category(time_category)
        embed_solvecumsum_category = self.embedding_solvecumsum_category(solvecumsum_category)
        # embed_user_tag_cluster = self.embedding_user_tag_cluster(user_tag_cluster)    
            
        embed_cat = torch.cat(
            [
                embed_interaction,
                # embed_test,
                embed_test_lgcn,
                # embed_question,
                embed_question_lgcn,
                embed_tag,
                embed_big,
                # embed_mid,
                embed_problem,
                embed_assIdx,
                embed_month,
                embed_day,
                embed_hour,
                embed_dayname,
                embed_time_category,
                embed_solvecumsum_category,
                # embed_user_tag_cluster
            ],
            2,
        ) #shape = (64,20,84)

        embed_cat = self.cat_proj(embed_cat)
        
        ## ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ ì‚¬ìš© ì‹œ
        # X = self.comb_proj(embed_cat) #(64,20,64)
        
        #####ğŸ˜˜ ì—°ì†í˜• ë³€ìˆ˜ ì¶”ê°€ ì‹œ #####
        # ì£¼ì˜í•  ì  : cat_projì™€ num_projì˜ out_dimì„ ê°ê° hidden_dim//2ë¡œ í•˜ê¸°,
        #           137ì¤„ embed_cat ëŒ€ì‹  embedë¡œ ë°”ê¾¸ê¸°
        #           __init__ì˜ self.num_projë„ ìˆ˜ì •í•˜ê¸°
        
        embed_num = torch.cat(
            [
        #     month_mean.unsqueeze(2),
            solvesec_3600.unsqueeze(2), #[64, 20, 1]
            test_mean.unsqueeze(2),
            test_std.unsqueeze(2),
        #     test_sum.unsqueeze(2),
            tag_mean.unsqueeze(2),
            tag_std.unsqueeze(2),
        #     tag_sum.unsqueeze(2),
            big_mean.unsqueeze(2),
            big_std.unsqueeze(2),
        #     big_sum.unsqueeze(2),
            user_correct_answer.unsqueeze(2),
            user_total_answer.unsqueeze(2),
            user_acc.unsqueeze(2),
            solvesec_cumsum.unsqueeze(2),
            big_category_cumconut.unsqueeze(2),
            big_category_user_acc.unsqueeze(2),
            big_category_user_std.unsqueeze(2),
            big_category_answer.unsqueeze(2),
            big_category_answer_log1p.unsqueeze(2),
            elo_assessmentItemID.unsqueeze(2)
            ],
            2,
        )
        
        embed_num = embed_num.type(torch.FloatTensor).to(device)
        embed_num = self.num_proj(embed_num)
        X = torch.cat([embed_cat, embed_num], 2)
        
        out, _ = self.lstm(X) #hidden, cell state ë°˜í™˜                 #out.shape = (64,20,64)
        out = out.contiguous().view(batch_size, -1, self.hidden_dim)
        out = self.fc(out).view(batch_size, -1)                       #out.shape = (64,20)
        return out


class LSTMATTN(nn.Module):
    def __init__(self, args):
        super(LSTMATTN, self).__init__()
        self.args = args

        self.hidden_dim = self.args.hidden_dim
        self.n_layers = self.args.n_layers
        self.n_heads = self.args.n_heads
        self.drop_out = self.args.drop_out

        # Embedding
        # interactionì€ í˜„ì¬ correctë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. correct(1, 2) + padding(0)
        self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 3)
        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim // 3)
        self.embedding_question = nn.Embedding(
            self.args.n_questions + 1, self.hidden_dim // 3
        )
        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim // 3)
        # big, mid, problem, month, dayname
        self.embedding_big = nn.Embedding(self.args.n_big + 1, self.hidden_dim // 3)
        self.embedding_mid = nn.Embedding(self.args.n_mid + 1, self.hidden_dim // 3)
        self.embedding_problem = nn.Embedding(self.args.n_problem + 1, self.hidden_dim // 3)
        self.embedding_month = nn.Embedding(self.args.n_month + 1, self.hidden_dim // 3)
        self.embedding_dayname = nn.Embedding(self.args.n_dayname + 1, self.hidden_dim // 3)
        
        # big, mid, problem, month, dayname
        self.cat_proj = nn.Linear((self.hidden_dim // 3) * 11, self.hidden_dim//2) 
        
        # solvesec_600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum
        self.num_proj = nn.Sequential(nn.Linear(10, self.hidden_dim//2),
                                nn.LayerNorm(self.hidden_dim//2))
        
        # embedding combination projection
        # self.comb_proj = nn.Linear((self.hidden_dim // 3) * 4, self.hidden_dim)

        self.lstm = nn.LSTM(
            self.hidden_dim, self.hidden_dim, self.n_layers, batch_first=True
        )

        ## ì´ ë¶€ë¶„ë„ Attentionì— ì˜í•´ ì¶”ê°€ëœ ë¶€ë¶„
        self.config = BertConfig(
            3,  # not used
            hidden_size=self.hidden_dim,
            num_hidden_layers=1,
            num_attention_heads=self.n_heads,
            intermediate_size=self.hidden_dim,
            hidden_dropout_prob=self.drop_out,
            attention_probs_dropout_prob=self.drop_out,
        )
        self.attn = BertEncoder(self.config)

        # Fully connected layer
        self.fc = nn.Linear(self.hidden_dim, 1)

        self.activation = nn.Sigmoid()
        
        # lightGCN embed matrixì™€ id2index
        self.test_embed_matrix, self.test_n_user = get_embed('testId')
        self.question_embed_matrix, self.question_n_user = get_embed('assessmentItemID')
        
        # ë‹¤ë¥¸ ì„ë² ë”© ë²¡í„°ë“¤ê³¼ ì°¨ì› ë§ì¶°ì£¼ê¸°
        ## assessmentItemID
        self.lgcn_linear = nn.Linear(128, self.hidden_dim // 3)
        ## testId
        self.lgcn_linear_test = nn.Linear(256, self.hidden_dim // 3)

    # ë¬¸ì œ lgcn embedding êµ¬í•˜ê¸°
    def lgcn_embedding(self, itemnode, item):
        item = item.detach().cpu().numpy()
        
        if itemnode == 'testId':
            embed_matrix, n_user = self.test_embed_matrix, self.test_n_user
        else:
            embed_matrix, n_user = self.question_embed_matrix, self.question_n_user
        embed_matrix = embed_matrix.detach().cpu().numpy()
        
        len_user = n_user - 1

        item_embed = []
        for user in item:
            user_li = []
            for i in user:
                user_li.append(embed_matrix[len_user + i])
            item_embed.append(user_li)

        item_embed = torch.Tensor(np.array(item_embed))
        
        return item_embed
    
    def forward(self, input):
        test, question, tag, _, mask, interaction, big, mid, problem, month, dayname, solvesec_600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum = input #(test, question, tag, correct, mask, interaction)
        # test, question, tag, _, mask, interaction, new_feature = input

        batch_size = interaction.size(0)

        # Embedding
        embed_interaction = self.embedding_interaction(interaction) #interactionì˜ ê°’ì€ 0/1/2 ì¤‘ í•˜ë‚˜ì´ë‹¤.
        
        embed_test = self.embedding_test(test)                #shape = (64,20,21)
        embed_test_lgcn = self.lgcn_embedding('testId', test).to(device)
        embed_test_lgcn = self.lgcn_linear_test(embed_test_lgcn)
        
        embed_question = self.embedding_question(question)
        embed_question_lgcn = self.lgcn_embedding('assessmentItemID', question).to(device)
        embed_question_lgcn = self.lgcn_linear(embed_question_lgcn)
        
        embed_tag = self.embedding_tag(tag)

        # big, mid, problem, month, dayname
        embed_big = self.embedding_big(big)
        embed_mid = self.embedding_mid(mid)
        embed_problem = self.embedding_problem(problem)
        embed_month = self.embedding_month(month)
        embed_dayname = self.embedding_dayname(dayname)
        
        embed_cat = torch.cat(
            [
                embed_interaction,
                embed_test, # 209ì¤„ ê°™ì´ ë°”ê¿”ì¤˜ì•¼ í•¨
                embed_test_lgcn,
                embed_question,
                embed_question_lgcn,
                embed_tag,
                embed_big,
                embed_mid,
                embed_problem,
                embed_month,
                embed_dayname
            ],
            2,
        ) #shape = (64,20,84)

        embed_cat = self.cat_proj(embed_cat)
        
        ## ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ ì‚¬ìš© ì‹œ
        # X = self.comb_proj(embed) #(64,20,64)
        
        #####ğŸ˜˜ ì—°ì†í˜• ë³€ìˆ˜ ì¶”ê°€ ì‹œ #####
        # ì£¼ì˜í•  ì  : cat_projì™€ num_projì˜ out_dimì„ ê°ê° hidden_dim//2ë¡œ í•˜ê¸°,
        #           137ì¤„ embed_cat ëŒ€ì‹  embedë¡œ ë°”ê¾¸ê¸°
        #           __init__ì˜ self.num_projë„ ìˆ˜ì •í•˜ê¸°
        
        # solvesec_600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum
        embed_num = torch.cat(
            [
            solvesec_600.unsqueeze(2), #[64, 20, 1]
            test_mean.unsqueeze(2),
            test_std.unsqueeze(2),
            test_sum.unsqueeze(2),
            tag_mean.unsqueeze(2),
            tag_std.unsqueeze(2),
            tag_sum.unsqueeze(2),
            big_mean.unsqueeze(2),
            big_std.unsqueeze(2),
            big_sum.unsqueeze(2),
            ],
            2,
        )
        
        embed_num = embed_num.type(torch.FloatTensor).to(device)

        embed_num = self.num_proj(embed_num)
        
        X = torch.cat([embed_cat, embed_num], 2)

        out, _ = self.lstm(X)
        out = out.contiguous().view(batch_size, -1, self.hidden_dim)

        ## ì´ ë¶€ë¶„ì´ íŠ¹ì§•ì ì¸  ë¶€ë¶„
        extended_attention_mask = mask.unsqueeze(1).unsqueeze(2) # ì°¨ì› ëŠ˜ë ¤ì£¼ê¸°
        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 # ë§ˆìŠ¤í‚¹ ëœ ë¶€ë¶„ ê°€ì¤‘ì¹˜ ë‚®ê²Œ ì¤˜ì„œ í•™ìŠµ ì•ˆë˜ë„ë¡
        head_mask = [None] * self.n_layers

        encoded_layers = self.attn(out, extended_attention_mask, head_mask=head_mask) # BERT Encoderë¥¼ ê°€ì ¸ë‹¤ ì”€
        sequence_output = encoded_layers[-1]

        out = self.fc(sequence_output).view(batch_size, -1)
        return out


class Bert(nn.Module):
    def __init__(self, args):
        super(Bert, self).__init__()
        self.args = args

        # Defining some parameters
        self.hidden_dim = self.args.hidden_dim
        self.n_layers = self.args.n_layers

        # Embedding
        # interactionì€ í˜„ì¬ correctìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. correct(1, 2) + padding(0)
        self.embedding_interaction = nn.Embedding(3, self.hidden_dim // 3)

        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim // 3)
        self.embedding_question = nn.Embedding(
            self.args.n_questions + 1, self.hidden_dim // 3
        )

        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim // 3)

        # big, mid, problem, month, dayname
        self.embedding_big = nn.Embedding(self.args.n_big + 1, self.hidden_dim // 3)
        self.embedding_mid = nn.Embedding(self.args.n_mid + 1, self.hidden_dim // 3)
        self.embedding_problem = nn.Embedding(self.args.n_problem + 1, self.hidden_dim // 3)
        self.embedding_month = nn.Embedding(self.args.n_month + 1, self.hidden_dim // 3)
        self.embedding_dayname = nn.Embedding(self.args.n_dayname + 1, self.hidden_dim // 3)
        
        # big, mid, problem, month, dayname
        self.cat_proj = nn.Linear((self.hidden_dim // 3) * 9, self.hidden_dim//2) 
        
        # solvesec_600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum
        self.num_proj = nn.Sequential(nn.Linear(10, self.hidden_dim//2),
                                nn.LayerNorm(self.hidden_dim//2))
        
        # embedding combination projection
        # self.comb_proj = nn.Linear((self.hidden_dim // 3) * 21, self.hidden_dim) # ì›í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì¤„ì´ê¸°

        # Bert config
        self.config = BertConfig(
            3,  # not used
            hidden_size=self.hidden_dim,
            num_hidden_layers=self.args.n_layers,
            num_attention_heads=self.args.n_heads,
            max_position_embeddings=self.args.max_seq_len,
        )

        # Defining the layers
        # Bert Layer
        self.encoder = BertModel(self.config) # ë²„íŠ¸ë¼ê³  ë˜ì–´ìˆì§€ë§Œ, íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë¶€ë¶„

        # Fully connected layer
        self.fc = nn.Linear(self.args.hidden_dim, 1)

        self.activation = nn.Sigmoid()
        
        # lightGCN embed matrixì™€ id2index
        self.test_embed_matrix, self.test_n_user = get_embed('testId')
        self.question_embed_matrix, self.question_n_user = get_embed('assessmentItemID')
        
        # ë‹¤ë¥¸ ì„ë² ë”© ë²¡í„°ë“¤ê³¼ ì°¨ì› ë§ì¶°ì£¼ê¸°
        ## assessmentItemID
        self.lgcn_linear = nn.Linear(128, self.hidden_dim // 3)
        ## testId
        self.lgcn_linear_test = nn.Linear(256, self.hidden_dim // 3)

    # ë¬¸ì œ lgcn embedding êµ¬í•˜ê¸°
    def lgcn_embedding(self, itemnode, item):
        item = item.detach().cpu().numpy()
        
        if itemnode == 'testId':
            embed_matrix, n_user = self.test_embed_matrix, self.test_n_user
        else:
            embed_matrix, n_user = self.question_embed_matrix, self.question_n_user
        embed_matrix = embed_matrix.detach().cpu().numpy()
        
        len_user = n_user - 1

        item_embed = []
        for user in item:
            user_li = []
            for i in user:
                user_li.append(embed_matrix[len_user + i])
            item_embed.append(user_li)

        item_embed = torch.Tensor(np.array(item_embed))
        
        return item_embed
    
    def forward(self, input):
        test, question, tag, _, mask, interaction, big, mid, problem, month, dayname, solvesec_600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum = input #(test, question, tag, correct, mask, interaction)
        # test, question, tag, _, mask, interaction, new_feature = input

        batch_size = interaction.size(0)

        # Embedding
        embed_interaction = self.embedding_interaction(interaction) #interactionì˜ ê°’ì€ 0/1/2 ì¤‘ í•˜ë‚˜ì´ë‹¤.
        
        embed_test = self.embedding_test(test)                #shape = (64,20,21)
        embed_test_lgcn = self.lgcn_embedding('testId', test).to(device)
        embed_test_lgcn = self.lgcn_linear_test(embed_test_lgcn)
        
        embed_question = self.embedding_question(question)
        embed_question_lgcn = self.lgcn_embedding('assessmentItemID', question).to(device)
        embed_question_lgcn = self.lgcn_linear(embed_question_lgcn)
        
        embed_tag = self.embedding_tag(tag)

        # big, mid, problem, month, dayname
        embed_big = self.embedding_big(big)
        embed_mid = self.embedding_mid(mid)
        embed_problem = self.embedding_problem(problem)
        embed_month = self.embedding_month(month)
        embed_dayname = self.embedding_dayname(dayname)
        
        embed_cat = torch.cat(
            [
                embed_interaction,
                # embed_test,
                embed_test_lgcn,
                # embed_question,
                embed_question_lgcn,
                embed_tag,
                embed_big,
                embed_mid,
                embed_problem,
                embed_month,
                embed_dayname
            ],
            2,
        ) #shape = (64,20,84)

        embed_cat = self.cat_proj(embed_cat)
        
        ## ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ ì‚¬ìš© ì‹œ
        # X = self.comb_proj(embed) #(64,20,64)
        
        #####ğŸ˜˜ ì—°ì†í˜• ë³€ìˆ˜ ì¶”ê°€ ì‹œ #####
        # ì£¼ì˜í•  ì  : cat_projì™€ num_projì˜ out_dimì„ ê°ê° hidden_dim//2ë¡œ í•˜ê¸°,
        #           137ì¤„ embed_cat ëŒ€ì‹  embedë¡œ ë°”ê¾¸ê¸°
        #           __init__ì˜ self.num_projë„ ìˆ˜ì •í•˜ê¸°
        
        # solvesec_600, test_mean, test_std, test_sum, tag_mean, tag_std, tag_sum, big_mean, big_std, big_sum
        embed_num = torch.cat(
            [
            solvesec_600.unsqueeze(2), #[64, 20, 1]
            test_mean.unsqueeze(2),
            test_std.unsqueeze(2),
            test_sum.unsqueeze(2),
            tag_mean.unsqueeze(2),
            tag_std.unsqueeze(2),
            tag_sum.unsqueeze(2),
            big_mean.unsqueeze(2),
            big_std.unsqueeze(2),
            big_sum.unsqueeze(2),
            ],
            2,
        )
        
        embed_num = embed_num.type(torch.FloatTensor).to(device)

        embed_num = self.num_proj(embed_num)
        
        X = torch.cat([embed_cat, embed_num], 2)
        
        # Bert
        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)
        out = encoded_layers[0] # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜

        out = out.contiguous().view(batch_size, -1, self.hidden_dim) # ë§ˆì§€ë§‰ ê°’ë§Œ ë½‘ì•„ì„œ

        out = self.fc(out).view(batch_size, -1) # loss ê³„ì‚°
        return out
